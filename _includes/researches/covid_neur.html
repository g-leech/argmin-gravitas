{%	assign id = "covidneurimg" 	%}

<tr>
	<td class="logo" style="padding-bottom: 10px">
		<a href="{{neurips_covid}}">
			<div id="{{id}}"></div>
		</a>
	</td>
	<td style="padding-left: 5px">
		<a class="noline" href="{{neurips_covid}}" target="_blank">How robust are estimates of Covid policies?</a> 

		<div class="journo">
		(2020),
			<i>NeurIPS</i> Spotlight
		</div>
		
		<span class="dropdown">
		  <button class="dropped" onclick="javascript:drop('covidneur')">The point</button>,
		  
		  <div id="covidneur" class="dropdown-content">
			<br>
			<b>Full title: How Robust are the Estimated Effects of Nonpharmaceutical Interventions against COVID-19? </b>
			<br><br>
			COVID-19 policy studies mostly don't do proper validation - very few papers check their performance on holdout data, and the sensitivity checks they perform are usually really limited.<br><br>
			
			We re-ran one of <a href="{{flax}}">the famous models</a>, and several variations of our own, and found that the famous model's results depend quite a lot on analysis decisions (ours is a bit more robust).<br><br>

			Also a couple theorems about how to interpret the effects: it's not the unconditional effect of doing policy p, it's the average additional effect of p, if you implement it alongside average existing policies (the average in your dataset).<br><br>

			<i>Authors</i>: Mrinank Sharma, Sören Mindermann, Jan M. Brauner, <span class="me">Gavin Leech</span>, Anna B. Stephenson, Tomáš Gavenciak, Jan Kulveit, Yee Whye Teh, Leonid Chindelevitch, Yarin Gal.
			<br><br>
			<i>Total hours I contributed</i>: 120.<br>
			<br>
		  </div>
		</span>
		<a href="{{neurtwitmrin}}">explainer</a>, <a href="{{robustvid}}">video</a>
	</td>
</tr>


<script>  
	var src="/img/papers/1.png"; 
    definiteEvent( createImg, [src, "{{id}}"] ); 
</script>