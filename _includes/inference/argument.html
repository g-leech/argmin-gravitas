<ol>
	<li>In statistics, "inference" means determining unobserved parameters θ from observed data X </li>
	<!-- (e.g., fitting a model to data: training). -->
	<li>In modern ML usage (post-2012), "inference" means computing outputs Y from fixed parameters θ </li>
	<!-- (e.g., running a trained model on new inputs). -->
	<li>These meanings are nearly opposite: statistical inference discovers unknowns from knowns, while ML inference generates outputs from knowns.</li>
	<li>The new usage spread from industry (Google's DistBelief and NVIDIA marketing).</li>
	<li>But this shift was unnecessary — lots of alternatives existed ("prediction", "evaluation", "forward pass", "apply"). It just sounds cooler I guess.</li>
	<li>"Inference" is now ambiguous between discovering unknowns from knowns (stats) or generating outputs from knowns.</li>
</ol>
