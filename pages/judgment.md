---
layout:     page
title:      My judgment
permalink:  /forecasting/
visible:    false
---

{%  assign book = "https://predictionbook.com/users/technicalities"    %}
{%  assign a2023 = "https://twitter.com/g_leech_/status/1769653432174920076"    %}


One way of checking if someone is capable of good judgment about latent variables (i.e. how good their model of the world is) is to see how they do on observable variables. ("judgmental forecasting")

<br>

<a href="{{a2023}}">I did well</a> on the 2023 ACX Forecasting Contest: 89th percentile. (The median single superforecaster was only 70th, and the Manifold aggregate of 150 nerds was also 89th.) Not totally obvious what the scales mean in absolute terms: ~0 overconfidence might mean _very_ good calibration.

<br>

You can also see my 2017 performance <a href="{{book}}">here</a> (with optional stopping though). 65% accurate, and good calibration (where 0 is perfect). But Brier scores are meaningless without a sense of the difficulty of the question set or a group of people doing the same questions to give a relative view. I didn't consciously pick easy ones but you'll have to judge for yourself.



