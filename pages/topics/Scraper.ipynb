{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "767ef790-dd7e-424d-8b74-55ff80ef6a14",
   "metadata": {},
   "source": [
    "## Notes\n",
    "There's a pattern of workshop data from year 2017 to 2022, can be scraped by incrementing the year in the following url:\n",
    "https://icml.cc/Conferences/2017/Schedule?type=Workshop to https://icml.cc/Conferences/2022/Schedule?type=Workshop\n",
    "After which the format of the website changes and so new code will have to be written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a5de133f-77a3-4b56-b5ef-7239f9832c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c378de10-933a-450e-8231-7647c177e9ac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fields_title = ['title', 'year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7330d160-99d9-4190-9a4a-6616b17297a6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://icml.cc/Conferences/2017/Schedule?type=Workshop 17\n",
      "ML on a budget: IoT, Mobile and other tiny-ML applications\n",
      "Principled Approaches to Deep Learning\n",
      "Video Games and Machine Learning\n",
      "Learning to Generate Natural Language\n",
      "Workshop on Human Interpretability in Machine Learning (WHI)\n",
      "Implicit Generative Models\n",
      "Lifelong Learning: A Reinforcement Learning Approach\n",
      "Automatic Machine Learning (AutoML 2017)\n",
      "Workshop on Computational Biology\n",
      "Workshop on Visualization for Deep Learning\n",
      "ICML Workshop on Machine Learning for Autonomous Vehicles 2017\n",
      "Interactive Machine Learning and Semantic Information Retrieval\n",
      "Picky Learners: Choosing Alternative Ways to Process Data.\n",
      "Time Series Workshop\n",
      "Human in the Loop Machine Learning\n",
      "Reinforcement Learning Workshop\n",
      "Private and Secure Machine Learning\n",
      "Machine Learning for Music Discovery\n",
      "Reliable Machine Learning in the Wild\n",
      "Machine Learning in Speech and Language Processing\n",
      "Reproducibility in Machine Learning Research\n",
      "Deep Structured Prediction\n",
      "https://icml.cc/Conferences/2018/Schedule?type=Workshop 18\n",
      "31st International Workshop on Qualitative Reasoning (QR 2018)\n",
      "6th Goal Reasoning Workshop\n",
      "Computer Games Workshop\n",
      "FCA4AI 2018\n",
      "Joint Workshop on AI in Health (day 1)\n",
      "Linguistic and Cognitive Approaches To Dialog Agents (LaCATODA 2018)\n",
      "Tenth International Workshop Modelling and Reasoning in Context (MRC)\n",
      "The 3rd International workshop on biomedical informatics with optimization and machine learning (BOOM)\n",
      "The 3rd International Workshop on Knowledge Discovery in Healthcare Data\n",
      "Towards learning with limited labels: Equivariance, Invariance, and Beyond\n",
      "Fairness, Interpretability, and Explainability Federation of Workshops (day 1)\n",
      "Autonomy in Teams -- Joint Workshop on Sharing Autonomy in Human-Robot Interaction\n",
      "Learning and Reasoning: Principles & Applications to Everyday Spatial and Temporal Knowledge (day 1)\n",
      "Cognitive Vision: Integrated Vision and AI for Embodied Perception and Interaction\n",
      "Joint Workshop on AI in Health (day 2)\n",
      "Learning and Reasoning: Principles & Applications to Everyday Spatial and Temporal Knowledge (day 2)\n",
      "Fairness, Interpretability, and Explainability Federation of Workshops (day 2-3) (day 1)\n",
      "10th International Workshop on Agents in Traffic and Transportation (ATT 2018)\n",
      "19th International Workshop on Multi-Agent-Based Simulation (MABS 2018)\n",
      "Adaptive and Learning Agents 2018 (day 1)\n",
      "AI and Computational Psychology: Theories, Algorithms and Applications (CompPsy)\n",
      "AI-MHRI (AI for Multimodal Human-Robot Interaction) (day 1)\n",
      "ALAW - Agents Living in Augmented Worlds\n",
      "AutoML 2018\n",
      "Bridging the Gap between Human and Automated Reasoning\n",
      "Data Science meets Optimization\n",
      "Eighth International Workshop on Statistical Relational AI\n",
      "Enabling Reproducibility in Machine Learning MLTrain@RML\n",
      "Engineering Multi-Agent Systems (day 1)\n",
      "First international workshop on socio-cognitive systems\n",
      "Game-Theoretic Mechanisms for Data and Information\n",
      "Goal Specifications for Reinforcement Learning\n",
      "International Workshop on Automated Negotiation (ACAN)\n",
      "International Workshop on Optimization in Multi-Agent Systems (OptMAS)\n",
      "Joint ICML and IJCAI Workshop on Computational Biology 2018\n",
      "Lifelong Learning: A Reinforcement Learning Approach\n",
      "Modern Trends in Nonconvex Optimization for Machine Learning\n",
      "The 2018 Joint Workshop on Machine Learning for Music\n",
      "The AAMAS-IJCAI Workshop on Agents and Incentives in Artificial Intelligence (day 1)\n",
      "Theoretical Foundations and Applications of Deep Generative Models (day 1)\n",
      "Theory of Deep Learning\n",
      "TRUST Workshop\n",
      "Domain Adaptation for Visual Understanding\n",
      "International Workshop on Massively Multi-Agent Systems\n",
      "Workshop on Efficient Credit Assignment in Deep Learning and Deep Reinforcement Learning (ECA) (day 1)\n",
      "Artificial Intelligence for Knowledge Management\n",
      "Prediction and Generative Modeling in Reinforcement Learning\n",
      "Privacy in Machine Learning and Artificial Intelligence (PiMLAI)\n",
      "Fairness, Interpretability, and Explainability Federation of Workshops (day 2-3) (day 2)\n",
      "International Workshop on Real Time compliant Multi-Agent Systems (RTcMAS)\n",
      "2nd International Joint Conference on Artificial Intelligence (IJCAI) Workshop on Artificial Intelligence in Affective Computing\n",
      "ABMUS-18 - Agent-Based Modelling of Urban Systems\n",
      "Adaptive and Learning Agents 2018 (day 2)\n",
      "AI for Aging, Rehabilitation and Independent Assisted Living (ARIAL) and Intelligent Conversation Agents in Home and Geriatric Care Applications\n",
      "AI for Synthetic Biology 2\n",
      "AI-MHRI (AI for Multimodal Human-Robot Interaction) (day 2)\n",
      "Architectures and Evaluation for Generality, Autonomy and Progress in AI (AEGAP)\n",
      "Engineering Multi-Agent Systems (day 2)\n",
      "Exploration in Reinforcement Learning\n",
      "Federated AI for Robotics Workshop (F-Rob-2018)\n",
      "First Workshop on Deep Learning for Safety-Critical in Engineering Systems\n",
      "Geometry in Machine Learning (GiMLi)\n",
      "Humanizing AI (HAI)\n",
      "Joint Workshop on Multimedia for Cooking and Eating Activities and Multimedia Assisted Dietary Management (CEA/MADiMa2018)\n",
      "Machine learning for Causal Inference, Counterfactual Prediction, and Autonomous Action (CausalML)\n",
      "Machine Learning: The Debates\n",
      "Neural Abstract Machines & Program Induction Workshop v2.0 (NAMPI_v2)\n",
      "Planning and Learning (PAL-18)\n",
      "The AAMAS-IJCAI Workshop on Agents and Incentives in Artificial Intelligence (day 2)\n",
      "The Joint International Workshop on Social Influence Analysis and Mining Actionable Insights from Social Networks (SocInf+MAISoN 2018)\n",
      "Theoretical Foundations and Applications of Deep Generative Models (day 2)\n",
      "Tractable Probabilistic Models\n",
      "Workshop on Efficient Credit Assignment in Deep Learning and Deep Reinforcement Learning (ECA) (day 2)\n",
      "Artificial Intelligence for Wildlife Conservation (AIWC) Workshop\n",
      "Workshop on AI for Internet of Things\n",
      "https://icml.cc/Conferences/2019/Schedule?type=Workshop 19\n",
      "ICML 2019 Workshop on Computational Biology\n",
      "ICML 2019 Time Series Workshop\n",
      "Workshop on the Security and Privacy of Machine Learning\n",
      "Joint Workshop on On-Device Machine Learning & Compact Deep Neural Network Representations (ODML-CDNNR)\n",
      "Human In the Loop Learning (HILL)\n",
      "Understanding and Improving Generalization in Deep Learning\n",
      "Uncertainty and Robustness in Deep Learning\n",
      "Reinforcement Learning for Real Life\n",
      "Negative Dependence: Theory and Applications in Machine Learning\n",
      "Generative Modeling and Model-Based Reasoning for Robotics and AI\n",
      "The Third Workshop On Tractable Probabilistic Modeling (TPM)\n",
      "Climate Change: How Can AI Help?\n",
      "Theoretical Physics for Deep Learning\n",
      "AI in Finance: Applications and Infrastructure for Multi-Agent Learning\n",
      "6th ICML Workshop on Automated Machine Learning (AutoML 2019)\n",
      "Real-world Sequential Decision Making: Reinforcement Learning and Beyond\n",
      "Stein’s Method for Machine Learning and Statistics\n",
      "AI For Social Good (AISG)\n",
      "Exploration in Reinforcement Learning Workshop\n",
      "Synthetic Realities: Deep Learning for Detecting AudioVisual Fakes\n",
      "Workshop on Multi-Task and Lifelong Reinforcement Learning\n",
      "Workshop on AI for autonomous driving\n",
      "Learning and Reasoning with Graph-Structured Representations\n",
      "Adaptive and Multitask Learning: Algorithms & Systems\n",
      "Identifying and Understanding Deep Learning Phenomena\n",
      "ICML Workshop on Imitation, Intent, and Interaction (I3)\n",
      "Machine Learning for Music Discovery\n",
      "Workshop on Self-Supervised Learning\n",
      "Coding Theory For Large-scale Machine Learning\n",
      "Invertible Neural Networks and Normalizing Flows\n",
      "The How2 Challenge: New Tasks for Vision & Language\n",
      "https://icml.cc/Conferences/2020/Schedule?type=Workshop 20\n",
      "Graph Representation Learning and Beyond (GRL+)\n",
      "XXAI: Extending Explainable AI Beyond Deep Models and Classifiers\n",
      "Self-supervision in Audio and Speech\n",
      "5th ICML Workshop on Human Interpretability in Machine Learning (WHI)\n",
      "Law & Machine Learning\n",
      "Learning with Missing Values\n",
      "Healthcare Systems, Population Health, and the Role of Health-tech\n",
      "Workshop on AI for Autonomous Driving (AIAD)\n",
      "Challenges in Deploying and Monitoring Machine Learning Systems\n",
      "MLRetrospectives: A Venue for Self-Reflection in ML Research\n",
      "ICML 2020 Workshop on Computational Biology\n",
      "Workshop on eXtreme Classification: Theory and Applications\n",
      "Participatory Approaches to Machine Learning\n",
      "Workshop on Continual Learning\n",
      "Object-Oriented Learning: Perception, Representation, and Reasoning\n",
      "Theoretical Foundations of Reinforcement Learning\n",
      "ML Interpretability for Scientific Discovery\n",
      "Uncertainty and Robustness in Deep Learning Workshop (UDL)\n",
      "Beyond first order methods in machine learning systems\n",
      "4th Lifelong Learning Workshop\n",
      "INNF+: Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models\n",
      "Inductive Biases, Invariances and Generalization in Reinforcement Learning\n",
      "Negative Dependence and Submodularity: Theory and Applications in Machine Learning\n",
      "Machine Learning for Global Health\n",
      "Federated Learning for User Privacy and Data Confidentiality\n",
      "Bridge Between Perception and Reasoning: Graph Neural Networks & Beyond\n",
      "Economics of privacy and data labor\n",
      "7th ICML Workshop on Automated Machine Learning (AutoML 2020)\n",
      "Real World Experiment Design and Active Learning\n",
      "1st Workshop on Language in Reinforcement Learning (LaReL)\n",
      "Workshop on Learning in Artificial Open Worlds\n",
      "Incentives in Machine Learning\n",
      "Machine Learning for Media Discovery\n",
      "2nd ICML Workshop on Human in the Loop Learning (HILL)\n",
      "https://icml.cc/Conferences/2021/Schedule?type=Workshop 21\n",
      "Challenges in Deploying and monitoring Machine Learning Systems\n",
      "INNF+: Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models\n",
      "Theory and Foundation of Continual Learning\n",
      "Tackling Climate Change with Machine Learning\n",
      "ICML Workshop on Theoretic Foundation, Criticism, and Application Trend of Explainable AI\n",
      "ICML 2021 Workshop on Unsupervised Reinforcement Learning\n",
      "Human-AI Collaboration in Sequential Decision-Making\n",
      "ICML Workshop on Representation Learning for Finance and E-Commerce Applications\n",
      "Reinforcement Learning for Real Life\n",
      "Uncertainty and Robustness in Deep Learning\n",
      "8th ICML Workshop on Automated Machine Learning (AutoML 2021)\n",
      "Interpretable Machine Learning in Healthcare\n",
      "Theory and Practice of Differential Privacy\n",
      "Machine Learning for Data: Automated Creation, Privacy, Bias\n",
      "The Neglected Assumptions In Causal Inference\n",
      "ICML Workshop on Human in the Loop Learning (HILL)\n",
      "ICML Workshop on Algorithmic Recourse\n",
      "A Blessing in Disguise: The Prospects and Perils of Adversarial Machine Learning\n",
      "International Workshop on Federated Learning for User Privacy and Data Confidentiality in Conjunction with ICML 2021 (FL-ICML'21)\n",
      "Workshop on Socially Responsible Machine Learning\n",
      "ICML 2021 Workshop on Computational Biology\n",
      "Subset Selection in Machine Learning: From Theory to Applications\n",
      "Workshop on Computational Approaches to Mental Health @ ICML 2021\n",
      "Beyond first-order methods in machine learning systems\n",
      "Information-Theoretic Methods for Rigorous, Responsible, and Reliable Machine Learning (ITR3)\n",
      "Workshop on Distribution-Free Uncertainty Quantification\n",
      "Self-Supervised Learning for Reasoning and Perception\n",
      "Time Series Workshop\n",
      "Over-parameterization: Pitfalls and Opportunities\n",
      "Workshop on Reinforcement Learning Theory\n",
      "https://icml.cc/Conferences/2022/Schedule?type=Workshop 22\n",
      "My ML Workshop [EXAMPLE]\n",
      "The 1st Workshop on Healthcare AI and COVID-19\n",
      "ICML 2022 Workshop on Computational Biology\n",
      "Adaptive Experimental Design and Active Learning in the Real World\n",
      "Spurious correlations, Invariance, and Stability (SCIS)\n",
      "Machine Learning for Astrophysics\n",
      "Workshop on Formal Verification of Machine Learning\n",
      "ICML workshop on Machine Learning for Cybersecurity (ICML-ML4Cyber)\n",
      "Knowledge Retrieval and Language Models\n",
      "DataPerf: Benchmarking Data for Data-Centric AI\n",
      "Topology, Algebra, and Geometry in Machine Learning (TAG-ML)\n",
      "Beyond Bayes: Paths Towards Universal Reasoning Systems\n",
      "New Frontiers in Adversarial Machine Learning\n",
      "1st ICML 2022 Workshop on Safe Learning for Autonomous Driving (SL4AD)\n",
      "Machine Learning for Audio Synthesis\n",
      "Theory and Practice of Differential Privacy\n",
      "Workshop on Machine Learning in Computational Design\n",
      "Decision Awareness in Reinforcement Learning\n",
      "Shift happens: Crowdsourcing metrics and test datasets beyond ImageNet\n",
      "Dynamic Neural Networks\n",
      "AI for Agent-Based Modelling (AI4ABM)\n",
      "Complex feedback in online learning\n",
      "Hardware-aware efficient training (HAET)\n",
      "The First Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward\n",
      "Workshop on Human-Machine Collaboration and Teaming\n",
      "Updatable Machine Learning\n",
      "The ICML Expressive Vocalizations (ExVo) Workshop and Competition 2022\n",
      "Responsible Decision Making in Dynamic Environments\n",
      "Principles of Distribution Shift (PODS)\n",
      "AI for Science\n",
      "Continuous Time Perspectives in Machine Learning\n",
      "Disinformation Countermeasures and Machine Learning (DisCoML)\n",
      "2nd Workshop on Interpretable Machine Learning in Healthcare (IMLH)\n",
      "Workshop on Distribution-Free Uncertainty Quantification\n"
     ]
    }
   ],
   "source": [
    "for year in range(17,23):\n",
    "    \n",
    "    URL = f'https://icml.cc/Conferences/20{year}/Schedule?type=Workshop'\n",
    "\n",
    "    print(URL, year)\n",
    "    \n",
    "    req = requests.get(URL)\n",
    "    soup = bs(req.text, 'html.parser')\n",
    "      \n",
    "    titles = soup.find_all('div',attrs = {'class','maincardBody'})\n",
    "    \n",
    "    with open('titles_icml', 'a') as f:\n",
    "        \n",
    "        write = csv.writer(f)\n",
    "        \n",
    "        #write.writerow(fields_title)\n",
    "        for titleNumber in range(0,len(titles)):\n",
    "            print(titles[titleNumber].text)\n",
    "            \n",
    "            write.writerow([titles[titleNumber].text, f'20{year}'])\n",
    "  \n",
    "    #print(titles[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "59548c36-dcb7-4718-b6c2-903d8aeac71e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My ML Workshop [EXAMPLE]\n"
     ]
    }
   ],
   "source": [
    "print(titles[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b24f1ac-e22e-44f8-9b21-291c9656f7d8",
   "metadata": {},
   "source": [
    "## Scraping abstracts\n",
    "Annoyingly there's no obvious pattern in the website for the different abstracts, so a manual inspect of each html element must be performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "02c2cf24-bd75-4634-a042-f09789268401",
   "metadata": {},
   "outputs": [],
   "source": [
    "array17 = [[2017], [*range(1,22),930]]\n",
    "array18 = [[2018], [*range(3280, 3353)]]\n",
    "array19 = [[2019], [*range(3502, 3533)]]\n",
    "array20 = [[2020],[*range(5715, 5749)]]\n",
    "array21 = [[2021],[*range(8347, 8377)]]\n",
    "array22 = [[2022], [*range(13446, 13479),21435]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f233fb95-0f8c-4428-94a3-67b740cb4ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "allArrays = [array17, array18, array19, array20, array21, array22]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230c543a-f56a-44ff-9c79-c9463765c61e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "### After manually inspecting each year to assertain the eventID we iterate over each year and list of eventIDs to produce a list of abstracts, sadly there are no abstracts for year 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5c4cd4c5-fe4d-4fef-9278-8018697ccb59",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://icml.cc/Conferences/2017/Schedule?showEvent=1 2017\n",
      "Machine learning has achieved considerable successes in recent years and an ever-growing number of disciplines rely on it. However, this success crucially relies on human machine learning experts, who select appropriate features, workflows, machine learning paradigms, algorithms, and their hyperparameters. As the complexity of these tasks is often beyond non-experts, the rapid growth of machine learning applications has created a demand for off-the-shelf machine learning methods that can be used easily and without expert knowledge. We call the resulting research area that targets progressive automation of machine learning AutoML.\n",
      "\n",
      "https://icml.cc/Conferences/2017/Schedule?showEvent=2 2017\n",
      "In recent years, deep learning has revolutionized machine learning. Most successful applications of deep learning involve predicting single variables (e.g., univariate regression or multi-class classification). However, many real problems involve highly dependent, structured variables. In such scenarios, it is desired or even necessary to model correlations and dependencies between the multiple input and output variables. Such problems arise in a wide range of domains, from natural language processing, computer vision, computational biology and others. Some approaches to these problems directly use deep learning concepts, such as those that generate sequences using recurrent neural networks or that output image segmentations through convolutions. Others adapt the concepts from structured output learning. These structured output prediction problems were traditionally handled using linear models and hand-crafted features, with a structured optimization such as inference. It has recently been proposed to combine the representational power of deep neural networks with modeling variable dependence in a structured prediction framework. There are numerous interesting research questions related to modeling and optimization that arise in this problem space.This workshop will bring together experts in machine learning and application domains whose research focuses on combining deep learning and structured models. Specifically, we aim to provide an overview of existing approaches from various domains to distill from their success principles that can be more generally applicable. We will also discuss the main challenges that arise in this setting and outline potential directions for future progress. The target audience consists of researchers and practitioners in machine learning and application areas.\n",
      "\n",
      "https://icml.cc/Conferences/2017/Schedule?showEvent=3 2017\n",
      "For details see:http://machlearn.gitlab.io/hitl2017/As machine learning systems become more ubiquitous in everybody’s day-to-day life or work, society and industry is in an intermediate state between fully manual and fully automatic systems. The gradient undoubtedly points towards full automation, but moving forward in this direction is going to face increasing challenges due to the fact that current machine learning research tends to focus on end to end systems, which puts aside the fact that for practical applications there are still gaps or caveats in the automation. Parts of these come from the presence of (or the necessity to have) the Human in the Loop.There are two main locations for the Human in the automated system: (i) upstream, in which case the focus is mainly  in the inputs of the algorithm. This can be essential for personalised assistants, that describe environments where the machine learning method is tightly embedded into the system. Such environments pose additional challenges related  to privacy at large; (ii) downstream: other domains have machine learning approaches analyse parts of the data, and human experts use the results and intuition to make decisions. The Human dependences between these two locations is also neither straightforward nor acyclic — some applications tend  to have feedback effects on data as actions or interventions are undertaken based on machine learning predictions. Furthermore there are often very few rounds of decision making in practice, but each round may affect the statement of the problems related to the Human presence, as witnessed for example by eventual privacy leakages.This workshop aims to bring together people who are working on systems where machine learning is only part of the solution. Participants will exchange ideas and experiences on human in the loop machine learning.Topics of interest include:- System architectures that allow for human decision making- User interfaces for interacting with machine learning systems- Validation of human in the loop software systems- Viewpoints from traditional fields such as reinforcement learning and Bayesian optimisation- Challenges related to the human presence in the loop (privacy, bias, fairness, etc.)- Case studies of deployed machine learning\n",
      "\n",
      "https://icml.cc/Conferences/2017/Schedule?showEvent=4 2017\n",
      "Although dramatic progress has been made in the field of autonomous driving, there are many major challenges in achieving full-autonomy. For example, how to make perception accurate and robust to accomplish safe autonomous driving? How to reliably track cars, pedestrians, and cyclists? How to learn long term driving strategies (known as driving policies) so that autonomous vehicles can be equipped with adaptive human negotiation skills when merging, overtaking and giving way, etc? How to achieve near-zero fatality?These complex challenges associated with autonomy in physical world naturally suggest that we take a machine learning approach. Deep learning and computer vision have found many real-world applications such as face tagging. However, perception for autonomous driving has a unique set of requirements such as safety and explainability. Autonomous vehicles need to choose actions, e.g. steering commands which will affect the subsequent inputs (driving scenes) encountered. This setting is well-suited to apply reinforcement learning to determine the best actions to take. Many autonomous driving tasks such as perception and tracking requires large data sets of labeled examples to learn rich and high-performance visual representation. However, the progress is hampered by the sheer expenses of human labelling needed. Naturally we would like to employ unsupervised learning, transfer learning leveraging simulators, and techniques can learn efficiently.The goal of this workshop is to bring together researchers and practitioners from in the field of autonomous driving to address core challenges with machine learning. These challenges include, but are not limited toaccurate and efficient pedestrian detection, pedestrian intent detection,machine learning for object tracking,unsupervised representation learning for autonomous driving,deep reinforcement learning for learning driving policies,cross-modal and simulator to real-world transfer learning, scene classification, real-time perception and prediction of traffic scenes,uncertainty propagation in deep neural networks,efficient inference with deep neural networksThe workshop will include invited speakers, panels, presentations of accepted papers and posters. We invite papers in the form of short, long and position papers to address the core challenges mentioned above. We encourage researchers and practitioners on self-driving cars, transportation systems and ride-sharing platforms to participate. Since this is a topic of broad and current interest, we expect at least 200 participants from leading university researchers, auto-companies and ride-sharing companies.\n",
      "\n",
      "https://icml.cc/Conferences/2017/Schedule?showEvent=5 2017\n",
      "Probabilistic models are a central implement in machine learning practice. They form the basis for models that generate realistic data, uncover hidden structure, and make predictions. Traditionally, probabilistic models in machine learning have focused on prescribed models. Prescribed models specify a joint density over observed and hidden variables that can be easily evaluated. The requirement of a tractable density simplifies their learning but limits their flexibility --- several real world phenomena are better described by simulators that do not admit a tractable density. Probabilistic models defined only via the simulations they produce are called implicit models.Arguably starting with generative adversarial networks, research on implicit models in machine learning has exploded in recent years. This workshop’s aim is to foster a discussion around the recent developments and future directions of implicit models.Implicit models have many applications. They are used in ecology where models simulate animal populations over time; they are used in phylogeny, where simulations produce hypothetical ancestry trees; they are used in physics to generate particle simulations for high energy processes. Recently, implicit models have been used to improve the state-of-the-art in image and content generation. Part of the workshop’s focus is to discuss the commonalities among applications of implicit models.Of particular interest at this workshop is to unite fields that work on implicit models. For example:+ Generative adversarial networks (a NIPS 2016 workshop) are implicit models with an adversarial training scheme.+ Recent advances in variational inference (a NIPS 2015 and 2016 workshop) have leveraged implicit models for more accurate approximations.+ Approximate Bayesian computation (a NIPS 2015 workshop) focuses on posterior inference for models with implicit likelihoods.+ Learning implicit models is deeply connected to two sample testing and density ratio estimation.We hope to bring together these different views on implicit models, identifying their core challenges and combining their innovations.We invite submission of 4 page papers for posters, contributed talks, and travel awards. Topics of interests are: implicit models, approximate Bayesian computation, generative adversarial networks, learning and inference for implicit models, implicit variational approximations, evaluation of implicit models and two sample testing. We encourage both theoretical and applied submissions.\n",
      "\n",
      "https://icml.cc/Conferences/2017/Schedule?showEvent=6 2017\n",
      "Retrieval techniques operating on text or semantic annotations have become the industry standard for retrieval from large document collections. However, traditional information retrieval techniques operate on the assumption that the user issues a single query and the system responds with a ranked list of documents. In recent years we have witnessed a substantial growth in text data coming from various online resources, such as online newspapers, blogs, specialised document collections (e.g. arXiv). Traditional information retrieval approaches often fail to provide users with adequate support when browsing such online resources, hence in recent years there has been a growing interest in developing new algorithms and design methods that can support interactive information retrieval. The aim of this workshop is to explore new methods and related system design for interactive data analytics and management in various domains, including specialised text collections (e.g. legal, medical, scientific) as well as for various tasks, such as semantic information retrieval, conceptual organization and clustering of data collections for sense making, semantic expert profiling, and document recommender systems.Of interest, also, is probabilistic and machine learning formulations of the interactive information retrieval task above and beyond the simple \"stochastic language models\" framework developed in the information retrieval community.The primary audience of the workshop are researchers and practitioners in the area of interactive and personalised system design as well as interactive machine learning both from academia and industry.\n",
      "\n",
      "https://icml.cc/Conferences/2017/Schedule?showEvent=7 2017\n",
      "Research on natural language generation is rapidly growing due to the increasing demand for human-machine communication in natural language. This workshop aims to promote the discussion, exchange, and dissemination of ideas on the topic of text generation, touching several important aspects in this modality: learning schemes and evaluation,  model design and structures, advanced decoding strategies, and natural language generation applications. This workshop aims to be a venue for the exchange of ideas regarding data-driven machine learning approaches for text generation, including mainstream tasks such as dialogue generation, instruction generation, and summarization; and for establishing new directions and ideas with potential for impact in the fields of machine learning, deep learning, and NLP.\n",
      "\n",
      "https://icml.cc/Conferences/2017/Schedule?showEvent=8 2017\n",
      "One of the most challenging and open problems in Artificial Intelligence (AI) is that of Lifelong Learning:​“Lifelong Learning is the continued learning of tasks, from one or more domains, over the course of a lifetime, by a lifelong learning system. A lifelong learning system efficiently and effectively (1) retains the knowledge it has learned; (2) selectively transfers knowledge to learn new tasks; and (3) ensures the effective and efficient interaction between (1) and (2).”Lifelong learning is still in its infancy. Many issues currently exist such as learning general representations, catastrophic forgetting, efficient knowledge retention mechanisms and hierarchical abstractions.  Much work has been done in the Reinforcement Learning (RL) community to tackle different elements of lifelong learning. Active research topics include hierarchical abstractions, transfer learning, multi-task learning and curriculum learning. With the emergence of powerful function approximators such as in Deep Learning, we feel that now is a perfect time to provide a forum to discuss ways to move forward and provide a truly general lifelong learning framework, using RL-based algorithms, with more rigour than ever before. This workshop will endeavour to promote interaction between researchers working on the different elements of lifelong learning to try and find a synergy between the various techniques.\n",
      "\n",
      "https://icml.cc/Conferences/2017/Schedule?showEvent=9 2017\n",
      "The ever-increasing size and accessibility of vast music libraries has created a demand more than ever for machine learning systems that are capable of understanding and organizing this complex data.  While this topic has received relatively little attention within the machine learning community, it has been an area of intense focus within the community of Music Information Retrieval (MIR), where significant progress has been made, but these problems remain far from solved.Furthermore, the recommender systems community has made great progress in terms of collaborative feedback recommenders, but these approaches suffer strongly from the cold-start problem.  As such, recommendation techniques often fall back on content-based machine learning systems, but defining musical similarity is extremely challenging as myriad features all play some role (e.g., cultural, emotional, timbral, rhythmic).We seek to use this workshop to bring together a group of world-class experts to discuss these challenges and share them with the greater machine learning community.  In addition to making progress on these challenges, we hope to engage the machine learning community with our nebulous problem space, and connect them with the many available datasets the MIR community has to offer (e.g., AcousticBrainz, Million Song Dataset), which offer near commercial scale to the academic research community.\n",
      "\n",
      "https://icml.cc/Conferences/2017/Schedule?showEvent=10 2017\n",
      "This workshop continues a tradition of MLSLP workshops held as satellites of ICML, ACL, and Interspeech conferences.  While research in speech and language processing has always involved machine learning (ML), current research is benefiting from even closer interaction between these fields.  Speech and language processing is continually mining new ideas from ML and ML, in turn, is devoting more interest to speech and language applications.  This workshop is a venue for locating and incubating the next waves of research directions for interaction and collaboration.  The workshop will (1) discuss emerging research ideas with potential for impact in speech/language and (2) bring together relevant researchers from ML and speech/language who may not regularly interact at conferences.  Example topics include new directions for deep learning in speech/language, reinforcement learning, unsupervised/semi-supervised learning, domain adaptation/transfer learning, and topics at the boundary of speech, text, and other modalities.\n",
      "\n",
      "https://icml.cc/Conferences/2017/Schedule?showEvent=11 2017\n",
      "We routinely encounter scenarios where at test-time we must predict on a budget. Feature costs in Internet, Healthcare, and Surveillance applications arise due to feature extraction time and feature/sensor acquisition~\\cite{trapeznikov:2013b} costs. Data analytics applications in mobile devices are often performed on remote cloud services due to the limited device capabilities, which imposes memory/prediction time costs. Naturally, in these settings, one needs to carefully understand the trade-off between accuracy and prediction cost. Uncertainty in the observations, which is typical in such scenarios, further adds to complexity of the task and requires a careful understanding of both the uncertainty as well as accuracy-cost tradeoffs. In this workshop, we aim to bring together researchers from various domains to discuss the key aspects of the above mentioned emerging and critical topic. The goal is to provide a platform where ML/statistics/optimization researchers can interact closely with domain experts who need to deploy ML models in resource-constrained settings (like an IoT device maker), and chart out the foundational problems in the area and key tools that can be used to solve them. Motivation===================Prediction under budget constraints is a critical problem that arise in several settings like medical diagnosis, search engines and surveillance. In these applications, budget constraints arise as a result of limits on computational cost, time, network-throughput and power-consumption. For instance, in search engines CPU cost during prediction-time must be budgeted to enable business models such as online advertising. Additionally, search engines have time constraints at prediction-time as users are known to abandon the service is the response time of the search engine is not within a few tens of milliseconds. In another example, modern passenger screening systems impose constraints on throughput. An extreme version of these problems appear in the Internet of Things (IoT) setting where one requires prediction on tiny IoT devices which might have at most 2KB of RAM and no floating point computation unit. IoT is considered to be the next multi-billion industry with “smart” devices being designed for production-line, cars, retail stores, and even for toothbrush and spoons. Given that IoT based solutions seem destined to significantly permeate our day-to-day lives,  ML based predictions on the device become critical due to several reasons like privacy, battery, latency etc.  Learning under resource constraints departs from the traditional machine learning setting and introduces new exciting challenges. For instance, features are accompanied by costs (e.g. extraction time in search engines or true monetary values in medical diagnosis) and their amortized sum is constrained at test-time. Also, different search strategies in prediction can have widely varying computational costs (e.g., binary search, linear search, dynamic programming). In other settings, a system must maintain a throughput constraint to keep pace with arriving traffic.In IoT setting, the model itself has to be deployed on a 2-16KB RAM, posing an extremely challenging constraint on the algorithm. The common aspect of all of these settings is that we must seeks trade-offs between prediction accuracy and prediction cost. Studying this tradeoff is an inherent challenge that needs to be investigated in a principled fashion in order to invent practically relevant machine learning algorithms. This problems lies at the intersection of ML, statistics, stochastic control and information theory. We aim to draw researchers working on foundational, algorithmic and application problems within these areas. We plan on organizing a demo session which would showcase ML algorithms running live on various resource-constrained device, demonstrating their effectiveness on challenging real-world tasks. In addition, we plan to invite Ofer Dekel from Microsoft Research to present a new platform for deploying ML on tiny devices which should provide a easy way to deploy and compare various ML techniques on realistic devices and further spur multiple research directions in this area.\n",
      "\n",
      "https://icml.cc/Conferences/2017/Schedule?showEvent=12 2017\n",
      "Picky Learners consists of a broad range of learning scenarios where the learner does not simply process every data point blindly, but instead can choose to incorporate them in alternative ways. Despite the growing costs of processing and labelling vast amounts of data, only isolated efforts have tackled this problem primarily in the areas of active learning, learning with rejection and on-line learning with feedback graphs.  In active learning, the learner can choose whether or not to query for a label of each data point, thereby paying different costs for each data point.  A key advantage in this setting is that the number of examples queried to learn a concept may be much smaller than the number of examples needed in standard supervised learning. More recently, some have used variations of confidence-based models to determine which labels to query.  Confidence-based models lie under the more general framework of learning with rejection, which is a key learning scenario where the algorithm can abstain from making a prediction, at the price of incurring a fixed cost. In this scenario, our picky learners can thus choose to abstain from providing a label. In the on-line setting, one can cast learning with rejection under the more general topic of on-line learning with feedback graphs, a setting that interpolates between bandit and full expert scenario in that the player observes a variety of different expert losses after choosing an action. On-line learning with feedback graphs can then in turn be connected back to active learning where depending on the feedback graph only certain labels are requested.In short, our picky learners can choose to query for the label (active learning), choose to abstain on the label (learning with rejection) or choose to receive different expert losses (on-line learning with feedback graphs). All of three of these fields attempt in different ways to reduce the cost of processing the data by allowing for picky learners, but the connections between these topics has not been fully explored in terms of both theory and practice. The goal of this workshop is then to bring together researchers and practitioners in these three areas in order to bridge the gap between active learning, learning with rejection, and on-line learning with feedback graphs. We expect that the fruitful collaborations started in this workshop will result in novel research that will help develop each field.\n",
      "\n",
      "https://icml.cc/Conferences/2017/Schedule?showEvent=13 2017\n",
      "The recent advancements in deep learning have revolutionized the field of machine learning, enabling unparalleled performance and many new real-world applications. Yet, the developments that led to this success have often been driven by empirical studies, and little is known about the theory behind some of the most successful approaches. While theoretically well-founded deep learning architectures had been proposed in the past, they came at a price of increased complexity and reduced tractability. Recently, we have witnessed considerable interest in principled deep learning. This led to a better theoretical understanding of existing architectures as well as development of more mature deep models with solid theoretical foundations. In this workshop, we intend to review the state of those developments and provide a platform for the exchange of ideas between the theoreticians and the practitioners of the growing deep learning community. Through a series of invited talks by the experts in the field, contributed presentations, and an interactive panel discussion, the workshop will cover recent theoretical developments, provide an overview of promising and mature architectures, highlight their challenges and unique benefits, and present the most exciting recent results.\n",
      "\n",
      "https://icml.cc/Conferences/2017/Schedule?showEvent=14 2017\n",
      "There are two complementary approaches to private and secure machine learning: differential privacy can guarantee privacy of the subjects of the training data with respect to the output of a differentially private learning algorithm, while cryptographic approaches can guarantee secure operation of the learning process in a potentially distributed environment. The aim of this workshop is to bring together researchers interested in private and secure machine learning, to stimulate interactions to advance either perspective or to combine them.\n",
      "\n",
      "https://icml.cc/Conferences/2017/Schedule?showEvent=15 2017\n",
      "When can we trust that a system that has performed well in the past will continue to do so in the future? Designing systems that are reliable in the wild is essential for high stakes applications such as self-driving cars and automated surgical assistants. This workshop aims to bring together researchers in diverse areas such as reinforcement learning, human-robot interaction, game theory, cognitive science, and security to further the field of reliability in machine learning. We will focus on three aspects — robustness (to adversaries, distributional shift, model misspecification, corrupted data); awareness (of when a change has occurred, when the model might be miscalibrated, etc.);and  adaptation (to new situations or objectives). We aim to consider each of these in the context of the complex human factors that impact the successful application or meaningful monitoring of any artificial intelligence technology. Together, these will aid us in designing and deploying reliable machine learning systems.\n",
      "\n",
      "https://icml.cc/Conferences/2017/Schedule?showEvent=16 2017\n",
      "This workshop focuses on issues of reproducibility and replication of results in the Machine Learning community. Papers from the Machine Learning community are supposed to be a valuable asset. They can help to inform and inspire future research. They can be a useful educational tool for students. They can give guidance to applied researchers in industry. Perhaps most importantly, they can help us to answer the most fundamental questions about our existence - what does it mean to learn and what does it mean to be human? Reproducibility, while not always possible in science (consider the study of a transient astrological phenomenon like a passing comet), is a powerful criteria for improving the quality of research. A result which is reproducible is more likely to be robust and meaningful and rules out many types of experimenter error (either fraud or accidental).There are many interesting open questions about how reproducibility issues intersect with the Machine Learning community:* How can we tell if papers in the Machine Learning community are reproducible even in theory? If a paper is about recommending news sites before a particular election, and the results come from running the system online in production - it will be impossible to reproduce the published results because the state of the world is irreversibly changed from when the experiment was ran.* What does it mean for a paper to be reproducible in theory but not in practice? For example, if a paper requires tens of thousands of GPUs to reproduce or a large closed-off dataset, then it can only be reproduced in reality by a few  large labs.* For papers which are reproducible both in theory and in practice - how can we ensure that papers published in ICML would actually be able to replicate if such an experiment were attempted?* What does it mean for a paper to have successful or unsuccessful replications?* Of the papers with attempted replications completed, how many have been published?* What can be done to ensure that as many papers which are reproducible in theory fall into the last category?* On the reproducibility issue, what can the Machine Learning community learn from other fields?Our aim in the following workshop is to raise the profile of these questions in the community and to search for their answers. In doing so we aim for papers focusing on the following topics:* Analysis of the current state of reproducibility in machine learning venues* Tools to help increase reproducibility* Evidence that reproducibility is important for science* Connections between the reproducibility situation in Machine Learning and other fields* Replications, both failed and successful, of influential papers in the Machine Learning literature.\n",
      "\n",
      "https://icml.cc/Conferences/2017/Schedule?showEvent=17 2017\n",
      "Time series data is ubiquitous. In domains as diverse as finance, entertainment, transportation and health-care, we observe a fundamental shift away from parsimonious, infrequent measurement to nearly continuous monitoring and recording. Rapid advances in diverse sensing technologies, ranging from remote sensors to wearables and social sensing, are generating a rapid growth in the size and complexity of time series archives. Thus, although time series analysis has been studied extensively, its importance only continues to grow. Furthermore, modern time series data pose significant challenges to existing techniques both in terms of the structure (e.g., irregular sampling in hospital records and spatiotemporal structure in climate data) and size. These challenges are compounded by the fact that standard i.i.d. assumptions used in other areas of machine learning are not appropriate for time series and new theory, models and algorithms are needed to process and analyse this data.The goal of this workshop is to bring together theoretical and applied researchers interested in the analysis of time series and development of new algorithms to process sequential data. This includes algorithms for time series prediction, classification, clustering, anomaly and change point detection, correlation discovery, dimensionality reduction as well as a general theory for learning and comparing stochastic processes. We invite researchers from the related areas of batch and online learning, reinforcement learning, data analysis and statistics, econometrics, and many others to contribute to this workshop. Our workshop will build on the success of past two time series workshops that were held at NIPS and KDD (also co-organized by the proposers). The workshop will attract a broader audience from ICML community. In particular, when we have the KDD workshop on time series in 2015 held in Sydney, it attracts many local researchers in Australia who work on time series research or related applications. We expect the proposed workshop will be a hit given its large interest in the ICML community as well as the local interest in Sydney.\n",
      "\n",
      "https://icml.cc/Conferences/2017/Schedule?showEvent=18 2017\n",
      "Good benchmarks are necessary for developing artificial intelligence. Recently, there has been a growing movement for the use of video games as machine learning benchmarks [1,2,3], and also an interest in the applications of machine learning from the video games community. While games have been used for AI research for a long time, only recently have we seen modern machine learning methods applied to video games.This workshop focuses on complex games which provide interesting and hard challenges for machine learning. Going beyond simple toy problems of the past, and games which can easily be solved with search, we focus on games where learning is likely to be necessary to play well. This includes strategy games such as StarCraft [4,5], open-world games such as MineCraft [6,7,8], first-person shooters such as Doom [9,10], as well as hard and unsolved 2D games such as Ms. Pac-Man and Montezuma's Revenge [11,12,13]. While we see most of the challenges in game-playing, there are also interesting machine learning challenges in modeling and content generation [14]. This workshop aims at bringing together all researchers from ICML who want to use video games as a benchmark. We will have talks by invited speakers from machine learning, from the game AI community, and from the video games industry.[1] Greg Brockman, Catherine Olsson, Alex Ray, et al. \"OpenAI Universe\", https://openai.com/blog/universe/ (2016).[2] Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Küttler, Andrew Lefrancq, Simon Green, Víctor Valdés, Amir Sadik, Julian Schrittwieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hassabis, Shane Legg, Stig Petersen, \"DeepMind Lab\", arXiv:1612.03801 (2016).[3] Gabriel Synnaeve, Nantas Nardelli, Alex Auvolat, Soumith Chintala, Timothée Lacroix, Zeming Lin, Florian Richoux, Nicolas Usunier, \"TorchCraft: a Library for Machine Learning Research on Real-Time Strategy Games\", arXiv:1611.00625 (2016).[4] Santiago Ontanon, Gabriel Synnaeve, Alberto Uriarte, Florian Richoux, David Churchill, Mike Preuss, \"A Survey of Real-Time Strategy Game AI Research and Competition in StarCraft\", IEEE Transactions on Computational Intelligence and AI in games 5.4 (2013): 293-311.[5] StarCraft AI Competition @ AIIDE 2016[6] Junhyuk Oh, Valliappa Chockalingam, Satinder Singh, and Honglak Lee, \"Control of Memory, Active Perception, and Action in Minecraft\", ICML (2016).[7] Chen Tessler, Shahar Givony, Tom Zahavy, Daniel J. Mankowitz, Shie Mannor, \"A Deep Hierarchical Approach to Lifelong Learning in Minecraft\", arXiv preprint arXiv:1604.07255 (2016).[8] Matthew Johnson, Katja Hofmann, Tim Hutton, David Bignell, \"The Malmo Platform for Artificial Intelligence Experimentation\", IJCAI (2016).[9] Visual Doom AI Competition @ CIG 2016[10] Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Tim Harley, Timothy P. Lillicrap, David Silver, Koray Kavukcuoglu, \"Asynchronous Methods for Deep Reinforcement Learning\", arXiv preprint arXiv:1602.01783 (2016).[11] Tejas D. Kulkarni, Karthik R. Narasimhan, Ardavan Saeedi, Joshua B. Tenenbaum, \"Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation\", arXiv prepint arXiv:1604.06057 (2016).[12] Marc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, Remi Munos, \"Unifying Count-Based Exploration and Intrinsic Motivation\", arXiv preprint arXiv:1606.01868 (2016).[13] Diego Perez-Liebana, Spyridon Samothrakis, Julian Togelius, Tom Schaul, Simon Lucas, \"General Video Game AI: Competition, Challenges and Opportunities\", AAAI (2016).[14] Julian Togelius, Georgios N. Yannakakis, Kenneth O. Stanley and Cameron Browne, \"Search-based Procedural Content Generation: a Taxonomy and Survey\". IEEE TCIAIG (2011).\n",
      "\n",
      "https://icml.cc/Conferences/2017/Schedule?showEvent=19 2017\n",
      "The workshop will showcase recent research in the field of Computational Biology. There has been significant development in genomic sequencing techniques as well as imaging technologies that not only generate huge amounts of data but provide unprecedented levels of resolution, that of a single cell and even subcellular resolution. This availability of high dimensional data, at multiple spatial and temporal resolutions and capturing several perspectives of biological phenomena has made machine learning methods increasingly relevant for computational analysis of the data. Conversely, biological data has also exposed unique challenges and problems that call for the development of new machine learning methods. This workshop aims at bringing in researchers working at the intersection of Machine Learning and Biology to present recent advances and open questions in computational biology to the ICML community.\n",
      "\n",
      "https://icml.cc/Conferences/2017/Schedule?showEvent=20 2017\n",
      "This workshop will bring together researchers who study the interpretability of predictive models, develop interpretable machine learning algorithms, and develop methodology to interpret black-box machine learning models (e.g., post-hoc interpretations).  This is a very exciting time to study interpretable machine learning, as the advances in large-scale optimization and Bayesian inference that have enabled the rise of black-box machine learning are now also starting to be exploited to develop principled approaches to large-scale interpretable machine learning. Participants in the workshop will exchange ideas on these and allied topics, including:●    Quantifying and axiomatizing interpretability●  Psychology of human concept learning●   Rule learning,Symbolic regression and case-based reasoning● Generalized additive models, sparsity and interpretability● Visual analytics●   Interpretable unsupervised models (clustering, topic models, e.t.c)●    Interpretation of black-box models (including deep neural networks)●    Causality of predictive models● Verifying, diagnosing and debugging machine learning systems●   Interpretability in reinforcement learning.Doctors, judges, business executives, and many other people are faced with making critical decisions that can have profound consequences. For example, doctors decide which treatment to administer to patients, judges decide on prison sentences for convicts, and business executives decide to enter new markets and acquire other companies. Such decisions are increasingly being supported by predictive models learned by algorithms from historical data.The latest trend in machine learning is to use highly nonlinear complex systems such as deep neural networks, kernel methods, and large ensembles of diverse classifiers. While such approaches often produce impressive, state-of-the art prediction accuracies, their black-box nature offers little comfort to decision makers. Therefore, in order for predictions to be adopted, trusted, and safely used by decision makers in mission-critical applications, it is imperative to develop machine learning methods that produce interpretable models with excellent predictive accuracy.  It is in this way that machine learning methods can have impact on consequential real-world applications.\n",
      "\n",
      "https://icml.cc/Conferences/2017/Schedule?showEvent=21 2017\n",
      "Deep networks have had profound impact across machine learning research and in many application areas. DNNs are complex to design and train. They are non-linear systems that almost always have many local optima and are often sensitive to training parameter settings and initial state. Systematic optimization of structure and hyperparameters is possible e.g. with Bayesian optimization, but hampered by the expense of training each design on realistic datasets. Exploration is still ongoing for best design principles. We argue that visualization can play an essential role in understanding DNNs and in developing new design principles. With rich tools for visual exploration of networks during training and inference, one should be able to form closer ties between theory and practice: validating expected behaviors, and exposing the unexpected which can lead to new insights. With the rise of generative modeling and reinforcement learning, more interesting directions like understanding and visualization of generative models, visual explanation for driving policy could be explored as well. As the second edition of this workshop, we are proposing changes based on the lessons we learned last year. We would like to organize a few domain specific tutorials, and panel discussions. We do think machine learning researchers need a lot of tutorials and advice from the visualization/HCI community and vice versa. Many audience in our workshop last year also suggested that more discussion can greatly help us better define such interdisciplinary area.\n",
      "\n",
      "https://icml.cc/Conferences/2017/Schedule?showEvent=930 2017\n",
      "The workshop will contain presentations of late-breaking reinforcement learning results in all areas of the field, including deep reinforcement learning, exploration, transfer learning and using auxiliary tasks, theoretical result etc, as well as applications of reinforcement learning to various domains. A panel discussion on the most interesting and challenging current research directions will conclude the workshop.\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3280 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3281 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3282 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3283 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3284 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3285 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3286 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3287 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3288 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3289 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3290 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3291 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3292 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3293 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3294 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3295 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3296 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3297 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3298 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3299 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3300 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3301 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3302 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3303 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3304 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3305 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3306 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3307 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3308 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3309 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3310 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3311 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3312 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3313 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3314 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3315 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3316 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3317 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3318 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3319 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3320 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3321 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3322 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3323 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3324 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3325 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3326 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3327 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3328 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3329 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3330 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3331 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3332 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3333 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3334 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3335 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3336 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3337 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3338 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3339 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3340 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3341 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3342 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3343 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3344 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3345 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3346 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3347 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3348 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3349 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3350 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3351 2018\n",
      "\n",
      "https://icml.cc/Conferences/2018/Schedule?showEvent=3352 2018\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3502 2019\n",
      "The 1st workshop on Generalization in Deep Networks: Theory and Practice will be held as part of ICML 2019. Generalization is one of the fundamental problems of machine learning, and increasingly important as deep networks make their presence felt in domains with big, small, noisy or skewed data. This workshop will consider generalization from both theoretical and practical perspectives. We welcome contributions from paradigms such as representation learning, transfer learning and reinforcement learning. The workshop invites researchers to submit working papers in the following research areas: Implicit regularization: the role of optimization algorithms in generalizationExplicit regularization methodsNetwork architecture choices that improve generalizationEmpirical approaches to understanding generalizationGeneralization bounds; empirical evaluation criteria to evaluate boundsRobustness: generalizing to distributional shift a.k.a dataset shiftGeneralization in the context of representation learning, transfer learning and deep reinforcement learning: definitions and empirical approaches\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3503 2019\n",
      "Machine learning has achieved considerable successes in recent years, but this success often relies on human experts, who construct appropriate features, design learning architectures, set their hyperparameters, and develop new learning algorithms. Driven by the demand for off-the-shelf machine learning methods from an ever-growing community, the research area of AutoML targets the progressive automation of machine learning aiming to make effective methods available to everyone. The workshop targets a broad audience ranging from core machine learning researchers in different fields of ML connected to AutoML, such as neural architecture search, hyperparameter optimization, meta-learning, and learning to learn, to domain experts aiming to apply machine learning to new types of problems.\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3504 2019\n",
      "Driven by progress in deep learning, the machine learning community is now able to tackle increasingly more complex problems—ranging from multi-modal reasoning to dexterous robotic manipulation—all of which typically involve solving nontrivial combinations of tasks. Thus, designing adaptive models and algorithms that can efficiently learn, master, and combine multiple tasks is the next frontier. AMTL workshop aims to bring together machine learning researchers from areas ranging from theory to applications and systems, to explore and discuss:* advantages, disadvantages, and applicability of different approaches to learning in multitask settings,* formal or intuitive connections between methods developed for different problems that help better understand the landscape of multitask learning techniques and inspire technique transfer between research lines,* fundamental challenges and open questions that the community needs to tackle for the field to move forward.Webpage: www.amtl-workshop.org\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3505 2019\n",
      "AI for Social Good\n",
      "Important information\n",
      "Contact information: aisg2019.icml.contact@gmail.com\n",
      "Submission deadline: EXTENDED to April 26th 2019 11:59PM ET\n",
      "Workshop website\n",
      "Submission website\n",
      "Poster Information:\n",
      "\n",
      "Poster Size -  36W x 48H inches or 90 x 122 cm\n",
      "Poster Paper - lightweight paper - not laminated\n",
      "\n",
      "Abstract\n",
      "This workshop builds on our AI for Social Good workshop at NeurIPS 2018 and ICLR 2019.\n",
      "Introduction: The rapid expansion of AI research presents two clear conundrums:\n",
      "\n",
      "the comparative lack of incentives for researchers to address social impact issues and\n",
      "the dearth of conferences and journals centered around the topic. Researchers motivated to help often find themselves without a clear idea of which fields to delve into.\n",
      "\n",
      "Goals: Our workshop address both these issues by bringing together machine learning researchers, social impact leaders, stakeholders, policy leaders, and philanthropists to discuss their ideas and applications for social good. To broaden the impact beyond the convening of our workshop, we are partnering with AI Commons to expose accepted projects and papers to the broader community of machine learning researchers and engineers. The projects/research may be at varying degrees of development, from formulation as a data problem to detailed requirements for effective deployment. We hope that this gathering of talent and information will inspire the creation of new approaches and tools by the community, help scientists access the data they need, involve social and policy stakeholders in the framing of machine learning applications, and attract interest from philanthropists invited to the event to make a dent in our shared goals.\n",
      "Topics: The UN Sustainable Development Goals (SDGs), a set of seventeen objectives whose completion is set to lead to a more equitable, prosperous, and sustainable world. In this light, our main areas of focus are the following: health, education, the protection of democracy, urban planning, assistive technology, agriculture, environmental protection and sustainability, social welfare and justice, developing world. Each of these themes presents unique opportunities for AI to reduce human suffering and allow citizens and democratic institutions to thrive.\n",
      "Across these topics, we have dual goals: recognizing high-quality work in machine learning motivated by or applied to social applications, and creating meaningful connections between communities dedicated to solving technical and social problems. To this extent, we propose two research tracks:\n",
      "\n",
      "Short Papers Track (Up to four page papers + unlimited pages for citations) for oral and/or poster presentation. The short papers should focus on past and current research work, showcasing actual results and demonstrating beneficial effects on society. We also accept short papers of recently published or submitted journal contributions to give authors the opportunity to present their work and obtain feedback from conference attendees.\n",
      "Problem Introduction Track (Application form, up to five page responses + unlimited pages for citations) which will present a specific solution that will be shared with stakeholders, scientists, and funders. The workshop will provide a suite of questions designed to: (1) estimate the feasibility and impact of the proposed solutions, and (2) estimate the importance of data in their implementation. The application responses should highlight ideas that have not yet been implemented in practice but can lead to real impact. The projects may be at varying degrees of development, from formulation as a data problem to structure for effective deployment. The workshop provides a supportive platform for developing these early-stage or hobby proposals into real projects. This process is designed to foster sharing different points of view ranging from the scientific assessment of feasibility, discussion of practical constraints that may be encountered, and attracting interest from philanthropists invited to the event. Accepted submissions may be promoted to the wider AI solutions community following the workshop via the AI Commons, with whom we are partnering to promote the longer-term development of projects.\n",
      "\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3506 2019\n",
      "Finance is a rich domain for AI and ML research. Model-driven strategies for stock trading and risk assessment models for loan approvals are quintessential financial applications that are reasonably well-understood. However, there are a number of other applications that call for attention as well. In particular, many finance domains involve ecosystems of interacting and competing agents. Consider for instance the detection of financial fraud and money-laundering. This is a challenging multi-agent learning problem, especially because the real world agents involved evolve their strategies constantly. Similarly, in algorithmic trading of stocks, commodities, etc., the actions of any given trading agent affects, and is affected by, other trading agents -- many of these agents are constantly learning in order to adapt to evolving market scenarios. Further, such trading agents operate at such a speed and scale that they must be fully autonomous. They have grown in sophistication to employ advanced ML strategies including deep learning, reinforcement learning, and transfer learning. Financial institutions have a long history of investing in technology as a differentiator and have been key drivers in advancing computing infrastructure (e.g., low-latency networking). As more financial applications employ deep learning and reinforcement learning, there is consensus now on the need for more advanced computing architectures--for training large machine learning models and simulating large multi-agent learning systems--that balance scale with the stringent privacy requirements of finance. Historically, financial firms have been highly secretive about their proprietary technology developments. But now, there is also emerging consensus on the need for (1) deeper engagement with academia to advance a shared knowledge of the unique challenges faced in FinTech, and (2) more open collaboration with academic and technology partners through intellectually sophisticated fora such as this proposed workshop.\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3507 2019\n",
      "Many in the machine learning community wish to take action on climate change, yet feel their skills are inapplicable.  This workshop aims to show that in fact the opposite is true: while no silver bullet, ML can be an invaluable tool both in reducing greenhouse gas emissions and in helping society adapt to the effects of climate change. Climate change is a complex problem, for which action takes many forms - from designing smart electrical grids to tracking deforestation in satellite imagery. Many of these actions represent high-impact opportunities for real-world change, as well as being interesting problems for ML research.\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3508 2019\n",
      "Coding Theory For Large-scale Machine Learning\n",
      "Coding theory involves the art and science of how to add redundancy to data to ensure that a desirable output is obtained at despite deviations from ideal behavior from the system components that interact with the data. Through a rich, mathematically elegant set of techniques, coding theory has come to significantly influence the design of modern data communications, compression and storage systems. The last few years have seen a rapidly growing interest in coding theory based approaches for the development of efficient machine learning algorithms towards robust, large-scale, distributed computational pipelines. \n",
      "The CodML workshop brings together researchers developing coding techniques for machine learning, as well as researchers working on systems implementations for computing, with cutting-edge presentations from both sides. The goal is to learn about non-idealities in system components as well as approaches to obtain reliable and robust learning despite these non-idealities, and identify problems of future interest.\n",
      "The workshop is co-located with ICML 2019, and will be held in Long Beach, California, USA on June 14th or 15th, 2019.\n",
      "Please see the website for more details: \n",
      "Call for Posters\n",
      "Scope of the Workshop\n",
      "In this workshop we solicit research papers focused on the application of coding and information-theoretic techniques for distributed machine learning. More broadly, we seek papers that address the problem of making machine learning more scalable, efficient, and robust. Both theoretical as well as experimental contributions are welcome. We invite authors to submit papers on topics including but not limited to: \n",
      "\n",
      "Asynchronous Distributed Training Methods\n",
      "Communication-Efficient Training\n",
      "Model Compression and Quantization\n",
      "Gradient Coding, Compression and Quantization\n",
      "Erasure Coding Techniques for Straggler Mitigation\n",
      "Data Compression in Large-scale Machine Learning\n",
      "Erasure Coding Techniques for ML Hardware Acceleration\n",
      "Fast, Efficient and Scalable Inference\n",
      "Secure and Private Machine Learning\n",
      "Data Storage/Access for Machine Learning Jobs\n",
      "Performance evaluation of coding techniques \n",
      "\n",
      "Submission Format and Instructions\n",
      "The authors should prepare extended abstracts in the ICML paper format and submit via CMT. Submitted papers may not exceed three (3) single-spaced double-column pages excluding references. All results, proofs, figures, tables must be included in the 3 pages. The submitted manuscripts should include author names and affiliations, and an abstract that does not exceed 250 words. The authors may include a link to an extended version of the paper that includes supplementary material (proofs, experimental details, etc.) but the reviewers are not required to read the extended version.\n",
      "Dual Submission Policy\n",
      "Accepted submissions will be considered non-archival and can be submitted elsewhere without modification, as long as the other conference allows it. Moreover, submissions to CodML based on work recently accepted to other venues are also acceptable (though authors should explicitly make note of this in their submissions). \n",
      "Key Dates\n",
      "Paper Submission: May 3rd, 2019, 11:59 PM anywhere on earth\n",
      "Decision Notification: May 12th, 2019.\n",
      "Workshop date: June 14 or 15, 2019\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3509 2019\n",
      "Exploration is a key component of reinforcement learning (RL). While RL has begun to solve relatively simple tasks, current algorithms cannot complete complex tasks. Our existing algorithms often endlessly dither, failing to meaningfully explore their environments in search of high-reward states. If we hope to have agents autonomously learn increasingly complex tasks, these machines must be equipped with machinery for efficient exploration. The goal of this workshop is to present and discuss exploration in RL, including deep RL, evolutionary algorithms, real-world applications, and developmental robotics. Invited speakers will share their perspectives on efficient exploration, and researchers will share recent work in spotlight presentations and poster sessions.\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3510 2019\n",
      "Workshop website: https://sites.google.com/view/mbrl-icml2019In the recent explosion of interest in deep RL, “model-free” approaches based on Q-learning and actor-critic architectures have received the most attention due to their flexibility and ease of use. However, this generality often comes at the expense of efficiency (statistical as well as computational) and robustness. The large number of required samples and safety concerns often limit direct use of model-free RL for real-world settings.Model-based methods are expected to be more efficient. Given accurate models, trajectory optimization and Monte-Carlo planning methods can efficiently compute near-optimal actions in varied contexts. Advances in generative modeling, unsupervised, and self-supervised learning provide methods for learning models and representations that support subsequent planning and reasoning. Against this backdrop, our workshop aims to bring together researchers in generative modeling and model-based control to discuss research questions at their intersection, and to advance the state of the art in model-based RL for robotics and AI. In particular, this workshops aims to make progress on questions related to:1. How can we learn generative models efficiently? Role of data, structures, priors, and uncertainty.2. How to use generative models efficiently for planning and reasoning? Role of derivatives, sampling, hierarchies, uncertainty, counterfactual reasoning etc.3. How to harmoniously integrate model-learning and model-based decision making?4. How can we learn compositional structure and environmental constraints? Can this be leveraged for better generalization and reasoning?\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3511 2019\n",
      "https://sites.google.com/view/hill2019/homeThis workshop is a joint effort between the 4th ICML Workshop on Human Interpretability in Machine Learning (WHI) and the ICML 2019 Workshop on Interactive Data Analysis System (IDAS). We have combined our forces this year to run Human in the Loop Learning (HILL) in conjunction with ICML 2019!The workshop will bring together researchers and practitioners who study interpretable and interactive learning systems with applications in large scale data processing, data annotations, data visualization, human-assisted data integration, systems and tools to interpret machine learning models as well as algorithm designs for active learning, online learning, and interpretable machine learning algorithms. The target audience for the workshop includes people who are interested in using machines to solve problems by having a human be an integral part of the process. This workshop serves as a platform where researchers can discuss approaches that bridge the gap between humans and machines and get the best of both worlds.We welcome high-quality submissions in the broad area of human in the loop learning. A few (non-exhaustive) topics of interest include:    Systems for online and interactive learning algorithms,    Active/Interactive machine learning algorithm design,     Systems for collecting, preparing, and managing machine learning data,     Model understanding tools (verifying, diagnosing, debugging, visualization, introspection, etc),    Design, testing and assessment of interactive systems for data analytics,    Psychology of human concept learning,    Generalized additive models, sparsity and rule learning,    Interpretable unsupervised models (clustering, topic models, etc.),    Interpretation of black-box models (including deep neural networks),    Interpretability in reinforcement learning.\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3512 2019\n",
      "With the latest advances of deep generative models, synthesis of images and videos as well as of human voices have achieved impressive realism. In many domains, synthetic media are already difficult to distinguish from real by the human eye and ear. The potential of misuses of these technologies is seldom discussed in academic papers; instead, vocal concerns are rising from media and security organizations as well as from governments. Researchers are starting to experiment on new ways to integrate deep learning with traditional media forensics and security techniques as part of a technological solution. This workshop will bring together experts from the communities of machine learning, computer security and forensics in an attempt to highlight recent work and discuss future effort to address these challenges. Our agenda will alternate contributed papers with invited speakers. The latter will emphasize connections among the interested scientific communities and the standpoint of institutions and media organizations.\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3513 2019\n",
      "Stein's method is a technique from probability theory for bounding the distance between probability measures using differential and difference operators. Although the method was initially designed as a technique for proving central limit theorems, it has recently caught the attention of the machine learning (ML) community and has been used for a variety of practical tasks. Recent applications include generative modeling, global non-convex optimisation, variational inference, de novo sampling, constructing powerful control variates for Monte Carlo variance reduction, and measuring the quality of (approximate) Markov chain Monte Carlo algorithms. Stein's method has also been used to develop goodness-of-fit tests and was the foundational tool in one of the NeurIPS 2017 Best Paper awards. Although Stein's method has already had significant impact in ML, most of the applications only scratch the surface of this rich area of research in probability theory. There would be significant gains to be made by encouraging both communities to interact directly, and this inaugural workshop would be an important step in this direction. More precisely, the aims are: (i) to introduce this emerging topic to the wider ML community, (ii) to highlight the wide range of existing applications in ML, and (iii) to bring together experts in Stein's method and ML researchers to discuss and explore potential further uses of Stein's method.\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3514 2019\n",
      "There has been growing interest in rectifying deep neural network vulnerabilities. Challenges arise when models receive samples drawn from outside the training distribution. For example, a neural network tasked with classifying handwritten digits may assign high confidence predictions to cat images. Anomalies are frequently encountered when deploying ML models in the real world. Well-calibrated predictive uncertainty estimates are indispensable for many machine learning applications, such as self-driving cars and medical diagnosis systems. Generalization to unseen and worst-case inputs is also essential for robustness to distributional shift. In order to have ML models reliably predict in open environment, we must deepen technical understanding in the following areas: (1) learning algorithms that are robust to changes in input data distribution (e.g., detect out-of-distribution examples); (2) mechanisms to estimate and calibrate confidence produced by neural networks and (3) methods to improve robustness to adversarial and common corruptions, and (4) key applications for uncertainty such as in artificial intelligence (e.g., computer vision, robotics, self-driving cars, medical imaging) as well as broader machine learning tasks.This workshop will bring together researchers and practitioners from the machine learning communities, and highlight recent work that contribute to address these challenges. Our agenda will feature contributed papers with invited speakers. Through the workshop we hope to help identify fundamentally important directions on robust and reliable deep learning, and foster future collaborations.\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3515 2019\n",
      "Reinforcement learning (RL) is a general learning, predicting, and decision making paradigm. RL provides solution methods for sequential decision making problems as well as those can be transformed into sequential ones. RL connects deeply with optimization, statistics, game theory, causal inference, sequential experimentation, etc., overlaps largely with approximate dynamic programming and optimal control, and applies broadly in science, engineering and arts.RL has been making steady progress in academia recently, e.g., Atari games, AlphaGo, visuomotor policies for robots. RL has also been applied to real world scenarios like recommender systems and neural architecture search. See a recent collection about RL applications at https://medium.com/@yuxili/rl-applications-73ef685c07eb. It is desirable to have RL systems that work in the real world with real benefits. However, there are many issues for RL though, e.g. generalization, sample efficiency, and exploration vs. exploitation dilemma. Consequently, RL is far from being widely deployed. Common, critical and pressing questions for the RL community are then: Will RL have wide deployments? What are the issues? How to solve them?The goal of this workshop is to bring together researchers and practitioners from industry and academia interested in addressing practical and/or theoretical issues in applying RL to real life scenarios, review state of the arts, clarify impactful research problems, brainstorm open challenges, share first-hand lessons and experiences from real life deployments, summarize what has worked and what has not, collect tips for people from industry looking to apply RL and RL experts interested in applying their methods to real domains, identify potential opportunities, generate new ideas for future lines of research and development, and promote awareness and collaboration. This is not \"yet another RL workshop\": it is about how to successfully apply RL to real life applications. This is a less addressed issue in the RL/ML/AI community, and calls for immediate attention for sustainable prosperity of RL research and development.\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3516 2019\n",
      "Workshop website: https://realworld-sdm.github.io/\n",
      "This workshop aims to bring together researchers from industry and academia in order to describe recent advances and discuss future research directions pertaining to real-world sequential decision making, broadly construed. We aim to highlight new and emerging research opportunities for the machine learning community that arise from the evolving needs for making decision making theoretically and practically relevant for realistic applications. Research interest in reinforcement and imitation learning has surged significantly over the past several years, with the empirical successes of self-playing in games and availability of increasingly realistic simulation environments. We believe the time is ripe for the research community to push beyond simulated domains and start exploring research directions that directly address the real-world need for optimal decision making. We are particularly interested in understanding the current theoretical and practical challenges that prevent broader adoption of current policy learning and evaluation algorithms in high-impact applications, across a broad range of domains. This workshop welcomes both theory and application contributions.\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3517 2019\n",
      "The ever-increasing size and accessibility of vast music libraries has created a demand more than ever for artificial systems that are capable of understanding, organizing, or even generating such complex data. While this topic has received relatively marginal attention within the machine learning community, it has been an area of intense focus within the community of Music Information Retrieval (MIR). While significant progress has been made, these problems remain far from solved. Furthermore, the recommender systems community has made great advances in terms of collaborative feedback recommenders, but these approaches suffer strongly from the cold-start problem. As such, recommendation techniques often fall back on content-based machine learning systems, but defining musical similarity is extremely challenging as myriad features all play some role (e.g., cultural, emotional, timbral, rhythmic). Thus, for machines must actually understand music to achieve an expert level of music recommendation. On the other side of this problem sits the recent explosion of work in the area of machine creativity. Relevant examples are both Google Magenta and the startup Jukedeck, who seek to develop algorithms capable of composing and performing completely original (and compelling) works of music. These algorithms require a similar deep understanding of music and present challenging new problems for the machine learning and AI community at large. This workshop proposal is timely in that it will bridge these separate pockets of otherwise very related research. And in addition to making progress on the challenges above, we hope to engage the wide AI and machine learning community with our nebulous problem space, and connect them with the many available datasets the MIR community has to offer (e.g., Audio Set, AcousticBrainz, Million Song Dataset), which offer near commercial scale to the academic research community.\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3518 2019\n",
      "Models of negative dependence are increasingly important in machine learning. Whether selecting training data, finding an optimal experimental design, exploring in reinforcement learning, or making suggestions with recommender systems, selecting high-quality but diverse items has become a core challenge. This workshop aims to bring together researchers who, using theoretical or applied techniques, leverage negative dependence in their work. We will delve into the rich underlying mathematical theory, understand key applications, and discuss the most promising directions for future research.\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3519 2019\n",
      "Graph-structured representations are widely used as a natural and powerful way to encode information such as relations between objects or entities, interactions between online users (e.g., in social networks), 3D meshes in computer graphics, multi-agent environments, as well as molecular structures, to name a few. Learning and reasoning with graph-structured representations is gaining increasing interest in both academia and industry, due to its fundamental advantages over more traditional unstructured methods in supporting interpretability, causality, transferability, etc. Recently, there is a surge of new techniques in the context of deep learning, such as graph neural networks, for learning graph representations and performing reasoning and prediction, which have achieved impressive progress. However, it can still be a long way to go to obtain satisfactory results in long-range multi-step reasoning, scalable learning with very large graphs, flexible modeling of graphs in combination with other dimensions such as temporal variation and other modalities such as language and vision. New advances in theoretical foundations, models and algorithms, as well as empirical discoveries and applications are therefore all highly desirable. The aims of this workshop are to bring together researchers to dive deeply into some of the most promising methods which are under active exploration today, discuss how we can design new and better benchmarks, identify impactful application domains, encourage discussion and foster collaboration. The workshop will feature speakers, panelists, and poster presenters from machine perception, natural language processing, multi-agent behavior and communication, meta-learning, planning, and reinforcement learning, covering approaches which include (but are not limited to): -Deep learning methods on graphs/manifolds/relational data (e.g., graph neural networks) -Deep generative models of graphs (e.g., for drug design) -Unsupervised graph/manifold/relational embedding methods (e.g., hyperbolic embeddings) -Optimization methods for graphs/manifolds/relational data -Relational or object-level reasoning in machine perception -Relational/structured inductive biases for reinforcement learning, modeling multi-agent behavior and communication -Neural-symbolic integration -Theoretical analysis of capacity/generalization of deep learning models for graphs/manifolds/ relational data -Benchmark datasets and evaluation metrics\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3520 2019\n",
      "This joint workshop aims to bring together researchers, educators, practitioners who are interested in techniques as well as applications of on-device machine learning and compact, efficient neural network representations. One aim of the workshop discussion is to establish close connection between researchers in the machine learning community and engineers in industry, and to benefit both academic researchers as well as industrial practitioners. The other aim is the evaluation and comparability of resource-efficient machine learning methods and compact and efficient network representations, and their relation to particular target platforms (some of which may be highly optimized for neural network inference). The research community has still to develop established evaluation procedures and metrics.The workshop also aims at reproducibility and comparability of methods for compact and efficient neural network representations, and on-device machine learning. Contributors are thus encouraged to make their code available. The workshop organizers plan to make some example tasks and datasets available, and invite contributors to use them for testing their work. In order to provide comparable performance evaluation conditions, the use of a common platform (such as Google Colab) is intended.\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3521 2019\n",
      "Invertible neural networks have been a significant thread of research in the ICML community for several years. Such transformations can offer a range of unique benefits: (1) They preserve information, allowing perfect reconstruction (up to numerical limits) and obviating the need to store hidden activations in memory for backpropagation. (2) They are often designed to track the changes in probability density that applying the transformation induces (as in normalizing flows). (3) Like autoregressive models, normalizing flows can be powerful generative models which allow exact likelihood computations; with the right architecture, they can also allow for much cheaper sampling than autoregressive models. While many researchers are aware of these topics and intrigued by several high-profile papers, few are familiar enough with the technical details to easily follow new developments and contribute. Many may also be unaware of the wide range of applications of invertible neural networks, beyond generative modelling and variational inference.\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3522 2019\n",
      "Our understanding of modern neural networks lags behind their practical successes. As this understanding gap grows, it poses a serious challenge to the future pace of progress because fewer pillars of knowledge will be available to designers of models and algorithms. This workshop aims to close this understanding gap in deep learning. It solicits contributions that view the behavior of deep nets as a natural phenomenon to investigate with methods inspired from the natural sciences, like physics, astronomy, and biology. We solicit empirical work that isolates phenomena in deep nets, describes them quantitatively, and then replicates or falsifies them.As a starting point for this effort, we focus on the interplay between data, network architecture, and training algorithms. We are looking for contributions that identify precise, reproducible phenomena, as well as systematic studies and evaluations of current beliefs such as “sharp local minima do not generalize well” or “SGD navigates out of local minima”. Through the workshop, we hope to catalogue quantifiable versions of such statements, as well as demonstrate whether or not they occur reproducibly.\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3523 2019\n",
      "Website: https://sites.google.com/view/icml-i3\n",
      "Abstract: A key challenge for deploying interactive machine learning systems in the real world is the ability for machines to understand human intent. Techniques such as imitation learning and inverse reinforcement learning are popular data-driven paradigms for modeling agent intentions and controlling agent behaviors, and have been applied to domains ranging from robotics and autonomous driving to dialogue systems. Such techniques provide a practical solution to specifying objectives to machine learning systems when they are difficult to program by hand. While significant progress has been made in these areas, most research effort has concentrated on modeling and controlling single agents from dense demonstrations or feedback. However, the real world has multiple agents, and dense expert data collection can be prohibitively expensive. Surmounting these obstacles requires progress in frontiers such as: 1) the ability to infer intent from multiple modes of data, such as language or observation, in addition to traditional demonstrations. 2) the ability to model multiple agents and their intentions, both in cooperative and adversarial settings. 3) handling partial or incomplete information from the expert, such as demonstrations that lack dense action annotations, raw videos, etc.. The workshop on Imitation, Intention, and Interaction (I3) seeks contributions at the interface of these frontiers, and will bring together researchers from multiple disciplines such as robotics, imitation and reinforcement learning, cognitive science, AI safety, and natural language understanding. Our aim will be to reexamine the assumptions in standard imitation learning problem statements (e.g., inverse reinforcement learning) and connect distinct application disciplines, such as robotics and NLP, with researchers developing core imitation learning algorithms. In this way, we hope to arrive at new problem formulations, new research directions, and the development of new connections across distinct disciplines that interact with imitation learning methods.\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3524 2019\n",
      "The workshop will showcase recent research in the field of Computational Biology. There has been significant development in genomic sequencing techniques and imaging technologies. These approaches not only generate huge amounts of data but provide unprecedented resolution of single cells and even subcellular structures. The availability of high dimensional data, at multiple spatial and temporal resolutions has made machine learning and deep learning methods increasingly critical for computational analysis and interpretation of the data. Conversely, biological data has also exposed unique challenges and problems that call for the development of new machine learning methods. This workshop aims to bring together researchers working at the intersection of Machine Learning and Biology to present recent advances and open questions in Computational Biology to the ML community. The workshop is a sequel to the WCB workshops we organized in the last three years Joint ICML and IJCAI 2018, Stockholm, ICML 2017, Sydney and ICML 2016, New York as well as Workshop on Bioinformatics and AI at IJCAI 2015 Buenos Aires, IJCAI 2016 New York, IJCAI 2017 Melbourne which had excellent line-ups of talks and were well-received by the community. Every year, we received 60+ submissions. After multiple rounds of rigorous reviewing around 50 submissions were selected from which the best set of papers were chosen for Contributed talks and Spotlights and the rest were invited as Posters. We have a steadfast and growing base of reviewers making up the Program Committee. For the past edition, a special issue of Journal of Computational Biology will be released in the following weeks with extended versions of 14 accepted papers. We have two confirmed invited speakers and we will invite at least one more leading researcher in the field. Similar to previous years, we plan to request partial funding from Microsoft Research, Google, Python, Swiss Institute of Bioinformatics that we intend to use for student travel awards. In past years, we have also been able to provide awards for best poster/paper and partially contribute to travel expenses for at least 8 students per year. The Workshop proceedings will be available through CEUR proceedings. We would also have an extended version to be included in a special issue of the Journal of Computational Biology (JCB) for which we already have an agreement with JCB.\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3525 2019\n",
      "Time series data is both quickly growing and already ubiquitous. In domains spanning as broad a range as climate, robotics, entertainment, finance, healthcare, and transportation, there has been a significant shift away from parsimonious, infrequent measurements to nearly continuous monitoring and recording. Rapid advances in sensing technologies, ranging from remote sensors to wearables and social sensing, are generating rapid growth in the size and complexity of time series data streams. Thus, the importance and impact of time series analysis and modelling techniques only continues to grow. At the same time, while time series analysis has been extensively studied by econometricians and statisticians, modern time series data often pose significant challenges for the existing techniques both in terms of their structure (e.g., irregular sampling in hospital records and spatiotemporal structure in climate data) and size. Moreover, the focus on time series in the machine learning community has been comparatively much smaller. In fact, the predominant methods in machine learning often assume i.i.d. data streams, which is generally not appropriate for time series data. Thus, there is both a great need and an exciting opportunity for the machine learning community to develop theory, models and algorithms specifically for the purpose of processing and analyzing time series data. We see ICML as a great opportunity to bring together theoretical and applied researchers from around the world and with different backgrounds who are interested in the development and usage of time series analysis and algorithms. This includes methods for time series prediction, classification, clustering, anomaly and change point detection, causal discovery, and dimensionality reduction as well as general theory for learning and analyzing stochastic processes. Since time series have been studied in a variety of different fields and have many broad applications, we plan to host leading academic researchers and industry experts with a range of perspectives and interests as invited speakers. Moreover, we also invite researchers from the related areas of batch and online learning, deep learning, reinforcement learning, data analysis and statistics, and many others to both contribute and participate in this workshop.\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3526 2019\n",
      "As machine learning has increasingly been deployed in critical real-world applications, the dangers of manipulation and misuse of these models has become of paramount importance to public safety and user privacy. In applications such as online content recognition to financial analytics to autonomous vehicles all have shown the be vulnerable to adversaries wishing to manipulate the models or mislead models to their malicious ends. This workshop will focus on recent research and future directions about the security and privacy problems in real-world machine learning systems. We aim to bring together experts from machine learning, security, and privacy communities in an attempt to highlight recent work in these area as well as to clarify the foundations of secure and private machine learning strategies. We seek to come to a consensus on a rigorous framework to formulate adversarial attacks targeting machine learning models, and to characterize the properties that ensure the security and privacy of machine learning systems. Finally, we hope to chart out important directions for future work and cross-community collaborations.\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3527 2019\n",
      "Self-supervised learning is a promising alternative where proxy tasks are developed that allow models and agents to learn without explicit supervision in a way that helps with downstream performance on tasks of interest. One of the major benefits of self-supervised learning is increasing data efficiency: achieving comparable or better performance with less labeled data or fewer environment steps (in Reinforcement learning / Robotics).The field of self-supervised learning (SSL) is rapidly evolving, and the performance of these methods is creeping closer to the fully supervised approaches. However, many of these methods are still developed in domain-specific sub-communities, such as Vision, RL and NLP, even though many similarities exist between them. While SSL is an emerging topic and there is great interest in these techniques, there are currently few workshops, tutorials or other scientific events dedicated to this topic.This workshop aims to bring together experts with different backgrounds and applications areas to share inter-domain ideas and increase cross-pollination, tackle current shortcomings and explore new directions. The focus will be on the machine learning point of view rather than the domain side.https://sites.google.com/corp/view/self-supervised-icml2019\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3528 2019\n",
      "Website link: https://sites.google.com/view/mtlrl/Significant progress has been made in reinforcement learning, enabling agents to accomplish complex tasks such as Atari games, robotic manipulation, simulated locomotion, and Go. These successes have stemmed from the core reinforcement learning formulation of learning a single policy or value function from scratch. However, reinforcement learning has proven challenging to scale to many practical real world problems due to problems in learning efficiency and objective specification, among many others. Recently, there has been emerging interest and research in leveraging structure and information across multiple reinforcement learning tasks to more efficiently and effectively learn complex behaviors. This includes:1. curriculum and lifelong learning, where the problem requires learning a sequence of tasks, leveraging their shared structure to enable knowledge transfer2. goal-conditioned reinforcement learning techniques that leverage the structure of the provided goal space to learn many tasks significantly faster3. meta-learning methods that aim to learn efficient learning algorithms that can learn new tasks quickly4. hierarchical reinforcement learning, where the reinforcement learning problem might entail a compositions of subgoals or subtasks with shared structureMulti-task and lifelong reinforcement learning has the potential to alter the paradigm of traditional reinforcement learning, to provide more practical and diverse sources of supervision, while helping overcome many challenges associated with reinforcement learning, such as exploration, sample efficiency and credit assignment. However, the field of multi-task and lifelong reinforcement learning is still young, with many more developments needed in terms of problem formulation, algorithmic and theoretical advances as well as better benchmarking and evaluation.The focus of this workshop will be on both the algorithmic and theoretical foundations of multi-task and lifelong reinforcement learning as well as the practical challenges associated with building multi-tasking agents and lifelong learning benchmarks. Our goal is to bring together researchers that study different problem domains (such as games, robotics, language, and so forth), different optimization approaches (deep learning, evolutionary algorithms, model-based control, etc.), and different formalisms (as mentioned above) to discuss the frontiers, open problems and meaningful next steps in multi-task and lifelong reinforcement learning.\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3529 2019\n",
      "A diverse set of methods have been devised to develop autonomous driving platforms. They range from modular systems, systems that perform manual decomposition of the problem, systems where the components are optimized independently, and a large number of rules are programmed manually, to end-to-end deep-learning frameworks. Today’s systems rely on a subset of the following: camera images, HD maps, inertial measurement units, wheel encoders, and active 3D sensors (LIDAR, radar). There is a general agreement that much of the self-driving software stack will continue to incorporate some form of machine learning in any of the above mentioned systems in the future.Self-driving cars present one of today’s greatest challenges and opportunities for Artificial Intelligence (AI). Despite substantial investments, existing methods for building autonomous vehicles have not yet succeeded, i.e., there are no driverless cars on public roads today without human safety drivers. Nevertheless, a few groups have started working on extending the idea of learned tasks to larger functions of autonomous driving. Initial results on learned road following are very promising.The goal of this workshop is to explore ways to create a framework that is capable of learning autonomous driving capabilities beyond road following, towards fully driverless cars. The workshop will consider the current state of learning applied to autonomous vehicles and will explore how learning may be used in future systems. The workshop will span both theoretical frameworks and practical issues especially in the area of deep learning.\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3530 2019\n",
      "Probabilistic modeling has become the de facto framework to reason about uncertainty in Machine Learning and AI. One of the main challenges in probabilistic modeling is the trade-off between the expressivity of the models and the complexity of performing various types of inference, as well as learning them from data. This inherent trade-off is clearly visible in powerful -- but intractable -- models like Markov random fields, (restricted) Boltzmann machines, (hierarchical) Dirichlet processes and Variational Autoencoders. Despite these models’ recent successes, performing inference on them resorts to approximate routines. Moreover, learning such models from data is generally harder as inference is a sub-routine of learning, requiring simplifying assumptions or further approximations. Having guarantees on tractability at inference and learning time is then a highly desired property in many real-world scenarios. Tractable probabilistic modeling (TPM) concerns methods guaranteeing exactly this: performing exact (or tractably approximate) inference and/or learning. To achieve this, the following approaches have been proposed: i) low or bounded-treewidth probabilistic graphical models and determinantal point processes, that exchange expressiveness for efficiency; ii) graphical models with high girth or weak potentials, that provide bounds on the performance of approximate inference methods; and iii) exchangeable probabilistic models that exploit symmetries to reduce inference complexity. More recently, models compiling inference routines into efficient computational graphs such as arithmetic circuits, sum-product networks, cutset networks and probabilistic sentential decision diagrams have advanced the state-of-the-art inference performance by exploiting context-specific independence, determinism or by exploiting latent variables. TPMs have been successfully used in numerous real-world applications: image classification, completion and generation, scene understanding, activity recognition, language and speech modeling, bioinformatics, collaborative filtering, verification and diagnosis of physical systems. The aim of this workshop is to bring together researchers working on the different fronts of tractable probabilistic modeling, highlighting recent trends and open challenges. At the same time, we want to foster the discussion across similar or complementary sub-fields in the broader probabilistic modeling community. In particular, the rising field of neural probabilistic models, such as normalizing flows and autoregressive models that achieve impressive results in generative modeling. It is an interesting open challenge for the TPM community to keep a broad range of inference routines tractable while leveraging these models’ expressiveness. Furthermore, the rising field of probabilistic programming promises to be the new lingua franca of model-based learning. This offers the TPM community opportunities to push the expressiveness of the models used for general-purpose universal probabilistic languages, such as Pyro, while maintaining efficiency. We want to promote discussions and advance the field both by having high quality contributed works, as well as high level invited speakers coming from the aforementioned tangent sub-fields of probabilistic modeling.\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3531 2019\n",
      "Though the purview of physics is broad and includes many loosely connected subdisciplines, a unifying theme is the endeavor to provide concise, quantitative, and predictive descriptions of the often large and complex systems governing phenomena that occur in the natural world. While one could debate how closely deep learning is connected to the natural world, it is undeniably the case that deep learning systems are large and complex; as such, it is reasonable to consider whether the rich body of ideas and powerful tools from theoretical physicists could be harnessed to improve our understanding of deep learning. The goal of this workshop is to investigate this question by bringing together experts in theoretical physics and deep learning in order to stimulate interaction and to begin exploring how theoretical physics can shed light on the theory of deep learning.We believe ICML is an appropriate venue for this gathering as members from both communities are frequently in attendance and because deep learning theory has emerged as a focus at the conference, both as an independent track in the main conference and in numerous workshops over the last few years. Moreover, the conference has enjoyed an increasing number of papers using physics tools and ideas to draw insights into deep learning.\n",
      "\n",
      "https://icml.cc/Conferences/2019/Schedule?showEvent=3532 2019\n",
      "Research at the intersection of vision and language has been attracting a lot of attention in recent years. Topics include the study of multi-modal representations, translation between modalities, bootstrapping of labels from one modality into another, visually-grounded question answering, segmentation and storytelling, and grounding the meaning of language in visual data. An ever-increasing number of tasks and datasets are appearing around this recently-established field. At NeurIPS 2018, we released the How2 data-set, containing more than 85,000 (2000h) videos, with audio, transcriptions, translations, and textual summaries. We believe it presents an ideal resource to bring together researchers working on the previously mentioned separate tasks around a single, large dataset. This rich dataset will facilitate the comparison of tools and algorithms, and hopefully foster the creation of additional annotations and tasks. We want to foster discussion about useful tasks, metrics, and labeling techniques, in order to develop a better understanding of the role and value of multi-modality in vision and language. We seek to create a venue to encourage collaboration between different sub-fields, and help establish new research directions and collaborations that we believe will sustain machine learning research for years to come. \n",
      "Workshop Homepage\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5715 2020\n",
      "Recent years have seen a surge in research on graph representation learning, including techniques for deep graph embeddings, generalizations of CNNs to graph-structured data, and neural message-passing approaches. These advances in graph neural networks and related techniques have led to new state-of-the-art results in numerous domains: chemical synthesis, 3D-vision, recommender systems, question answering, continuous control, self-driving and social network analysis. Building on the successes of three related workshops from last year (at ICML, ICLR and NeurIPS), the primary goal for this workshop is to facilitate community building, and support expansion of graph representation learning into more interdisciplinary projects with the natural and social sciences. With hundreds of new researchers beginning projects in this area, we hope to bring them together to consolidate this fast-growing area into a healthy and vibrant subfield. Especially, we aim to strongly promote novel and exciting applications of graph representation learning across the sciences, reflected in our choices of invited speakers.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5716 2020\n",
      "Artificial Intelligence (AI), and Machine Learning systems in particular, often depend on the information provided by multiple agents. The most well-known example is federated learning, but also sensor data, crowdsourced human computation, or human trajectory inputs for inverse reinforcement learning. However, eliciting accurate data can be costly, either due to the effort invested in obtaining it, as in crowdsourcing, or due to the need to maintain automated systems, as in distributed sensor systems. Low-quality data not only degrades the performance of AI systems, but may also pose safety concerns. Thus, it becomes important to verify the correctness of data and be smart in how data is aggregated, and to provide incentives to promote effort and high-quality data. During the recent workshop on Federated Learning at NeurIPS 2019, 4 of 6 panel members mentioned incentives as the most important open issue.This workshop is proposed to understand this aspect of Machine Learning, both theoretically and empirically. We particularly encourage contributions on the following aspects:- How to collect high quality and credible data for machine learning systems from self-interested and possibly malicious agents, considering the game-theoretical properties of the problem?- How to evaluate the quality of data supplied by self-interested and possibly malicious agents and how to optimally aggregate it?- How to make use of machine learning in game-theoretic mechanisms that will facilitate the collection of high-quality data?\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5717 2020\n",
      "There has been growing interest in rectifying deep neural network instabilities. Challenges arise when models receive samples drawn from outside the training distribution. For example, a neural network tasked with classifying handwritten digits may assign high confidence predictions to cat images. Anomalies are frequently encountered when deploying ML models in the real world. Well-calibrated predictive uncertainty estimates are indispensable for many machine learning applications, such as self-driving vehicles and medical diagnosis systems. Generalization to unseen and worst-case inputs is also essential for robustness to distributional shift. In order to have ML models reliably predict in open environment, we must deepen technical understanding in the emerging areas of: (1) learning algorithms that can detect  changes in data distribution (e.g. out-of-distribution examples); (2) mechanisms to estimate and calibrate confidence produced by neural networks in typical and unforeseen scenarios; (3) methods to improve out-of-distribution generalization, including generalization to temporal, geographical, hardware, adversarial, and image-quality changes; (4) benchmark datasets and protocols for evaluating model performance under distribution shift; and (5) key applications of robust and uncertainty-aware deep learning (e.g., computer vision, robotics, self-driving vehicles, medical imaging) as well as broader machine learning tasks.This workshop will bring together researchers and practitioners from the machine learning communities, and highlight recent work that contributes to addressing these challenges. Our agenda will feature contributed papers with invited speakers. Through the workshop we hope to help identify fundamentally important directions on robust and reliable deep learning, and foster future collaborations.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5718 2020\n",
      "Description: the workshop proposal in “Law and Machine Learning” aims to contribute to the research on social and legal risks of the deployment of AI systems using machine learning based decisions. Today, algorithms have been infiltrating and governing every aspect of our lives as individuals and as a society. Specifically, Algorithmic Decision Systems (ADS) are involved in many social decisions. For instance, such systems are increasingly used to support decision-making in fields, such as child welfare, criminal justice, school assignment, teacher evaluation, fire risk assessment, homelessness prioritization, healthcare, Medicaid benefit, immigration decision systems or risk assessment, and predictive policing, among other things. Law enforcement agencies are increasingly using facial recognition, algorithmic predictive policing systems to forecast criminal activity and allocate police resources. However, these predictive systems challenge fundamental rights and guarantees of the criminal procedure. For several years, numerous studies have revealed, social risks of ML, especially the risks of opacity, bias, manipulation of information.While it is only the starting point of the deployment of such systems, more interdisciplinary research is needed. Our purpose is to contribute to this new field which brings together legal researchers, mathematicians and computer scientists, by bridging the gap between the performance of algorithmic systems and legal standards. For instance, notions like “privacy” or “fairness” are formulated in law, as well as mathematical definitions in computer science. However, the meaning and the impact of such requirements are not necessarily identical. Besides, legal norms to regulate AI systems appear in certain national laws but have to be relevant and compatible with technical requirements. Furthermore, these standards must be checked by legal experts and regulators, which presupposes that AI systems are sufficiently meaningful and transparent. These issues emerge in different topics, such as privacy in data analysis and fairness in algorithmic decision-making. The topic will cover the research that denounces the risks and, above all, multidisciplinary research that proposes solutions, especially legal and technical solutions.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5719 2020\n",
      "Extreme classification is a rapidly growing research area focusing on multi-class and multi-label problems, where the label space is extremely large. It brings many diverse approaches under the same umbrella including natural language processing (NLP), computer vision, information retrieval, recommendation systems, computational advertising, and embedding methods. Extreme classifiers have been deployed in many real-world applications in the industry ranging from language modelling to document tagging in NLP, face recognition to learning universal feature representations in computer vision, etc. Moreover, extreme classification finds application in recommendation, tagging, and ranking systems since these problems can be reformulated as multi-label learning tasks where each item to be ranked or recommended is treated as a separate label. Such reformulations have led to significant gains over traditional collaborative filtering and content-based recommendation techniques. The proposed workshop aims to offer a timely collection of information to benefit the researchers and practitioners working in the aforementioned research fields of core supervised learning, theory of extreme classification, as well as application domains. These issues are well-covered by the Topics of Interest in ICML 2020. The workshop aims to bring together researchers interested in these areas to encourage discussion, facilitate interaction and collaboration and improve upon the state-of-the-art in extreme classification. The workshop will provide plethora of opportunities for research discussions, including poster sessions, invited talks, contributed talks, and a panel. During the panel the speakers will discuss challenges & opportunities in the field of extreme classification, in particular: 1) how to deal with the long tail labels problem?, 2) how to effectively combine deep learning approaches with extreme multi-label classification techniques?, 3) how to develop the theoretical foundations for this area? We expect a healthy participation from both industry and academia.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5720 2020\n",
      "The designers of a machine learning (ML) system typically have far more power over the system than the individuals who are ultimately impacted by the system and its decisions. Recommender platforms shape the users’ preferences; the individuals classified by a model often do not have means to contest a decision; and the data required by supervised ML systems necessitates that the privacy and labour of many yield to the design choices of a few.The fields of algorithmic fairness and human-centered ML often focus on centralized solutions, lending increasing power to system designers and operators, and less to users and affected populations. In response to the growing social-science critique of the power imbalance present in the research, design, and deployment of ML systems, we wish to consider a new set of technical formulations for the ML community on the subject of more democratic, cooperative, and participatory ML systems.Our workshop aims to explore methods that, by design, enable and encourage the perspectives of those impacted by an ML system to shape the system and its decisions. By involving affected populations in shaping the goals of the overall system, we hope to move beyond just tools for enabling human participation and progress towards a redesign of power dynamics in ML systems.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5721 2020\n",
      "The workshop will showcase recent research in the field of Computational Biology.\n",
      "Computational biology is an interdisciplinary field that develops and applies analytical methods,\n",
      "mathematical and statistical modeling and simulation to analyze and interpret vast collections of\n",
      "biological data, such as genetic sequences, cellular features or protein structures, and imaging\n",
      "datasets to make new predictions towards clinical response, discover new biology or aid drug\n",
      "discovery. The availability of high-dimensional data, at multiple spatial and temporal resolutions\n",
      "has made machine learning and deep learning methods increasingly critical for computational\n",
      "analysis and interpretation of the data. Conversely, biological data has also exposed unique\n",
      "challenges and problems that call for the development of new machine learning methods.\n",
      "This workshop aims to bring together researchers working at the unique intersection of Machine\n",
      "Learning and Biology that include areas (and not limited to) such as computational genomics,\n",
      "neuroscience, pathology, radiology, evolutionary biology, population genomics, phenomics,\n",
      "ecology, cancer biology, causality, and representation learning and disentanglement to present\n",
      "recent advances and open questions to the ML community.\n",
      "The workshop is a sequel to the WCB workshops we organized in the last four years ICML\n",
      "2019, Long Beach , Joint ICML and IJCAI 2018, Stockholm , ICML 2017, Sydney and ICML\n",
      "2016, New York as well as Workshop on Bioinformatics and AI at IJCAI 2015 Buenos Aires,\n",
      "IJCAI 2016 New York, IJCAI 2017 Melbourne which had excellent line-ups of talks and was\n",
      "well-received by the community. Every year, we received 60+ submissions. After multiple\n",
      "rounds of rigorous reviewing around 50 submissions were selected from which the best set of\n",
      "papers were chosen for Contributed talks and Spotlights and the rest were invited as Posters.\n",
      "We have a steadfast and growing base of reviewers making up the Program Committee. For\n",
      "the past two editions, a special issue of Journal of Computational Biology has been released\n",
      "with extended versions of a select set of accepted papers.\n",
      "We invited Thomas Fuchs, Debora Marks, Fabian Theis, Olga Troyanskaya. We have received funding confirmation from PAIGE and Amazon Web Services that we intend to use for student travel awards. In past years, we have also been able to provide awards for the best poster/paper and partially contribute to the student registration fee.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5722 2020\n",
      "In situations where a task can be cleanly formulated and data is plentiful, modern machine learning (ML) techniques have achieved impressive (and often super-human) results.  Here, plentiful data'' can mean labels from humans, access to a simulator and well designed reward function, or other forms of interaction and supervision.<br><br>On the other hand, in situations where tasks cannot be cleanly formulated and plentifully supervised, ML has not yet shown the same progress.  We still seem far from flexible  agents that can learn without human engineers carefully designing or collating their supervision. This is problematic in many settings where machine learning is or will be applied in real world settings, where these agents have to interact with human users and may be used in settings that go beyond any initial clean training data used during system development. A key open question is how to make machine learning effective and robust enough to operate in real world open domains.<br><br>Artificial {\\it open} worlds are ideal laboratories for studying how to extend the successes of ML to build such agents. <br>Open worlds are characterized by:<br>\\begin{itemize}<br>    \\item Large (or perhaps infinite) collections of tasks, often not specified till test time; or lack of well defined tasks altogether (despite there being lots to do).<br>    \\itemunbounded'' environments, long ``epsiodes''; or no episodes at all.    \\item Many interacting agents; more generally, emergent behavior from interactions with the environment.\\end{itemize}On one hand, they retain many of the challenging features of the real world with respect to studying learning agents.  On the other hand,they allow cheap collection of environment interaction data.  Furthermore, because many artificial worlds of interest are games that people enjoy playing, they could allow interaction with humans at scale.A particularly promising direction is that open world games can bridge: the closed domains and benchmarks that have traditionally driven research progress and open ended real world applications in which resulting technology is deployed.We propose a workshop designed to catalyze research towards addressing these challenges posed by machine learning in open worlds.Our goal is to bring together researchers with a wide range of perspectives whose work focuses on, or is enabled by, open worlds. This would be the very first workshop focused on this topic, and we anticipate that it would play a key role in sharing experience, brainstorming ideas, and catalyzing novel directions for research.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5723 2020\n",
      "Although data is considered to be the “new oil”, it is very hard to be priced. Raw use of data has been invaluable in several sectors such as advertising, healthcare, etc, but often in violation of people’s privacy. Labeled data has also been extremely valuable for the training of machine learning models (driverless car industry). This is also indicated by the growth of annotation companies such as Figure8 and Scale.AI, especially in the image space. Yet, it is not clear what is the right pricing for data workers who annotate the data or the individuals who contribute their personal data while using digital services. In the latter case, it is very unclear how the value of the services offered is compared to the private data exchanged.  While the first data marketplaces have appeared, such as AWS, Narattive.io, nitrogen.ai, etc, they suffer from a lack of good pricing models. They also fail to maintain the right of the data owners to define how their own data will be used. There have been numerous suggestions for sharing data while maintaining privacy, such as training generative models that preserve original data statistics.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5724 2020\n",
      "Analysis of large amounts of data offers new opportunities to understand many processes better. Yet, data accumulation often implies relaxing acquisition procedures or compounding diverse sources, leading to many observations with missing features. From questionnaires to collaborative filtering, from electronic health records to single-cell analysis, missingness is everywhere at play and is rather the norm than the exception. Even “clean” data sets are often barely “cleaned” versions of incomplete data sets—with all the unfortunate biases this cleaning process may have created.Despite this ubiquity, tackling missing values is often overlooked.  Handling missing values poses many challenges, and there is a vast literature in the statistical community, with many implementations available. Yet, there are still many open issues and the need to design new methods or to introduce new point of views: for missing values in a supervised-learning setting, in deep learning architectures, to adapt available methods for high dimensional observed data with different type of missing values, deal with feature mismatch and distribution mismatch. Missing data is one of the eight pillars of causal wisdom for Judea Pearl who brought graphical model reasoning to tackle some missing not at random values.To the best of our knowledge, this is the first workshop at the major machine learning conferences focusing primarily on missing value problems in recent years. The goal of our workshop is to give more momentum and exposition to research on missing values, both theoretical and methodological, and emphasize the connections with other areas of machine learning (e.g. causal inference, generative modelling, uncertainty quantification, transfer learning, distributional shift, etc.). We will also attach importance to discussing the reproducibility problems that can be caused by missing data, the danger of forgetting the missing values issues and the importance of providing sound implementations.We welcome both academic and industrial practitioners/researchers. In particular, since missing data is a critical issue in many applications, we would like to federate industrial/applied know-how and various academic approaches.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5725 2020\n",
      "Machine learning has achieved considerable successes in recent years, but this success often relies on human experts, who construct appropriate features, design learning architectures, set their hyperparameters, and develop new learning algorithms. Driven by the demand for off-the-shelf machine learning methods from an ever-growing community, the research area of AutoML targets the progressive automation of machine learning aiming to make effective methods available to everyone. Hence, the workshop targets a broad audience ranging from core machine learning researchers in different fields of ML connected to AutoML, such as neural architecture search, hyperparameter optimization, meta-learning, and learning to learn, to domain experts aiming to apply machine learning to new types of problems. The schedule is wrt CEST (i.e., the time zone of Vienna) \n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5726 2020\n",
      "A person’s health is determined by a variety of factors beyond those captured by electronic health records or the genome. Many healthcare organizations recognize the importance of the social determinants of health (SDH) such as socioeconomic status, employment, food security, education, and community cohesion. Capturing such comprehensive portraits of patient data is necessary to transform a healthcare system and improve population health while simultaneously delivering personalized healthcare provisions. Machine learning (ML) is well-positioned to transform system-level healthcare through the design of intelligent algorithms that incorporate SDH into clinical and policy interventions, such as population health programs and clinical decision support systems. Innovations in health-tech through wearable devices and mobile health, among others, provide rich sources of data, including those characterizing SDH. The guiding metric of success should be health outcomes: the improvement of health and care at both the individual and population levels. This workshop will identify the needs of system-level healthcare transformation that ML may satisfy. We will bring together ML researchers, health policy practitioners, clinical organization experts, and individuals from all areas of clinic-, hospital-, and community-based healthcare.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5727 2020\n",
      "Models of negative dependence and submodularity are increasingly important in machine learning. Whether selecting training data, finding an optimal experimental design, exploring in reinforcement learning and Bayesian optimization, or designing recommender systems, selecting high-quality yet diverse items has become a core challenge. This workshop aims to bring together researchers who, using theoretical or applied techniques, leverage negative dependence and submodularity in their work. Expanding upon last year's workshop, we will highlight recent developments in the rich mathematical theory of negative dependence, cover novel critical applications, and discuss the most promising directions for future research.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5728 2020\n",
      "Machine learning is increasingly being applied to problems in the healthcare domain. However, there is a risk that the development of machine learning models for improving health remain focused within areas and diseases which are more economically incentivised and resourced. This presents the risk that as research and technological entities aim to develop machine-learning-assisted consumer healthcare devices, or bespoke algorithms for their populations within a certain geographical region, that the challenges of healthcare in resource-constrained settings will be overlooked. The predominant research focus of machine learning for healthcare in the “economically advantaged” world means that there is a skew in our current knowledge of how machine learning can be used to improve health on a more global scale – for everyone.  This workshop aims to draw attention to the ways that machine learning can be used for problems in global health, and to promote research on problems outside high-resource environments.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5729 2020\n",
      "In many settings such as education, healthcare, drug design, robotics, transportation, and achieving better-than-human performance in strategic games, it is important to make decisions sequentially. This poses two interconnected algorithmic and statistical challenges: effectively exploring to learn information about the underlying dynamics and effectively planning using this information. Reinforcement Learning (RL) is the main paradigm tackling both of these challenges simultaneously which is essential in the aforementioned applications. Over the last years, reinforcement learning has seen enormous progress both in solidifying our understanding on its theoretical underpinnings and in applying these methods in practice. This workshop aims to highlight recent theoretical contributions, with an emphasis on addressing significant challenges on the road ahead. Such theoretical understanding is important in order to design algorithms that have robust and compelling performance in real-world applications. As part of the ICML 2020 conference, this workshop will be held virtually. It will feature keynote talks from six reinforcement learning experts tackling different significant facets of RL. It will also offer the opportunity for contributed material (see below the call for papers and our outstanding program committee). The authors of each accepted paper will prerecord a 10-minute presentation and will also appear in a poster session. Finally, the workshop will have a panel discussing important challenges in the road ahead.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5730 2020\n",
      "Training machine learning models in a centralized fashion often faces significant challenges due to regulatory and privacy concerns in real-world use cases. These include distributed training data, computational resources to create and maintain a central data repository, and regulatory guidelines (GDPR, HIPAA) that restrict sharing sensitive data. Federated learning (FL) is a new paradigm in machine learning that can mitigate these challenges by training a global model using distributed data, without the need for data sharing. The extensive application of machine learning to analyze and draw insight from real-world, distributed, and sensitive data necessitates familiarization with and adoption of this relevant and timely topic among the scientific community.Despite the advantages of federated learning, and its successful application in certain industry-based cases, this field is still in its infancy due to new challenges that are imposed by limited visibility of the training data, potential lack of trust among participants training a single model, potential privacy inferences, and in some cases, limited or unreliable connectivity.The goal of this workshop is to bring together researchers and practitioners interested in FL. This day-long event will facilitate interaction among students, scholars, and industry professionals from around the world to understand the topic, identify technical challenges, and discuss potential solutions. This will lead to an overall advancement of FL and its impact in the community.For a detailed workshop schedule, please visit: http://federated-learning.org/fl-icml-2020/Workshop date: July 18, 2020 (Saturday)Starting at 9 am in US Eastern Daylight Time, https://time.is/EDT\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5731 2020\n",
      "Objects, and the interactions between them, are the foundations on which our understanding of the world is built. Similarly, abstractions centered around the perception and representation of objects play a key role in building human-like AI, supporting high-level cognitive abilities like causal reasoning, object-centric exploration, and problem solving. Indeed, prior works have shown how relational reasoning and control problems can greatly benefit from having object descriptions. Yet, many of the current methods in machine learning focus on a less structured approach in which objects are only implicitly represented, posing a challenge for interpretability and the reuse of knowledge across tasks. Motivated by the above observations, there has been a recent effort to reinterpret various learning problems from the perspective of object-oriented representations.In this workshop, we will showcase a variety of approaches in object-oriented learning, with three particular emphases. Our first interest is in learning object representations in an unsupervised manner. Although computer vision has made an enormous amount of progress in learning about objects via supervised methods, we believe that learning about objects with little to no supervision is preferable: it minimizes labeling costs, and also supports adaptive representations that can be changed depending on the particular situation and goal. The second primary interest of this workshop is to explore how object-oriented representations can be leveraged for downstream tasks such as reinforcement learning and causal reasoning. Lastly, given the central importance of objects in human cognition, we will highlight interdisciplinary perspectives from cognitive science and neuroscience on how people perceive and understand objects.We have invited speakers whose research programs cover unsupervised and supervised 2-D and 3-D perception, reasoning, concept learning, reinforcement learning, as well as psychology and neuroscience. We will additionally source contributed works focusing on unsupervised object-centric representations, applications of such object-oriented representations (such as in reinforcement learning), and object-centric aspects of human cognition. To highlight and support research from a range of different perspectives, our invited speakers vary in their domain of expertise, institution, seniority, and gender. We will also encourage participation from underrepresented groups by providing travel grants courtesy of DeepMind and Kakao Brain. We are also planning to coordinate with the main conference and the speakers to provide remote access to the workshop.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5732 2020\n",
      "Even though supervised learning using large annotated corpora is still the dominant approach in machine learning,  self-supervised learning is gaining considerable popularity.  Applying self-supervised learning to audio and speech sequences, however, remains particularly challenging. Speech signals, in fact, are not only high-dimensional, long, and variable-length sequences, but also entail a complex hierarchical structure that is difficult to infer without supervision (e.g.phonemes, syllables, words). Moreover, speech is characterized by an important variability due to different speaker identities, accents, recording conditions and noises that highly increase the level of complexity.We believe that self-supervised learning will play a crucial role in the future of artificial intelligence, and we think that great research effort is needed to efficiently take advantage of it in audio and speech applications. With our initiative, we wish to foster more progress in the field, and we hope to encourage a discussion amongst experts and practitioners from both academia and industry that might bring different points of view on this topic. Furthermore, we plan to extend the debate to multiple disciplines, encouraging discussions on how insights from other fields (e.g., computer vision and robotics) can be applied to speech, and how findings on speech can be used on other sequence processing tasks. The workshop will be conceived to promote communication and exchange of ideas between machine learning and speech communities. Throughout a series of invited talks, contributed presentations, poster sessions, as well as a panel discussion we want to foster a fruitful scientific discussion that cannot be done with that level of detail during the main ICML conference.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5733 2020\n",
      "Self-driving cars and advanced safety features present one of today’s greatest challenges and opportunities for Artificial Intelligence (AI). Despite billions of dollars of investments and encouraging progress under certain operational constraints, there are no driverless cars on public roads today without human safety drivers. Autonomous Driving research spans a wide spectrum, from modular architectures -- composed of hardcoded or independently learned sub-systems -- to end-to-end deep networks with a single model from sensors to controls. In any system, Machine Learning is a key component. However, there are formidable learning challenges due to safety constraints, the need for large-scale manual labeling, and the complex high dimensional structure of driving data, whether inputs (from cameras, HD maps, inertial measurement units, wheel encoders, LiDAR, radar, etc.) or predictions (e.g., world state representations, behavior models, trajectory forecasts, plans, controls). The goal of this workshop is to explore the frontier of learning approaches for safe, robust, and efficient Autonomous Driving (AD) at scale. The workshop will span both theoretical frameworks and practical issues especially in the area of deep learning.Website: https://sites.google.com/view/aiad2020\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5734 2020\n",
      "Over the years, ML models have steadily grown in complexity, gaining predictivity often at the expense of interpretability. An active research area called explainable AI (or XAI) has emerged with the goal to produce models that are both predictive and understandable. XAI has reached important successes, such as robust heatmap-based explanations of DNN classifiers. From an application perspective, there is now a need to massively engage into new scenarios such as explaining unsupervised / reinforcement learning, as well as producing explanations that are optimally structured for the human. In particular, our planned workshop will cover the following topics:-   Explaining beyond DNN classifiers: random forests, unsupervised learning, reinforcement learning-   Explaining beyond heatmaps: structured explanations, Q/A and dialog systems, human-in-the-loop- Explaining beyond explaining: Improving ML models and algorithms, verifying ML, getting insightsXAI has received an exponential interest in the research community, and awareness of the need to explain ML models have grown in similar proportions in industry and in the sciences. With the sizable XAI research community that has formed, there is now a key opportunity to achieve this push towards successful applications. Our hope is that our proposed XXAI workshop can accelerate this process, foster a more systematic use of XAI to produce improvement on models in applications, and finally, also serves to better identify in which way current XAI methods need to be improved and what kind of theory of XAI is needed.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5735 2020\n",
      "One of the most significant and challenging open problems in Artificial Intelligence (AI) is the problem of Lifelong Learning. Lifelong Machine Learning considers systems that can continually learn many tasks (from one or more domains) over a lifetime. A lifelong learning system efficiently and effectively:1. retains the knowledge it has learned from different tasks;2. selectively transfers knowledge (from previously learned tasks) to facilitate the learning of new tasks;3. ensures the effective and efficient interaction between (1) and (2).Lifelong Learning introduces several fundamental challenges in training models that generally do not arise in a single task batch learning setting. This includes problems like catastrophic forgetting and capacity saturation. This workshop aims to explore solutions for these problems in both supervised learning and reinforcement learning settings.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5736 2020\n",
      "The ever-increasing size and accessibility of vast media libraries has created a demand more than ever for AI-based systems that are capable of organizing, recommending, and understanding such complex data. While this topic has received only limited attention within the core machine learning community, it has been an area of intense focus within the applied communities such as the Recommender Systems (RecSys), Music Information Retrieval (MIR), and Computer Vision communities. At the same time, these domains have surfaced nebulous problem spaces and rich datasets that are of tremendous potential value to machine learning and the AI communities at large.This year's Machine Learning for Media Discovery (ML4MD) aims to build upon the five previous Machine Learning for Music Discovery editions at ICML, broadening the topic area from music discovery to media discovery. The added topic diversity is aimed towards having a broader conversation with the machine learning community and to offer cross-pollination across the various media domains.One of the largest areas of focus in the media discovery space is on the side of content understanding. The recommender systems community has made great advances in terms of collaborative feedback recommenders, but these approaches suffer strongly from the cold-start problem.  As such, recommendation techniques often fall back on content-based machine learning systems, but defining the similarity of media items is extremely challenging as myriad features all play some role (e.g., cultural, emotional, or content features, etc.). While significant progress has been made, these problems remain far from solved.In addition, these complex data present many challenges beyond the development of machine learning systems to model and understand them. One of the largest challenges is scale. One example is commercial music libraries, which span into the tens of millions. However, user-generated content platforms such as YouTube and Pinterest have libraries stretching into the billions--a scale at which many of the traditional approaches discussed in the literature simply cannot perform. On the other side of this problem sits the recent explosion of work in the area of Creative AI. Relevant examples include Google Magenta, Amazon's DeepComposer, who seek to develop algorithms capable of composing and performing completely original (and compelling) works of music. The same also happens in the world of visual media creation (e.g., DeepDream, Deep Fakes). Certain work in this area adds an interesting dimension to the conversation as understanding how content is created is a prerequisite to generating. This workshop proposal is timely in that it will bridge these separate pockets of otherwise very related research. In addition to making progress on the challenges above, we hope to engage the wide AI and machine learning community with our rich problem space, and connect them with the many available datasets the community has to offer.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5737 2020\n",
      "Optimization lies at the heart of many exciting developments in machine learning, statistics and signal processing. As models become more complex and datasets get larger, finding efficient, reliable and provable methods is one of the primary goals in these fields.In the last few decades, much effort has been devoted to the development of first-order methods. These methods enjoy a low per-iteration cost and have optimal complexity, are easy to implement, and have proven to be effective for most machine learning applications. First-order methods, however, have significant limitations: (1) they require fine hyper-parameter tuning, (2) they do not incorporate curvature information, and thus are sensitive to ill-conditioning, and (3) they are often unable to fully exploit the power of distributed computing architectures.Higher-order methods, such as Newton, quasi-Newton and adaptive gradient descent methods, are extensively used in many scientific and engineering domains. At least in theory, these methods possess several nice features: they exploit local curvature information to mitigate the effects of ill-conditioning, they avoid or diminish the need for hyper-parameter tuning, and they have enough concurrency to take advantage of distributed computing environments. Researchers have even developed stochastic versions of higher-order methods, that feature speed and scalability by incorporating curvature information in an economical and judicious manner. However, often higher-order methods are “undervalued.”This workshop will attempt to shed light on this statement. Topics of interest include, but are not limited to, second-order methods, adaptive gradient descent methods, regularization techniques, as well as techniques based on higher-order derivatives.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5738 2020\n",
      "Until recently, Machine Learning has been mostly applied in industry by consulting academics, data scientists within larger companies, and a number of dedicated Machine Learning research labs within a few of the world’s most innovative tech companies. Over the last few years we have seen the dramatic rise of companies dedicated to providing Machine Learning software-as-a-service tools, with the aim of democratizing access to the benefits of Machine Learning. All these efforts have revealed major hurdles to ensuring the continual delivery of good performance from deployed Machine Learning systems. These hurdles range from challenges in MLOps, to fundamental problems with deploying certain algorithms, to solving the legal issues surrounding the ethics involved in letting algorithms make decisions for your business.This workshop will invite papers related to the challenges in deploying and monitoring ML systems. It will encourage submission on: subjects related to MLOps for deployed ML systems (such as testing ML systems, debugging ML systems, monitoring ML systems, debugging ML Models, deploying ML at scale); subjects related to the ethics around deploying ML systems (such as ensuring fairness, trust and transparency of ML systems, providing privacy and security on ML Systems); useful tools and programming languages for deploying ML systems; specific challenges relating to deploying reinforcement learning in ML systemsand performing continual learning and providing continual delivery in ML systems;and finally data challenges for deployed ML systems.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5739 2020\n",
      "The ICML Workshop on Retrospectives in Machine Learning will build upon the success of the 2019 NeurIPS Retrospectives workshop to further encourage the publication of retrospectives. A retrospective of a paper or a set of papers, by its author, takes the form of an informal paper. It provides a venue for authors to reflect on their previous publications, to talk about how their thoughts have changed following publication, to identify shortcomings in their analysis or results, and to discuss resulting extensions. The overarching goal of MLRetrospectives is to improve the science, openness, and accessibility of the machine learning field, by widening what is publishable and helping to identify opportunities for improvement. Retrospectives also give researchers and practitioners unable to attend conferences access to the author’s updated understanding of their work, which would otherwise only be accessible to their immediate circle. The machine learning community would benefit from retrospectives on much of the research which shapes our field, and this workshop will present an opportunity for a few retrospectives to be presented.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5740 2020\n",
      "ML has shown great promise in modeling and predicting complex phenomenon in many scientificdisciples such as predicting cardiovascular risk factors from retinal images, understanding howelectrons behave at the atomic level [3], identifying patterns of weather and climate phenomena, etc.  Further, models are able to learn directly (and better) from raw data as opposed to human selected features.  The ability to interpret the model and find significant predictors couldprovide new scientific insights.Traditionally, the scientific discovery process has been based on careful observations of nat-ural  phenomenon,  followed  by  systematic  human  analysis  (of  hypothesis  generation  and  ex-perimental validation).  ML interpretability has the potential to bring a radically different yetprincipled approach.  While general interpretability relies on ‘human parsing’ (common sense),scientific domains have semi-structured and highly structured bases for interpretation.  Thus,despite differences in data modalities and domains, be it brain sciences, the behavioral sciences,or  material  sciences,  there  is  a  need  for  a  common  set  of  tools  that  address  a  similar  flavor of problem, one of interpretability or fitting models to a known structure. This workshop aims to bring together members from the ML and physical sciences communities to introduce exciting problems to the broader community, and stimulate the productionof new approaches towards solving open scientific problems.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5741 2020\n",
      "One proposed solution towards the goal of designing machines that can extrapolate experience across environments and tasks, are inductive biases. Providing and starting algorithms with inductive biases might help to learn invariances e.g. a causal graph structure, which in turn will allow the agent to generalize across environments and tasks. While some inductive biases are already available and correspond to common knowledge, one key requirement to learn inductive biases from data seems to be the possibility to perform and learn from interventions. This assumption is partially motivated by the accepted hypothesis in psychology about the need to experiment in order to discover causal relationships. This corresponds to an reinforcement learning environment, where the agent can discover causal factors through interventions and observing their effects. We believe that one reason which has hampered progress on building intelligent agents is the limited availability of good inductive biases. Learning inductive biases from data is difficult since this corresponds to an interactive learning setting, which compared to classical regression or classification frameworks is far less understood e.g. even formal definitions of generalization in RL have not been developed.  While Reinforcement Learning has already achieved impressive results, the sample complexity required to achieve consistently good performance is often prohibitively high. This has limited most RL to either games or settings where an accurate simulator is available.  Another issue is that RL agents are often brittle in the face of even tiny changes to the environment (either visual or mechanistic changes) unseen in the training phase.  To build intuition for the scope of the generalization problem in RL, consider the task of training a robotic car mechanic that can diagnose and repair any problem with a car. Current methods are all insufficient in some respect -- on-policy policy gradient algorithms need to cycle through all possible broken cars on every single iteration, off-policy algorithms end up with a mess of instability due to perception and highly diverse data, and model-based methods may struggle to fully estimate a complex web of causality.  In our workshop we hope to explore research and new ideas on topics related to inductive biases, invariances and generalization, including: - What are efficient ways to learn inductive biases from data?- Which inductive biases are most suitable to achieve generalization? - Can we make the problem of generalization in particular for RL more concrete and figure out standard terms for discussing the problem?  - Causality and generalization especially in RL- Model-based RL and generalization.  - Sample Complexity in reinforcement learning. - Can we create models that are robust visual environments, assuming all the underlying mechanics are the same. Should this count as generalization or transfer learning?- Robustness to changes in the mechanics of the environment, such as scaling of rewards.  - Can we create a theoretical understanding of generalization in RL, and understand how it is related to the well developed ideas from statistical learning theory.  - in RL, the training data is collected by the agent and it is affected by the agent's policy. Therefore, the training distribution is not a fixed distribution.  How does this affect how we should think about generalization?  The question of generalization in reinforcement learning is essential to the field’s future both in theory and in practice.  However there are still open questions about the right way to think about generalization in RL, the right way to formalize the problem, and the most important tasks.  This workshop would help to address this issue by bringing together researchers from different backgrounds to discuss these challenges.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5742 2020\n",
      "Normalizing flows are explicit likelihood models using invertible neural networks to construct flexible probability distributions of high-dimensional data.  Compared to other generative models,  the main advantage of normalizing flows is that they can offer exact and efficient likelihood computation and data generation.  Since their recent introduction, flow-based models have seen a significant resurgence of interest in the machine learning community.  As a result, powerful flow-based models have been developed, with successes in density estimation, variational inference, and generative modeling of images, audio and video. This workshop is the 2nd iteration of the ICML 2019 workshop on Invertible Neural Networks and Normalizing Flows.  While the main goal of last year’s workshop was to make flow-based models more accessible to the general machine learning community, as the field is moving forward, we believe there is now a need to consolidate recent progress and connect ideas from related fields.  In light of the interpretation of latent variable models and autoregressive models as flows, this year we expand the scope of the workshop and consider likelihood-based models more broadly, including flow-based models, latent variable models and autoregressive models.  We encourage the researchers to use these models in conjunction to exploit the their benefits at once, and to work together to resolve some common issues of likelihood-based methods, such as mis-calibration of out-of-distribution uncertainty.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5743 2020\n",
      "Machine learning systems are commonly applied to isolated tasks or narrow domains (e.g. control over similar robotic bodies). It is further assumed that the learning system has simultaneous access to all the data points of the tasks at hand. In contrast, Continual Learning (CL) studies the problem of learning from a stream of data from changing domains, each connected to a diﬀerent learning task. The objective of CL is to quickly adapt to new situations or tasks by exploiting previously acquired knowledge, while protecting previous learning from being erased. Meeting the objectives of CL will provide an opportunity for systems to quickly learn new skills given knowledge accumulated in the past and continually extend their capabilities to changing environments, a hallmark of natural intelligence.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5744 2020\n",
      "Deep learning has achieved great success in a variety of tasks such as recognizing objects in images, predicting the sentiment of sentences, or image/speech synthesis by training on a large-amount of data. However, most existing success are mainly focusing on perceptual tasks, which is also known as System I intelligence. In real world, many complicated tasks, such as autonomous driving, public policy decision making, and multi-hop question answering, require understanding the relationship between high-level variables in the data to perform logical reasoning, which is known as System II intelligence. Integrating system I and II intelligence lies in the core of artificial intelligence and machine learning. Graph is an important structure for System II intelligence, with the universal representation ability to capture the relationship between different variables, and support interpretability, causality, and transferability / inductive generalization. Traditional logic and symbolic reasoning over graphs has relied on methods and tools which are very different from deep learning models, such Prolog language, SMT solvers, constrained optimization and discrete algorithms. Is such a methodology separation between System I and System II intelligence necessary? How to build a flexible, effective and efficient bridge to smoothly connect these two systems, and create higher order artificial intelligence? Graph neural networks, have emerged as the tool of choice for graph representation learning, which has led to impressive progress in many classification and regression problems such as chemical synthesis, 3D-vision, recommender systems and social network analysis. However, prediction and classification tasks can be very different from logic/symbolic reasoning.Bits and pieces of evidence can be gleaned from recent literature, suggesting graph neural networks may be a general tool to make such a connection. For example, \\cite{battaglia2018relational,barcelo2019logical} viewed graph neural networks as tools to incorporate explicitly logic reasoning bias. \\cite{kipf2018neural} used graph neural network to reason about interacting systems, \\cite{yoon2018inference,zhang2020efficient} used neural networks for logic and probabilistic inference, \\cite{hudson2019learning, hu2019language} used graph neural networks for reasoning on scene graphs for visual question reasoning, \\cite{qu2019probabilistic} studied reasoning on knowledge graphs with graph neural networks, and \\cite{khalil2017learning, xu2018powerful, velickovic2019neural, sato2019approximation} used graph neural networks for discrete graph algorithms. However, there can still be a long way to go for a satisfactory and definite answers on the ability of graph neural networks for automatically discovering logic rules, and conducting long-range multi-step complex reasoning in combination with perception inputs such as language, vision, spatial and temporal variation.   {\\bf Can graph neural networks be the key bridge to connect System I and System II intelligence? Are there  other more flexible, effective and efficient alternatives?} For instance, \\citep{wang2019satnet} combined max satisfiability solver with deep learning, \\citep{manhaeve2018deepproblog} combined directed graphical and Problog with deep learning, \\citep{skryagin2020splog}~combined sum product network with deep learning, \\citep{silver2019few,alet2019graph}~combined logic reasoning with reinforcement learning. How do these alternative methods compare with graph neural networks for being a bridge?The goal of this workshop is to bring researchers from previously separate fields, such as deep learning, logic/symbolic reasoning, statistical relational learning, and graph algorithms, into a common roof to discuss this potential interface and integration between System I and System intelligence. By providing a venue for the confluence of new advances in theoretical foundations, models and algorithms, as well as empirical discoveries, new benchmarks and impactful applications,\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5745 2020\n",
      "This workshop aims to bring together researchers from academia and industry to discuss major challenges, outline recent advances, and highlight future directions pertaining to novel and existing large-scale real-world experiment design and active learning problems. We aim to highlight new and emerging research opportunities for the machine learning community that arise from the evolving needs to make experiment design and active learning procedures that are theoretically and practically relevant for realistic applications.The intended audience and participants include everyone whose research interests, activities, and applications involve experiment design, active learning, bandit/Bayesian optimization, efficient exploration, and parameter search methods and techniques. We expect the workshop to attract substantial interest from researchers working in both academia and industry. The research of our invited speakers spans both theory and applications, and represents a diverse range of domains where experiment design and active learning are of fundamental importance (including robotics & control, biology, physical sciences, crowdsourcing, citizen science, etc.).The schedule is with respect to UTC (i.e., Universal Time) time zone.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5746 2020\n",
      "This workshop will bring together artificial intelligence (AI) researchers who study the interpretability of AI systems, develop interpretable machine learning algorithms, and develop methodology to interpret black-box machine learning models (e.g., post-hoc interpretations).  This is a very exciting time to study interpretable machine learning, as the advances in large-scale optimization and Bayesian inference that have enabled the rise of black-box machine learning are now also starting to be exploited to develop principled approaches to large-scale interpretable machine learning. Interpretability also forms a key bridge between machine learning and other AI research directions such as machine reasoning and planning.  Participants in the workshop will exchange ideas on these and allied topics.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5747 2020\n",
      "Recent years have witnessed the rising need for learning agents that can interact with humans. Such agents usually involve applications in computer vision, natural language processing, human computer interaction, and robotics. Creating and running such agents call for interdisciplinary research of artificial intelligence, machine learning, and software engineering design, which we abstract as Human in the Loop Learning (HILL). HILL is a modern machine learning paradigm of significant practical and theoretical interest. For HILL, models and humans engage in a two-way dialog to facilitate more accurate and interpretable learning. The workshop aims to bring together researchers and practitioners working on the broad areas of human in the loop learning, ranging from the interactive/active learning algorithm designs for real-world decision making systems (e.g., autonomous driving vehicles, robotic systems, etc.), models with strong explainability, as well as interactive system designs  (e.g., data visualization, annotation systems, etc.). In particular, we aim to elicit new connections among these diverse fields, identifying theory, tools and design principles tailored to practical machine learning workflows. The target audience for the workshop includes people who are interested in using machines to solve problems by having a human be an integral part of the learning process. In this year’s HILL workshop, we emphasize on the interactive/active learning algorithms for real-world decision making systems as well as learning algorithms with strong explainability. We continue the previous effort to provide a platform for researchers to discuss approaches that bridge the gap between humans and machines and get the best of both worlds. We believe the theme of the workshop will be interesting to ICML attendees, especially those who are interested in interdisciplinary study.\n",
      "\n",
      "https://icml.cc/Conferences/2020/Schedule?showEvent=5748 2020\n",
      "Language is one of the most impressive human accomplishments and is believed to be the core to our ability to learn, teach, reason and interact with others. Yet, current state-of-the-art reinforcement learning agents are unable to use or understand human language at all. The ability to integrate and learn from language, in addition to rewards and demonstrations, has the potential to improve the generalization, scope and sample efficiency of agents. Furthermore, many real-world tasks, including personal assistants and general household robots, require agents to process language by design, whether to enable interaction with humans, or simply use existing interfaces. The aim of our workshop is to advance this emerging field of research by bringing together researchers from several diverse communities to discuss recent developments in relevant research areas such as instruction following and embodied language learning, and identify the most important challenges and promising research avenues.\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8347 2021\n",
      "Machine learning (ML) systems have been increasingly used in many applications, ranging from decision-making systems to safety-critical tasks. While the hope is to improve decision-making accuracy and societal outcomes with these ML models, concerns have been incurred that they can inflict harm if not developed or used with care. It has been well-documented that ML models can: (1) inherit pre-existing biases and exhibit discrimination against already-disadvantaged or marginalized social groups; (2) be vulnerable to security and privacy attacks that deceive the models and leak the training data's sensitive information; (3) make hard-to-justify predictions with a lack of transparency. Therefore, it is essential to build socially responsible ML models that are fair, robust, private, transparent, and interpretable.Although extensive studies have been conducted to increase trust in ML, many of them either focus on well-defined problems that enable nice tractability from a mathematical perspective but are hard to adapt to real-world systems, or they mainly focus on mitigating risks in real-world applications without providing theoretical justifications. Moreover, most work studies those issues separately; the connections among them are less well-understood. This workshop aims to build connections by bringing together both theoretical and applied researchers from various communities (e.g., machine learning, fairness & ethics, security, privacy, etc.). We aim to synthesize promising ideas and research directions, as well as strengthen cross-community collaborations. We hope to chart out important directions for future work. We have an advisory committee and confirmed speakers whose expertise represents the diversity of the technical problems in this emerging research field.\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8348 2021\n",
      "Machine learning systems are commonly applied to isolated tasks (such as image recognition or playing chess) or narrow domains (such as control over similar robotic bodies). It is further assumed that the learning system has simultaneous access to all annotated data points of the tasks at hand. In contrast, Continual Learning (CL), also referred to as Lifelong or Incremental Learning, studies the problem of learning from a stream of data from changing domains, each connected to a different learning task. The objective of CL is to quickly adapt to new situations or tasks by exploiting previously acquired knowledge, while protecting previous learning from being erased.Significant advances have been made in CL over the past few years, mostly through empirical investigations and benchmarking. However, theoretical understanding is still lagging behind. For instance, while Catastrophic Forgetting (CF) is a recurring ineffectiveness that most works try to tackle, little understanding is provided in the literature from a theoretical point of view. Many real life applications share common assumptions and settings with CL, what are the convergence guarantees when deploying a certain method? If memory capacity is an important constraint for replay methods, how can we select the minimal examples such that CF is minimized? While answers to the questions above are key ingredients to design better heuristics, very little theoretical guidance is provided in the literature.The aim of this workshop is to achieve an understanding of different components of continual learning to bridge the gap with empirical results. Furthermore, we are also interested in submissions that draw connections between Continual Learning and other areas, such as Neuroscience and Meta-learning. For more info visit our workshop website\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8349 2021\n",
      "As causality enjoys increasing attention in various areas of machine learning, this workshop turns the spotlight on the assumptions behind the successful application of causal inference techniques. It is well known that answering causal queries from observational data requires strong and sometimes untestable assumptions. On the theoretical side, a whole host of settings as been established in which causal effects are identifiable and consistently estimable under a set of by now considered \"standard\" assumptions. While these can be reasonable in specific scenarios, they were often at least partially motivated by rendering estimation theoretically feasible. Such assumptions tell us what we would need to assert about the data generating process in order to be able to answer causal queries. Unfortunately, in applications we often find them taken for granted as properties that can safely be assumed to hold without further scrutiny. This starts with fundamentally untestable assumptions such as the stable unit treatment value assumption or ignorability and continues to no interference, faithfulness, positivity or overlap, no unobserved confounding and even reaches blanket one-size-fits all assumptions on the linearity of structural equations or the additivity of noise. This situation may lead practitioners to either believe that well founded causal inference is unattainable altogether, or that established off-the-shelf methods can be trusted to deliver reliable causal estimates in virtually any situation. Similarly, as ideas from causality are increasingly picked up by researchers in deep-, reinforcement-, or meta-learning, there is a risk that the role of assumptions for causal inference gets lost in translation. One of the main goals of this workshop is to help the research community and practitioners understand the concrete challenges of trustworthy assumptions for effective causal inference.\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8350 2021\n",
      "The focus of this workshop is on the use of machine learning to help in addressing climate change, encompassing mitigation efforts (reducing the severity of climate change), adaptation measures (preparing for unavoidable consequences), and climate science (our understanding of the climate and future climate predictions). Topics within the scope of this workshop include climate-relevant applications of machine learning to the power sector, buildings and transportation infrastructure, agriculture and land use, extreme event prediction, disaster response, climate policy, and climate finance. The goals of the workshop are: (1) to showcase high-impact applications of ML to climate change mitigation, adaptation, and climate science, (2) to demonstrate that the associated ML methods are interesting in their own right, (3) to encourage fruitful collaboration between the ML community and a diverse set of researchers and practitioners from climate change-related fields, and (4) to promote dialogue with decision-makers in the private and public sectors, ensuring that the works presented in this workshop have impact on the thoughtful deployment of ML in climate solutions. Building on our previous workshops in this series, this workshop will have a particular focus on ML for the assessment and implementation of objectives set under the Paris Agreement, though submitted works may be on any topic at the intersection of ML and climate change.\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8351 2021\n",
      "A growing number of machine learning problems involve finding subsets of data points. Examples range from selecting subset of labeled or unlabeled data points, to subsets of features or model parameters, to selecting subsets of pixels, keypoints, sentences etc. in image segmentation, correspondence and summarization problems. The workshop would encompass a wide variety of topics ranging from theoretical aspects of subset selection e.g. coresets, submodularity, determinantal point processes, to several practical applications, {\\em e.g.}, time and energy efficient learning, learning under resource constraints, active learning, human assisted learning, feature selection, model compression, feature induction, {\\em etc.} \n",
      "We believe that this workshop is very timely since, a) subset selection is naturally emerging and has often been considered in isolation in many of the above applications, and b) by connecting researchers working on both the theoretical and application domains above, we can foster a much needed discussion on reusing a several technical innovations across these subareas and applications. Furthermore, we would also like to connect researchers working on the theoretical foundations of subset selection (in areas such as coresets and submodularity) with researchers working in applications (such as feature selection, active learning, data efficient learning, model compression, and human assisted machine learning).\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8352 2021\n",
      "The rising prevalence of mental illness has posed a growing global burden, with one in four people adversely affected at some point in their lives, accounting for 32.4% of years lived with disability. This has only been exacerbated during the current pandemic, and while the capacity of acute care has been significantly increased in response to the crisis, it has at the same time led to the scaling back of many mental health services. This, together with the advances in the field of machine learning (ML), has motivated exploration of how machine learning methods can be applied to the provision of more effective and efficient mental healthcare, from varied approaches to continual monitoring of individual mental health  or identification of mental health issues through inferences about behaviours on social media, online searches or mobile apps, to predictive models for early diagnosis and intervention, understanding disease progression or recovery, and the personalization of therapies.  This workshop aims to bring together clinicians, behavioural scientists and machine learning researchers working in various facets of mental health and care provision, to identify the key opportunities and challenges in developing solutions for this domain, and discussing the progress made.\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8353 2021\n",
      "Self-supervised learning (SSL) is an unsupervised approach for representation learning without relying on human-provided labels. It creates auxiliary tasks on unlabeled input data and learns representations by solving these tasks. SSL has demonstrated great success on images, texts, robotics, etc. On a wide variety of tasks, SSL without using human-provided labels achieves performance that is close to fully supervised approaches. Existing SSL research mostly focuses on perception tasks such as image classification, speech recognition, text classification, etc. SSL for reasoning tasks (e.g., symbolic reasoning on graphs, relational reasoning in computer vision, multi-hop reasoning in NLP) is largely ignored. In this workshop, we aim to bridge this gap. We bring together SSL-interested researchers from various domains to discuss how to develop SSL methods for reasoning tasks, such as how to design pretext tasks for symbolic reasoning, how to develop contrastive learning methods for relational reasoning, how to develop SSL approaches to bridge reasoning and perception, etc. Different from previous SSL-related workshops which focus on perception tasks, our workshop focuses on promoting SSL research for reasoning.\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8354 2021\n",
      "Reinforcement learning (RL) is a general learning, predicting, and decision making paradigm and applies broadly in many disciplines, including science, engineering and humanities. RL has seen prominent successes in many problems, such as games, robotics, recommender systems.  However, applying RL in the real world remains challenging, and a natural question is: Why isn’t RL used even more often and how can we improve this?The main goals of the workshop are to: (1) identify key research problems that are critical for the success of real-world applications; (2) report progress on addressing these critical issues; and (3) have practitioners share their success stories of applying RL to real-world problems, and the insights gained from such applications.We invite paper submissions successfully applying RL algorithms to real-life problems and/or addressing practically relevant RL issues.  Our topics of interest are general, including (but not limited to): 1) practical RL algorithms, which covers all algorithmic challenges of RL, especially those that directly address challenges faced by real-world applications; 2) practical issues: generalization, sample efficiency, exploration, reward, scalability, model-based learning, prior knowledge, safety, accountability, interpretability, reproducibility, hyper-parameter tuning; and 3) applications.We have 6 premier panel discussions and 70+ great papers/posters. Welcome!\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8355 2021\n",
      "The proposed workshop pays a special interests in theoretic foundations, limitations, and new application trends in the scope of XAI. These issues reflect new bottlenecks in the future development of XAI, for example: (1) no theoretic definition of XAI and no solid and widely-used formulation for even a specific explanation task. (2) No sophisticated formulation of the essence of ``semantics'' encoded in a DNN. (3) How to bridge the gap between connectionism and symbolism in AI research has not been sophisticatedly explored. (4) How to evaluate the correctness and trustworthiness of an explanation result is still an open problem. (5) How to bridge the intuitive explanation (e.g., the attribution/importance-based explanation) and a DNN's representation capacity (e.g., the generalization power) is still a significant challenge. (6) Using the explanation to guide the architecture design or substantially boost the performance of a DNN is a bottleneck. Therefore, this workshop aims to bring together researchers, engineers as well as industrial practitioners, who concern about the interpretability, safety, and reliability of artificial intelligence. In this workshop, we hope to use a broad discussion on the above bottleneck issues to explore new critical and constructive views of the future development of XAI. Research outcomes are also expected to profoundly influences critical industrial applications such as medical diagnosis, finance, and autonomous driving.Accepted papers: https://arxiv.org/html/2107.08821\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8356 2021\n",
      "As the use of machine learning (ML) becomes ubiquitous, there is a growing understanding and appreciation for the role that data plays for building successful ML solutions. Classical ML research has been primarily focused on learning algorithms and their guarantees. Recent progress has shown that data is playing an increasingly central role in creating ML solutions, such as the massive text data used for training powerful language models, (semi-)automatic engineering of weak supervision data that enables applications in few-labels settings, and various data augmentation and manipulation techniques that lead to performance boosts on many real world tasks. On the other hand, data is one of the main sources of security, privacy, and bias issues in deploying ML solutions in the real world. This workshop will focus on the new perspective of machine learning for data --- specifically how ML techniques can be used to facilitate and automate a range of data operations (e.g. ML-assisted labeling, synthesis, selection, augmentation), and the associated challenges of quality, security, privacy and fairness for which ML techniques can also enable solutions.\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8357 2021\n",
      "Modern machine learning models are often highly over-parameterized. The prime examples are neural network architectures achieving state-of-the-art performance, which have many more parameters than training examples. While these models can empirically perform very well, they are not well understood. Worst-case theories of learnability do not explain their behavior. Indeed, over-parameterized models sometimes exhibit \"benign overfitting\", i.e., they have the power to perfectly fit training data (even data modified to have random labels), yet they achieve good performance on the test data. There is evidence that over-parameterization may be helpful both computational and statistically, although attempts to use phenomena like double/multiple descent to explain that over-parameterization helps to achieve small test error remain controversial. Besides benign overfitting and double/multiple descent, many other interesting phenomena arise due to over-parameterization, and many more may have yet to be discovered. Many of these effects depend on the properties of data, but we have only simplistic tools to measure, quantify, and understand data. In light of rapid progress and rapidly shifting understanding, we believe that the time is ripe for a workshop focusing on understanding over-parameterization from multiple angles.Gathertown room1 link: https://eventhosts.gather.town/DbHJbA5ArXpTIoap/icml-oppo-2021Gathertown room2 link: https://eventhosts.gather.town/UtqQ3jSJ7wnN0anj/icml-oppo-2021-room-2\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8358 2021\n",
      "Applying machine learning (ML) in healthcare is gaining momentum rapidly. However, the black-box characteristics of existing ML approaches inevitably lead to less interpretability and verifiability in making clinical predictions. To enhance the interpretability of medical intelligence, it becomes critical to develop methodologies to explain predictions as these systems are pervasively being introduced to the healthcare domain, which requires a higher level of safety and security. Such methodologies would make medical decisions more trustworthy and reliable for physicians, which could facilitate the deployment ultimately. On the other hand, it is also essential to develop more interpretable and transparent ML systems. For instance, by exploiting structured knowledge or prior clinical information, one can design models to learn aspects more coherent with clinical reasoning. Also, it may help mitigate biases in the learning process, or identify more relevant variables for making medical decisions.In this workshop, we aim to bring together researchers in ML, computer vision, healthcare, medicine, NLP, and clinical fields to facilitate discussions including related challenges, definition, formalisms, evaluation protocols regarding interpretable medical machine intelligence. Additionally, we will also introduce possible solutions such as logic and symbolic reasoning over medical knowledge graphs, uncertainty quantification, composition models, etc. We hope that the proposed workshop is fruitful in offering a step toward building autonomous clinical decision systems with a higher-level understanding of interpretability.\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8359 2021\n",
      "Training machine learning models in a centralized fashion often faces significant challenges due to regulatory and privacy concerns in real-world use cases. These include distributed training data, computational resources to create and maintain a central data repository, and regulatory guidelines (GDPR, HIPAA) that restrict sharing sensitive data. Federated learning (FL) is a new paradigm in machine learning that can mitigate these challenges by training a global model using distributed data, without the need for data sharing. The extensive application of machine learning to analyze and draw insight from real-world, distributed, and sensitive data necessitates familiarization with and adoption of this relevant and timely topic among the scientific community. Despite the advantages of FL, and its successful application in certain industry-based cases, this field is still in its infancy due to new challenges that are imposed by limited visibility of the training data, potential lack of trust among participants training a single model, potential privacy inferences, and in some cases, limited or unreliable connectivity.The goal of this workshop is to bring together researchers and practitioners interested in FL. This day-long event will facilitate interaction among students, scholars, and industry professionals from around the world to understand the topic, identify technical challenges, and discuss potential solutions. This will lead to an overall advancement of FL and its impact in the community, while noting that FL has become an increasingly popular topic in the ICML community in recent years.\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8360 2021\n",
      "Normalizing flows are explicit likelihood models (ELM) characterized by a flexible invertible reparameterization of high-dimensional probability distributions. Unlike other ELMs, they offer both exact and efficient likelihood computation and data generation.  Since their recent introduction, flow-based models have seen a significant resurgence of interest in the machine learning community.  As a result, powerful flow-based models have been developed, with successes in density estimation, variational inference, and generative modeling of images, audio and video. \n",
      "As the field is moving forward, the main goal of the workshop is to consolidate recent progress and connect ideas from related fields.  Over the past few years, we’ve seen that normalizing flows are deeply connected to latent variable models, autoregressive models, and more recently, diffusion-based generative models.  This year, we would like to further push the forefront of these explicit likelihood models through the lens of invertible reparameterization.  We encourage researchers to use these models in conjunction to exploit the their benefits at once, and to work together to resolve some common issues of likelihood-based methods, such as mis-calibration of out-of-distribution uncertainty.\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8361 2021\n",
      "One of the fundamental promises of deep learning is its ability to build increasingly meaningful representations of data from complex but raw inputs. These techniques demonstrate remarkable efficacy on high dimensional data with unique proximity structures (image, natural language, graphs). \n",
      "Not only are these types of data prevalent in financial services and e-commerce, but also they often capture extremely interesting aspects of social and economic behavior. For example, financial transactions and online purchases can be viewed as edges on graphs of economic activity. To date, these graphs are far less studied than social networks, though they provide a unique look at behavior, social structures, and risk. ​Meanwhile, activity or transaction sequences, usually determined by user sessions, can reflect the users’ long term and short term interests, which can be modeled by sequential models, and used to predict the user’s future activities. Although language models have been explored in session data modeling, how to re-use the representations learned from one job to another job effectively is still an open question.\n",
      "Our goal is to bring together researchers from different domains to discuss the application of representation learning to financial services and e-commerce. For the first time, four major e-commerce companies (Amazon, Walmart, Alibaba and eBay) and two banks (JP Morgan and Capital One) have come together to organize this workshop along with researchers from academia. A shared goal across these industries and application areas is to transform large-scale representational data into tangible revenue for businesses. Towards this goal, our confirmed invited speakers will share diverse perspectives on ways that representation learning can be used to solve problems in financial services and e-commerce. This will also be a forum to share how research on financial services and e-commerce data provides unique insights into socio-economic behavior.\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8362 2021\n",
      "Recent years have witnessed the rising need for the machine learning systems that can interact with humans in the learning loop. Such systems can be applied to computer vision, natural language processing, robotics, and human computer interaction. Creating and running such systems call for interdisciplinary research of artificial intelligence, machine learning, and software engineering design, which we abstract as Human in the Loop Learning (HILL). The HILL workshop aims to bring together researchers and practitioners working on the broad areas of HILL, ranging from the interactive/active learning algorithms for real-world decision making systems (e.g., autonomous driving vehicles, robotic systems, etc.), lifelong learning systems that retain knowledge from different tasks and selectively transfer knowledge to learn new tasks over a lifetime, models with strong explainability, as well as interactive system designs  (e.g., data visualization, annotation systems, etc.). The HILL workshop continues the previous effort to provide a platform for researchers from interdisciplinary areas to share their recent research. In this year’s workshop, a special feature is to encourage the debate between HILL and label-efficient learning: Are these two learning paradigms contradictory with each other, or can they be organically combined to create a more powerful learning system? We believe the theme of the workshop will be of interest for broad ICML attendees, especially those who are interested in interdisciplinary study.\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8363 2021\n",
      "Machine learning is increasingly used to inform decision-making in sensitive situations where decisions have consequential effects on individuals' lives. In these settings, in addition to requiring models to be accurate and robust, socially relevant values such as fairness, privacy, accountability, and explainability play an important role for the adoption and impact of said technologies. In this workshop, we focus on algorithmic recourse, which is concerned with providing explanations and recommendations to individuals who are unfavourably treated by automated decision-making systems. Specifically, we plan to facilitate workshop interactions that will shed light onto the following 3 questions: (i) What are the practical, legal and ethical considerations that decision-makers need to account for when providing recourse? (ii) How do humans understand and act based on recourse explanations from a psychological and behavioral perspective? (iii) What are the main technical advances in explainability and causality in ML required for achieving recourse? Our ultimate goal is to foster conversations that will help bridge the gaps arising from the interdisciplinary nature of algorithmic recourse and contribute towards the wider adoption of such methods.\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8364 2021\n",
      "Unsupervised learning has begun to deliver on its promise in the recent past with tremendous progress made in the fields of natural language processing and computer vision whereby large scale unsupervised pre-training has enabled fine-tuning to downstream supervised learning tasks with limited labeled data. This is particularly encouraging and appealing in the context of reinforcement learning considering that it is expensive to perform rollouts in the real world with annotations either in the form of reward signals or human demonstrations. We therefore believe that a workshop in the intersection of unsupervised and reinforcement learning is timely and we hope to bring together researchers with diverse views on how to make further progress in this exciting and open-ended subfield.\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8365 2021\n",
      "The empirical success of state-of-the-art machine learning (ML) techniques has outpaced their theoretical understanding. Deep learning models, for example, perform far better than classical statistical learning theory predicts, leading to its widespread use by Industry and Government. At the same time, the deployment of ML systems that are not fully understood often leads to unexpected and detrimental individual-level impact. Finally, the large-scale adoption of ML means that ML systems are now critical infrastructure on which millions rely. In the face of these challenges, there is a critical need for theory that provides rigorous performance guarantees for practical ML models; guides the responsible deployment of ML in applications of social consequence; and enables the design of reliable ML systems in large-scale, distributed environments.\n",
      "For decades, information theory has provided a mathematical foundation for the systems and algorithms that fuel the current data science revolution. Recent advances in privacy, fairness, and generalization bounds demonstrate that information theory will also play a pivotal role in the next decade of ML applications: information-theoretic methods can sharpen generalization bounds for deep learning, provide rigorous guarantees for compression of neural networks, promote fairness and privacy in ML training and deployment, and shed light on the limits of learning from noisy data.\n",
      "We propose a workshop that brings together researchers and practitioners in ML and information theory to encourage knowledge transfer and collaboration between the sister fields. For information theorists, the workshop will highlight novel and socially-critical research directions that promote reliable, responsible, and rigorous development of ML. Moreover, the workshop will expose ICML attendees to emerging information-theoretic tools that may play a critical role in the next decade of ML applications.\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8366 2021\n",
      "The ICML Workshop on Computational Biology will highlight how machine learning approaches can be tailored to making discoveries with biological data. Practitioners at the intersection of computation, machine learning, and biology are in a unique position to frame problems in biomedicine, from drug discovery to vaccination risk scores, and the Workshop will showcase such recent research. Commodity lab techniques lead to the proliferation of large complex datasets, and require new methods to interpret these collections of high-dimensional biological data, such as genetic sequences, cellular features or protein structures, and imaging datasets. These data can be used to make new predictions towards clinical response, to uncover new biology, or to aid in drug discovery. This workshop aims to bring together interdisciplinary machine learning researchers working at the intersection of machine learning and biology that includes areas such as computational genomics; neuroscience; metabolomics; proteomics; bioinformatics; cheminformatics; pathology; radiology; evolutionary biology; population genomics; phenomics; ecology, cancer biology; causality; representation learning and disentanglement to present recent advances and open questions to the machine learning community.  The workshop is a sequel to the WCB workshops we organized in the last five years at ICML, which had excellent line-ups of talks and were well-received by the community. Every year, we received 60+ submissions. After multiple rounds of rigorous reviewing, around 50 submissions were selected from which the best set of papers were chosen for Contributed talks and Spotlights and the rest were invited for Poster presentations. We have a steadfast and growing base of reviewers making up the Program Committee. For two of the previous editions, a special issue of Journal of Computational Biology has been released with extended versions of a selected set of accepted papers.\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8367 2021\n",
      "A key challenge for the successful deployment of many real world human-facing automated sequential decision-making systems is the need for human-AI collaboration. Effective collaboration ensures that the complementary abilities and skills of the human-users and the AI system are leveraged to maximize utility. This is for instance important in applications such as autonomous driving, in which a human user’s skill might be required in safety critical situations, or virtual personal assistants, in which a human user can perform real-world physical interactions which the AI system cannot. Facilitating such collaboration requires cooperation, coordination, and communication, e.g., in the form of accountability, teaching interactions, provision of feedback, etc. Without effective human-AI collaboration, the utility of automated sequential decision-making systems can be severely limited. Thus there is a surge of interest in better facilitating human-AI collaboration in academia and industry. Most existing research has focussed only on basic approaches for human-AI collaboration with little focus on long-term interactions and the breadth needed for next-generation applications. In this workshop we bring together researchers to advance this important topic, focussing on the following three directions: (a) Accountability and trust; (b) Adaptive behavior for long-term collaboration; (c) Robust collaboration under mismatch.\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8368 2021\n",
      "Until recently, many industrial Machine Learning applications have been the remit of consulting academics, data scientists within larger companies, and a number of dedicated Machine Learning research labs within a few of the world’s most innovative tech companies.  Over the last few years we have seen the dramatic rise of companies dedicated to providing Machine Learning software-as-a-service tools, with the aim of democratizing access to the benefits of Machine Learning. All these efforts have revealed major hurdles to ensuring the continual delivery of good performance from deployed Machine Learning systems. These hurdles range from challenges in MLOps, to fundamental problems with deploying certain algorithms, to solving the legal issues surrounding the ethics involved in letting algorithms make decisions for your business.This workshop will invite papers related to the challenges in deploying and monitoring ML systems. It will encourage submission on subjects related to: MLOps for deployed ML systems; the ethics around deploying ML systems; useful tools and programming languages for deploying ML systems; specific challenges relating to deploying reinforcement learning in ML systems and performing continual learning and providing continual delivery in ML systems;and finally data challenges for deployed ML systems.We will also invite the submission of open problems and encourage the discussion (through two live panels) on topics related to the areas of: \"Deploying machine learning applications in the legal system\" and \"Deploying machine learning on devices or constrained hardware\".These subjects represent a wealth of topical and high-impact issues for the community to work on.\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8369 2021\n",
      "Optimization lies at the heart of many exciting developments in machine learning, statistics and signal processing. As models become more complex and datasets get larger, finding efficient, reliable and provable methods is one of the primary goals in these fields.In the last few decades, much effort has been devoted to the development of first-order methods. These methods enjoy a low per-iteration cost and have optimal complexity, are easy to implement, and have proven to be effective for most machine learning applications. First-order methods, however, have significant limitations: (1) they require fine hyper-parameter tuning, (2) they do not incorporate curvature information, and thus are sensitive to ill-conditioning, and (3) they are often unable to fully exploit the power of distributed computing architectures.Higher-order methods, such as Newton, quasi-Newton and adaptive gradient descent methods, are extensively used in many scientific and engineering domains. At least in theory, these methods possess several nice features: they exploit local curvature information to mitigate the effects of ill-conditioning, they avoid or diminish the need for hyper-parameter tuning, and they have enough concurrency to take advantage of distributed computing environments. Researchers have even developed stochastic versions of higher-order methods, that feature speed and scalability by incorporating curvature information in an economical and judicious manner. However, often higher-order methods are “undervalued.”This workshop will attempt to shed light on this statement. Topics of interest include, but are not limited to, second-order methods, adaptive gradient descent methods, regularization techniques, as well as techniques based on higher-order derivatives.\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8370 2021\n",
      "Adversarial machine learning is a new gamut of technologies that aim to study the vulnerabilities of ML approaches and detect malicious behaviors in adversarial settings. The adversarial agents can deceive an ML classifier by significantly altering its response with imperceptible perturbations to the inputs. Although it is not to be alarmist, researchers in machine learning are responsible for preempting attacks and building safeguards, especially when the task is critical for information security and human lives. We need to deepen our understanding of machine learning in adversarial environments. While the negative implications of this nascent technology have been widely discussed, researchers in machine learning are yet to explore their positive opportunities in numerous aspects. The positive impacts of adversarial machine learning are not limited to boost the robustness of ML models but cut across several other domains. Since there are both positive and negative applications of adversarial machine learning, tackling adversarial learning to its use in the right direction requires a framework to embrace the positives. This workshop aims to bring together researchers and practitioners from various communities (e.g., machine learning, computer security, data privacy, and ethics) to synthesize promising ideas and research directions and foster and strengthen cross-community collaborations on both theoretical studies and practical applications. Different from the previous workshops on adversarial machine learning, our proposed workshop seeks to explore the prospects besides reducing the unintended risks for sophisticated ML models.This is a one-day workshop, planned with a 10-minute opening, 11 invited keynotes, about 9 contributed talks, 2 poster sessions, and 2 special sessions for panel discussion about the prospects and perils of adversarial machine learning.The workshop is kindly sponsored by RealAI Inc. and Bosch.\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8371 2021\n",
      "Machine learning (ML) has achieved considerable successes in recent years, but this success often relies on human experts, who construct appropriate features, design learning architectures, set their hyperparameters, and develop new learning algorithms. Driven by the demand for robust, off-the-shelf ML methods from an ever-growing community, the research area of AutoML targets the progressive automation of machine learning aiming to make effective methods available to everyone. Hence, the workshop targets a broad audience ranging from core ML researchers in different fields of ML connected to AutoML, such as neural architecture search (NAS), hyperparameter optimization, meta-learning, and learning-to-learn, to domain experts aiming to apply ML to new types of problems.\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8372 2021\n",
      "While over many years we have witnessed numerous impressive demonstrations of the power of various reinforcement learning (RL) algorithms, and while much progress was made on the theoretical side as well, the theoretical understanding of the challenges that underlie RL is still rather limited. The best-studied problem settings, such as learning and acting in finite state-action Markov decision processes, or simple linear control systems fail to capture the essential characteristics of seemingly more practically relevant problem classes, where the size of the state-action space is often astronomical, the planning horizon is huge, the dynamics is complex, interaction with the controlled system is not permitted, or learning has to happen based on heterogeneous offline data, etc. To tackle these diverse issues, more and more theoreticians with a wide range of backgrounds came to study RL and have proposed numerous new models along with exciting novel developments on both algorithm design and analysis. The workshop's goal is to highlight advances in theoretical RL and bring together researchers from different backgrounds to discuss RL theory from different perspectives: modeling, algorithm, analysis, etc.\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8373 2021\n",
      "Visit https://sites.google.com/berkeley.edu/dfuq21/ for details!While improving prediction accuracy has been the focus of machine learning in recent years, this alone does not suffice for reliable decision-making. Deploying learning systems in consequential settings also requires calibrating and communicating the uncertainty of predictions. A recent line of work we call distribution-free predictive inference (i.e., conformal prediction and related methods) has developed a set of methods that give finite-sample statistical guarantees for any (possibly incorrectly specified) predictive model and any (unknown) underlying distribution of the data, ensuring reliable uncertainty quantification (UQ) for many prediction tasks. This line of work represents a promising new approach to UQ with complex prediction systems but is relatively unknown in the applied machine learning community. Moreover, much remains to be done integrating distribution-free methods with existing approaches to UQ via calibration (e.g. with temperature scaling) -- little work has been done to bridge these two worlds. To facilitate the emerging topics on distribution-free methods, the proposed workshop has two goals. First, to bring together researchers in distribution-free methods with researchers specializing in calibration techniques to catalyze work at this interface. Second, to introduce distribution-free methods to a wider ML audience. Given the important recent emphasis on the reliable real-world performance of ML models, we believe a large fraction of ICML attendees will find this workshop highly relevant.\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8374 2021\n",
      "There has been growing interest in ensuring that deep learning systems are robust and reliable.  Challenges arise when models receive samples drawn from outside the training distribution. For example, a neural network tasked with classifying handwritten digits may assign high confidence predictions to cat images. Anomalies are frequently encountered when deploying ML models in the real world. Well-calibrated predictive uncertainty estimates are indispensable for many machine learning applications, such as self-driving vehicles and medical diagnosis systems. Generalization to unseen and worst-case inputs is also essential for robustness to distributional shift. In order to have ML models safely deployed in open environments, we must deepen technical understanding in the following areas: (1) Learning algorithms that can detect changes in data distribution (e.g. out-of-distribution examples) and improve out-of-distribution generalization (e.g. temporal, geographical, hardware, adversarial shifts);(2) Mechanisms to estimate and calibrate confidence produced by neural networks in typical and unforeseen scenarios; (3) Guide learning towards an understanding of the underlying causal mechanisms that can guarantee robustness with respect to distribution shift.In order to achieve these goals, it is critical to dedicate substantial effort on(4) Creating benchmark datasets and protocols for evaluating model performance under distribution shift(5) Studying key applications of robust and uncertainty-aware deep learning (e.g., computer vision, robotics, self-driving vehicles, medical imaging), as well as broader machine learning tasks.This workshop will bring together researchers and practitioners from the machine learning communities to foster future collaborations. Our agenda will feature invited speakers, contributed talks, poster sessions in multiple time-zones and a panel discussion on fundamentally important directions for robust and reliable deep learning.\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8375 2021\n",
      "Time series is one of the fastest growing and richest types of data. In a variety of domains including dynamical systems, healthcare, climate science and economics, there have been increasing amounts of complex dynamic data due to a shift away from parsimonious, infrequent measurements to nearly continuous real-time monitoring and recording. This burgeoning amount of new data calls for novel theoretical and algorithmic tools and insights.The goals of our workshop are to: (1) highlight the fundamental challenges that underpin learning from time series data (e.g. covariate shift, causal inference, uncertainty quantification), (2) discuss recent developments in theory and algorithms for tackling these problems, and (3) explore new frontiers in time series analysis and their connections with emerging fields such as causal discovery and machine learning for science. In light of the recent COVID-19 outbreak, we also plan to have a special emphasis on non-stationary dynamics, causal inference, and their applications to public health at our workshop.Time series modeling has a long tradition of inviting novel approaches from many disciplines including statistics, dynamical systems, and the physical sciences. This has led to broad impact and a diverse range of applications, making it an ideal topic for the rapid dissemination of new ideas that take place at ICML. We hope that the diversity and expertise of our speakers and attendees will help uncover new approaches and break new ground for these challenging and important settings. Our previous workshops have received great popularity at ICML, and we envision our workshop will continue to appeal to the ICML audience and stimulate many interdisciplinary discussions.Gather.Town: Morning Poster: https://eventhosts.gather.town/app/4H5nUSQOWXc9pC43/tsw-poster-room-1Afternoon Poster: https://eventhosts.gather.town/app/zIejrcEqf10T4UKT/tsw-poster-room-2\n",
      "\n",
      "https://icml.cc/Conferences/2021/Schedule?showEvent=8376 2021\n",
      "Differential privacy is a promising approach to privacy-preserving data analysis. It has been the subject of a decade of intense scientific study, and has now been deployed in products at government agencies such as the U.S. Census Bureau and companies like Microsoft, Apple, and Google.  MIT Technology Review named differential privacy one of 10 breakthrough technologies of 2020.Since data privacy is a pervasive concern, differential privacy has been studied by researchers from many distinct communities, including machine learning, statistics, algorithms, computer security, cryptography, databases, data mining, programming languages, social sciences, and law. We believe that this combined effort across a broad spectrum of computer science is essential for differential privacy to realize its full potential.  To this end, our workshop will stimulate discussion among participants about both the state-of-the-art in differential privacy and the future challenges that must be addressed to make differential privacy more practical.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13446 2022\n",
      "The Disinformation Countermeasures and Machine Learning (DisCoML) workshop at ICML 2022 in Baltimore will address machine learning techniques to counter disinformation. Today, disinformation is an important challenge that all governments and their citizens face, affecting politics, public health, financial markets, and elections. Specific examples such as lynchings catalyzed by disinformation spread over social media highlight that the threat it poses crosses social scales and boundaries. This threat even extends into the realm of military combat, as a recent NATO StratCom experiment highlighted. Machine learning plays a central role in the production and propagation of dissemination. Bad actors scale disinformation operations by using ML-enabled bots, deepfakes, cloned websites, and forgeries. The situation is exacerbated by proprietary algorithms of search engines and social media platforms, driven by advertising models, that can effectively isolate internet users from alternative information and viewpoints. In fact, social media's business model, with its behavioral tracking algorithms, is arguably optimized for launching a global pandemic of cognitive hacking. Machine learning is also essential for identifying and inhibiting the spread of disinformation at internet speed and scale, but DisCoML welcomes approaches that contribute to countering disinformation in a broad sense. While the \"cybersecurity paradox\"–i.e. increased technology spending has not equated to an improved security posture–also applies to disinformation and indicates the need to address human behavior, there is an arms race quality to both problems. This suggests that technology, and ML in particular, will play a central role in countering disinformation well into the future. DisCoML will provide a forum for bringing leading researchers together and enabling stakeholders and policymakers to get up to date on the latest developments in the field.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13447 2022\n",
      "Much of the data that is fueling current rapid advances in machine learning is: high dimensional, structurally complex, and strongly nonlinear. This poses challenges for researcher intuition when they ask (i) how and why current algorithms work and (ii) what tools will lead to the next big break-though. Mathematicians working in topology, algebra, and geometry have more than a hundred years worth of finely-developed machinery whose purpose is to give structure to, help build intuition about, and generally better understand spaces and structures beyond those that we can naturally understand. This workshop will show-case work which brings methods from topology, algebra, and geometry and uses them to help answer challenging questions in machine learning. With this workshop we will create a vehicle for disseminating machine learning techniques that utilize rich mathematics and address core challenges described in the ICML call for papers. Additionally, this workshop creates opportunity for presentation of approaches which may address critical, domain-specific ML challenges but do not necessarily demonstrate improved performance on mainstream, data-rich benchmarks.  To this end our proposed workshop will open up IMCL to new researchers who in the past were not able to discuss their novel but data set-dependent analysis methods.We interpret topology, algebra, and geometry broadly and welcome submissions ranging from manifold methods to optimal transport to topological data analysis to mathematically informed deep learning. Through intellectual cross-pollination between data-driven and mathematically-inspired communities we believe this workshop will support the continued development of both groups and enable new solutions to problems in machine learning.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13448 2022\n",
      "Differential privacy is a promising approach to privacy-preserving data analysis. It has been the subject of a decade of intense scientific study, and has now been deployed in products at government agencies such as the U.S. Census Bureau and companies like Microsoft, Apple, and Google. MIT Technology Review named differential privacy one of 10 breakthrough technologies of 2020.Since data privacy is a pervasive concern, differential privacy has been studied by researchers from many distinct communities, including machine learning, statistics, algorithms, computer security, cryptography, databases, data mining, programming languages, social sciences, and law. We believe that this combined effort across a broad spectrum of computer science is essential for differential privacy to realize its full potential. To this end, our workshop will stimulate discussion among participants about both the state-of-the-art in differential privacy and the future challenges that must be addressed to make differential privacy more practical.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13449 2022\n",
      "Applying machine learning (ML) in healthcare is gaining momentum rapidly. However, the black-box characteristics of the existing ML approach inevitably lead to less interpretability and verifiability in making clinical predictions. To enhance the interpretability of medical intelligence, it becomes critical to develop methodologies to explain predictions as these systems are pervasively being introduced to the healthcare domain, which requires a higher level of safety and security. Such methodologies would make medical decisions more trustworthy and reliable for physicians, which could ultimately facilitate the deployment. In addition, it is essential to develop more interpretable and transparent ML systems. For instance, by exploiting structured knowledge or prior clinical information, one can design models to learn aspects more aligned with clinical reasoning. Also, it may help mitigate biases in the learning process, or identify more relevant variables for making medical decisions. In this workshop, we aim to bring together researchers in ML, computer vision, healthcare, medicine, NLP, public health, computational biology, biomedical informatics, and clinical fields to facilitate discussions including related challenges, definition, formalisms, and evaluation protocols regarding interpretable medical machine intelligence. The workshop appeals to ICML audiences as interpretability is a major challenge to deploy ML in critical domains such as healthcare. By providing a platform that fosters potential collaborations and discussions between attendees, we hope the workshop is fruitful in offering a step toward building autonomous clinical decision systems with a higher-level understanding of interpretability.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13450 2022\n",
      "Machine learning (ML) has revolutionized a wide array of scientific disciplines, including chemistry, biology, physics, material science, neuroscience, earth science, cosmology, electronics, mechanical science. It has solved scientific challenges that were never solved before, e.g., predicting 3D protein structure, imaging black holes, automating drug discovery, and so on. Despite this promise, several critical gaps stifle algorithmic and scientific innovation in AI for Science: (1) Under-explored theoretical analysis, (2) Unrealistic methodological assumptions or directions, (3) Overlooked scientific questions, (4) Limited exploration at the intersections of multiple disciplines, (5) Science of science, (6) Responsible use and development of AI for science. However, very little work has been done to bridge these gaps, mainly because of the missing link between distinct scientific communities. While many workshops focus on AI for specific scientific disciplines, they are all concerned with the methodological advances within a single discipline (e.g., biology) and are thus unable to examine the crucial questions mentioned above. This workshop will fulfill this unmet need and facilitate community building; with hundreds of ML researchers beginning projects in this area, the workshop will bring them together to consolidate the fast growing area of AI for Science into a recognized field.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13451 2022\n",
      "Deep networks have shown outstanding scaling properties both in terms of data and model sizes: larger does better. Unfortunately, the computational cost of current state-of-the-art methods is prohibitive. A number of new techniques have recently arisen to address and improve this fundamental quality-cost trade-off. For instance, methods like conditional computation, adaptive computation, dynamic model sparsification, and early-exit approaches are all aimed at addressing the above-mentioned quality-cost trade-off. This workshop explores such exciting and practically-relevant research avenues.More specifically, as part of contributed content we will invite high-quality papers on the following topics: dynamic routing, mixture-of-experts models, early-exit methods, conditional computations, capsules and object-oriented learning, reusable components, online network growing and pruning, online neural architecture search and applications of dynamic networks (continual learning, wireless/embedded devices and similar).The workshop is planned as a whole day event and will feature 2 keynote talks, a mix of panel discussion, contributed and invited talks, and a poster session. The invited speakers cover a diverse range of research fields (machine learning, computer vision, neuroscience, natural language processing) and backgrounds (academic, industry) and include speakers from underrepresented groups. All speakers confirmed their talks and the list ranges from senior faculty members (Gao Huang, Tinne Tuytelaars) to applied and theoretical research scientists (Weinan Sun, Francesco Locatello). The workshop builds on a set of previous workshops previously run at prime venues, such as CVPR, NeurIPS and ICLR.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13452 2022\n",
      "In machine learning, discrete time approaches such as gradient descent algorithms and discrete building layers for neural architectures have traditionally dominated. Recently, we have seen that by bridging these discrete systems with their continuous counterparts we can not only develop new insights but we can construct novel and competitive ML approaches. By leveraging time, we can tap into the centuries of research such as dynamical systems, numerical integration and differential equations, and continue enhancing what is possible in ML.The workshop aims to to disseminate knowledge about the use of continuous time methods in ML; to create a discussion forum and create a vibrant community around the topic; to provide a preview of what dynamical system methods might further bring to ML; to find the biggest hurdles in using continuous time systems in ML and steps to alleviate them; to showcase how continuous time methods can enable ML to have large impact in certain application domains, such as climate prediction and physical sciences.Recent work has shown that continuous time approaches can be useful in ML, but their applicability can be extended by increasing the visibility of these methods, fostering collaboration and an interdisciplinary approach to ensure their long-lasting impact. We thus encourage submissions with a varied set of topics: the intersection of machine learning and continuous-time methods; the incorporation of knowledge of continuous systems to analyse and improve on discrete approaches; the exploration of approaches from dynamical systems and related fields to machine learning; the software tools from the numerical analysis community.We have a diverse set of confirmed speakers and panellists with expertise in architectures, optimisation, RL, generative models, numerical analysis, gradient flows and climate. We hope this will foster an interdisciplinary and collaborative environment cohesive for the development of new research ideas.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13453 2022\n",
      "Algorithmic decision-making systems are increasingly used in sensitive applications such as advertising, resume reviewing, employment, credit lending, policing, criminal justice, and beyond. The long-term promise of these approaches is to automate, augment and/or eventually improve on the human decisions which can be biased or unfair, by leveraging the potential of machine learning to make decisions supported by historical data. Unfortunately, there is a growing body of evidence showing that the current machine learning technology is vulnerable to privacy or security attacks, lacks interpretability, or reproduces (and even exacerbates) historical biases or discriminatory behaviors against certain social groups. Most of the literature on building socially responsible algorithmic decision-making systems focus on a static scenario where algorithmic decisions do not change the data distribution. However, real-world applications involve nonstationarities and feedback loops that must be taken into account to measure and mitigate fairness in the long-term. These feedback loops involve the learning process which may be biased because of insufficient exploration, or changes in the environment's dynamics due to strategic responses of the various stakeholders. From a machine learning perspective, these sequential processes are primarily studied through counterfactual analysis and reinforcement learning.The purpose of this workshop is to bring together researchers from both industry and academia working on the full spectrum of responsible decision-making in dynamic environments, from theory to practice. In particular, we encourage submissions on the following topics: fairness, privacy and security, robustness, conservative and safe algorithms, explainability and interpretability.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13454 2022\n",
      "The ICML Expressive Vocalizations (ExVo) Workshop and Competition 2022 introduces, for the first time in a competition setting, the machine learning problem of understanding and generating vocal bursts – a wide range of emotional non-linguistic utterances. Participants of ExVo are presented with three tasks that utilize a single dataset. The dataset and three tasks draw attention to new innovations in emotion science and capture 10 dimensions of emotion reliably perceived in distinct vocal bursts: Awe, Excitement, Amusement, Awkwardness, Fear, Horror, Distress, Triumph, Sadness and Surprise. Of particular interest to the ICML community, these tasks highlight the need for advanced machine learning techniques for multi-task learning, audio generation, and personalized few-shot learning of nonverbal expressive style.With studies of vocal emotional expression often relying on significantly smaller datasets insufficient to apply the latest machine learning innovations, the ExVo competition and workshop provides an unprecedented platform for the development and discussion of novel strategies for understanding vocal bursts and will enable unique forms of collaborations by leading researchers from diverse disciplines. Organized by leading researchers in emotion science and machine learning, the following three tasks are proposed: the Multi-task High-Dimensional Emotion, Age & Country Task (ExVo Multi-Task); the Generative Emotional Vocal Burst Task (ExVo Generate); and the Few-Shot Emotion Recognition task (ExVo Few-Shot).Important dates (AoE)- Challenge Opening (data available): April 1, 2022.- Baselines and paper released: April 8, 2022.- ExVo MultiTask submission deadline: May 12, 2022.- ExVo Few-Shot (test-labels): May 13, 2022.- Workshop paper submission: ~~May 20, 2022~~ Extended June 6 2022.For those interested in submitting research to the ExVo workshop outside of the competition, we encourage contributions covering the following topics: - Detecting and Understanding Vocal Emotional Behavior- Multi-Task Learning in Affective Computing- Generating Nonverbal Vocalizations or Speech Prosody- Personalized Machine Learning for Affective Computing- Other topics related to Affective Verbal and Nonverbal Vocalization\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13455 2022\n",
      "Adversarial machine learning (AdvML), which aims at tricking ML models by providing deceptive inputs, has been identified as a powerful method to improve various trustworthiness metrics  (e.g.,  adversarial robustness, explainability,  and fairness)  and to advance versatile  ML  paradigms  (e.g.,  supervised and self-supervised learning,  and static and continual learning). As a consequence of the proliferation of AdvML-inspired research works,  the proposed workshop–New Frontiers in AdvML–aims to identify the challenges and limitations of current AdvML methods and explore new prospective and constructive views of AdvML across the full theory/algorithm/application stack. The workshop will explore the new frontiers of AdvML from the following new perspectives: (1) advances in foundational AdvML research, (2) principles and practice of scalable AdvML, and (3) AdvML for good. This will be a full-day workshop, which accepts full paper submissions  (up to 6 pages)  as well as “blue sky” extended abstract submissions (up to 2 pages).\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13456 2022\n",
      "Whether in robotics, protein design, or physical sciences, one often faces decisions regarding which data to collect or which experiments to perform. There is thus a pressing need for algorithms and sampling strategies that make intelligent decisions about data collection processes that allow for data-efficient learning. Experimental design and active learning have been major research focuses within machine learning and statistics, aiming to answer both theoretical and algorithmic aspects of efficient data collection schemes. The goal of this workshop is to identify missing links that hinder the direct application of these principled research ideas into practically relevant solutions.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13457 2022\n",
      "The past five years have seen rapid progress in large-scale pre-trained models across a variety of domains, such as computer vision, natural language processing, robotics, bioinformatics, etc. Leveraging a huge number of parameters, large-scale pre-trained models are capable of encoding rich knowledge from labeled and/or unlabeled examples. Supervised and self-supervised pre-training have been the two most representative paradigms, through which pre-trained models have demonstrated large benefits on a wide spectrum of downstream tasks. There are also other pre-training paradigms, e.g., meta-learning for few-shot learning, where pre-trained models are trained so that they quickly adapt to solve new tasks. However, there are still many remaining challenges and new opportunities ahead for pre-training, In this workshop, we propose to have the following two foci: (1) Which pre-training methods transfer across different applications/domains, which ones don't, and why? (2) In what settings should we expect pre-training to be effective, compared to learning from scratch?\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13458 2022\n",
      "Following a series of crippling cyber-attacks that targeted major of the public and social sectors — including schools, hospitals, critical infrastructure, and private businesses — the global community has increased its attention on the wider societal impacts of major cyber security events, forming task forces like the UN Open Ended Working Group on Cyber Security and undertaking policy efforts to mitigate these impacts. These actions are important, but policy changes only represent one side of the solution. On the other are technical developments, within which machine learning has been proposed as a key component of future of cyber defense tools, requiring rapid development to provide the speed and scale needed to detect and respond to new and emerging cyber security threats. Cybersecurity is inherently a systems problem and piece-wise application of off-the-shelf ML tools leave critical gaps in both sophistication and interpretable context needed for comprehensive security systems. To successfully develop ML-based cybersecurity defenses, a greater degree of cross-pollination across the ML and cybersecurity communities is needed because both are highly specialized technical domains. Moreover, the requisite ML topics needed to successfully leverage ML for cybersecurity — such as time series analytics, game theory, deep learning, reinforcement learning, representation learning, semi-supervised and self-supervised learning, learning on large scale streaming data, interpretable and robust autonomous systems, etc. - are foundational to the ICML community.The primary aim of this workshop is to build a mutual comprehensive awareness of the problem and solution spaces across the greater ML community and the Cybersecurity/ML for Cybersecurity communities. To provide meaningful engagement, workshop organizers will curate a program which defines the interdisciplinary boundary and opportunities between machine learning and cybersecurity.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13459 2022\n",
      "Recent years have seen a proliferation of models, algorithms, and infrastructure well-suited to complex problems in computational design, from virtual design problems in geometry, program synthesis, and web design to tangible design of molecules, materials, robots, architecture, carpentry, 3D printed models, and other domains.  This workshop provides an opportunity for researchers and practitioners to discuss shared problems and solutions in computational design and bridge the gaps between (and within) theory and practice.  The workshop will be highly interactive, featuring long talks, short talks, poster sessions, discussion panels, and demos of multiple forms.  This is the first workshop of its kind at ICML; we hope that this event will set the stage for many follow-on workshops to come.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13460 2022\n",
      "While improving prediction accuracy has been the focus of machine learning in recent years, this alone does not suffice for reliable decision-making. Deploying learning systems in consequential settings also requires calibrating and communicating the uncertainty of predictions. A recent line of work we call distribution-free predictive inference (i.e., conformal prediction and related methods) has developed a set of methods that give finite-sample statistical guarantees for any (possibly incorrectly specified) predictive model and any (unknown) underlying distribution of the data, ensuring reliable uncertainty quantification (UQ) for many prediction tasks. This line of work represents a promising new approach to UQ with complex prediction systems but is relatively unknown in the applied machine learning community. Moreover, much remains to be done integrating distribution-free methods with existing approaches to modern machine learning in computer vision, natural language, reinforcement learning, and so on -- little work has been done to bridge these two worlds. To facilitate the emerging topics on distribution-free methods, the proposed workshop has two goals. First, to bring together researchers in distribution-free methods with researchers specializing in applications of machine learning to catalyze work at this interface. Second, to bring together the existing community of distribution-free uncertainty quantification research, as no other workshop like this exists at a major conference. Given the important recent emphasis on the reliable real-world performance of ML models, we believe a large fraction of ICML attendees will find this workshop highly relevant.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13461 2022\n",
      "Machine learning models often break when deployed in the wild, despite excellent performance on benchmarks. In particular, models can learn to rely on apparently unnatural or irrelevant features. For instance, 1) in detecting lung disease from chest X-rays, models rely on the type of scanner rather than physiological signals, 2) in natural language inference, models rely on the number of shared words rather than the subject’s relationship with the object, 3) in precision medicine, polygenic risk scores for diseases like breast cancer rely on genes prevalent mainly in European populations, and predict poorly in other populations. In examples like these and others, the undesirable behavior stems from the model exploiting a spurious correlation. Improper treatment of spurious correlations can discourage the use of ML in the real world and lead to catastrophic consequences in extreme cases. The recent surge of interest in this issue is accordingly welcome and timely: more than 50 closely related papers have been published just in ICML 2021, NeurIPS 2021, and ICLR 2022. However, the most fundamental questions remain unanswered— e.g., how should the notion of spurious correlations be made precise? How should one evaluate models in the presence of spurious correlations? In which situations can a given method be expected to work, or fail? Which notions of invariance are fruitful and tractable? Further, relevant work has sprung up ad hoc from several distinct communities, with limited interplay between them: invariance and independence-constrained learning in causality-inspired ML, methods to decorrelate predictions and protected features (e.g. race) in algorithmic fairness, and stress testing procedures to discover unexpected model dependencies in reliable ML. This workshop will bring together these different communities to make progress on common foundational problems, and facilitate their interaction with domain-experts to build impactful collaborations.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13462 2022\n",
      "To reach top-tier performance, deep learning models usually require a large number of parameters and operations, using considerable power and memory. Several methods have been proposed to tackle this problem by leveraging quantization of parameters, pruning, clustering of parameters, decompositions of convolutions, or using distillation. However, most of these works focus mainly on improving efficiency at inference time, disregarding the training cost. In practice, however, most of the energy footprint of deep learning results from training. Hence, this workshop focuses on reducing the training complexity of deep neural networks. Our aim is to encourage submissions specifically concerning the reduction in energy, time, or memory usage at training time. Topics of interest include but are not limited to: (i) compression methods for memory and complexity reduction during training, (ii) energy-efficient hardware architectures, (iii) energy-efficient training algorithms, (iv) novel energy models or energy efficiency training benchmarks, (v) practical applications of low-energy training.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13463 2022\n",
      "The goal of reinforcement learning (RL) is to maximize a reward signal by taking optimal decisions. An RL system typically contains several moving components, possibly including a policy, a value function, and a model of the environment. We refer to decision awareness as the notion that each of the components and their combination should be explicitly trained to help the agent improve the total amount of collected reward. To better understand decision awareness, consider as an example a model-based method. For environments with rich observations (e.g., pixel-based), the world model is complex and standard approaches would need a large number of samples and a high-capacity function approximator to learn a reasonable approximation of the dynamics. However, a decision-aware agent might recognize that modeling all the granular complexity of the environment is neither feasible nor necessary to learn an optimal policy and instead focus on modeling aspects that are important for decision making. Decision awareness goes beyond the model learning aspect. In actor-critic algorithms, a critic is trained to predict the expected return while later used to aid policy learning. Is return prediction an optimal strategy for critic learning? And, in general, what is the best way to learn each component of an RL system? Our workshop aims at answering these questions and articulating that decision awareness might be a key towards solving grand challenges in RL, including exploration and sample efficiency. The workshop is about decision-aware RL algorithms, their implications, and real-world applications; we focus on decision-aware objectives, end-to-end procedures, and meta-learning techniques for training and discovering components in modular RL systems, as well as theoretical or empirical analyses of the interaction among multiple modules used by RL algorithms.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13464 2022\n",
      "Machine learning advances are used in self-driving cars, speech recognition systems, and translation software. However, the COVID-19 pandemic has highlighted the urgency of translating such advances to the domain of biomedicine. Such a pivot requires new machine learning methods to build long-term vaccines and therapeutic strategies, predict immune avoidance, and better repurpose small molecules as drugs.The ICML Workshop on Computational Biology (WCB) will highlight how machine learning approaches can be tailored to making both translational and basic scientific discoveries with biological data. Practitioners at the intersection of computation, machine learning, and biology are in a unique position to frame problems in biomedicine, from drug discovery to vaccination risk scores, and WCB will showcase such recent research. Commodity lab techniques lead to the proliferation of large complex datasets and require new methods to interpret these collections of high-dimensional biological data, such as genetic sequences, cellular features or protein structures and imaging datasets. These data can be used to make new predictions towards clinical response, uncover new biology, or aid in drug discovery.This workshop aims to bring together interdisciplinary machine learning researchers working in areas such as computational genomics; neuroscience; metabolomics; proteomics; bioinformatics; cheminformatics; pathology; radiology; evolutionary biology; population genomics; phenomics; ecology, cancer biology; causality; representation learning and disentanglement to present recent advances and open questions to the machine learning community. We especially encourage interdisciplinary submissions that might not neatly fit into one of these categories.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13465 2022\n",
      "The importance of robust predictions continues to grow as machine learning models are increasingly relied upon in high-stakes settings. Ensuring reliability in real-world applications remains an enormous challenge, particularly because data in the wild frequently differs substantially from the data on which models were trained. This phenomenon, broadly known as “distribution shift”, has become a major recent focus of the research community.With the growing interest in addressing this problem has come growing awareness of the multitude of possible meanings of “distribution shift” and the importance of understanding the distinctions between them: which types of shift occur in the real world, and under which of these is generalization feasible? Negative results seem just as common as positive ones; where provable generalization is possible, it often depends on strong structural assumptions whose likelihood of holding in reality is questionable. Existing approaches often lack rigor and clarity with regards to the precise problem they are trying to solve. Some work has been done to precisely define distribution shift and to produce benchmarks which properly reflect real-world distribution shift, but overall there seems to be little communication between the communities tackling foundations and applications respectively. Recent strides have been made to move beyond tinkering, bringing much needed rigor to the field, and we hope to encourage this effort by opening a dialogue to share ideas between these communities.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13466 2022\n",
      "While online learning has become one of the most successful and studied approaches in machine learning, in particular with reinforcement learning, online learning algorithms still interact with their environments in a very simple way.The complexity and diversity of the feedback coming from the environment in real applications is often reduced to the observation of a scalar reward. More and more researchers now seek to exploit fully the available feedback to allow faster and more human-like learning.This workshop aims to present a broad overview of the feedback types being actively researched, highlight recent advances and provide a networking forum for researchers and practitioners.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13467 2022\n",
      "Deep vision models are prone to short-cut learning, vulnerable to adversarial attacks, as well as natural and synthetic image corruptions. While OOD test sets have been proposed to measure the vulnerability of DNNs to distribution shifts of different kinds, it has been shown that the performance on popular OOD test sets such as ImageNet-C or ObjectNet is strongly correlated to the performance on clean ImageNet. Since performance on clean ImageNet clearly tests IID but not OOD generalization, this calls for new challenging OOD datasets testing different aspects of generalization.Our goal is to bring the robustness, domain adaptation, and out-of-distribution detection communities together to work on a new broad-scale benchmark that tests diverse aspects of current computer vision models and guides the way towards the next generation of models. Submissions to this workshop will contain novel datasets, metrics and evaluation settings.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13468 2022\n",
      "The 1st Machine Learning for Audio Synthesis workshop at ICML will attempt to cover the space of novel methods and applications of audio generation via machine learning. These include, but are not limited to: methods of speech modeling, environmental sound generation or other forms of ambient sound, novel generative models, music generation in the form of raw audio, and text-to-speech methods. Audio synthesis plays a significant and fundamental role in many audio-based machine learning systems, including smart speakers and voice-based interaction systems, real-time voice modification systems, and music or other content generation systems.We plan to solicit original workshop papers in these areas, some of which will present contributed talks and spotlights. Alongside these presentations will be talks from invited speakers, a poster session and interactive live demo session, and an invited speaker panel.We believe that a machine learning workshop focused around generation in the audio domain would provide a good opportunity to bring together both practitioners of audio generation tools along with core machine learning researchers interested in audio, in order to hopefully forge new directions in this important area of research.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13469 2022\n",
      "In recent two years, the COVID-19 pandemic continues to disrupt the world, and has changed most aspects of human life. Healthcare AI has a mission to help humans to tackle the issues that are caused by COVID-19, e.g., COVID-19 vaccine related prediction, COVID-19 medical imaging diagnosis. With the development of the epidemic, the virus keeps mutating, and meanwhile the related research is also evolving. As a result, more and more understanding, observation, policy are involved into daily life. All of these factors bring new challenges and opportunities to scientific research, including Healthcare AI. The goal of this workshop is to bring together perspectives from multiple disciplines (e.g., Healthcare AI, Machine Learning, Medical Image ML, Bioinformatics, Genomics, Epidemiology, Public Health, Health Policy, Computer Vision, Deep Learning, Cognitive Science) to highlight major open questions and to identify collaboration opportunities to address outstanding challenges in the domain of COVID-19 related Healthcare AI. Website:  https://healthcare-ai-covid19.github.io/\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13470 2022\n",
      "Many of the world's most pressing issues, such as climate change, pandemics, financial market stability and fake news, are emergent phenomena that result from the interaction between a large number of strategic or learning agents. Understanding these systems is thus a crucial frontier for scientific and technology development that has the potential to permanently improve the safety and living standards of humanity. Agent-Based Modelling (ABM) (also known as individual-based modelling) is an approach toward creating simulations of these types of complex systems by explicitly modelling the actions and interactions of the individual agents contained within. However, current methodologies for calibrating and validating ABMs rely on human expert domain knowledge and hand-coded behaviours for individual agents and environment dynamics. Recent progress in AI has the potential to offer exciting new approaches to learning, calibrating, validation, analysing and accelerating ABMs. This interdisciplinary workshop is meant to bring together practitioners and theorists to boost ABM method development in AI, and stimulate novel applications across disciplinary boundaries - making ICML the ideal venue.Our inaugural workshop will be organised along two axes. First, we seek to provide a venue where ABM researchers from a variety of domains can introduce AI researchers to their respective domain problems. To this end, we are inviting a number of high-profile speakers across various application domains. Second, we seek to stimulate research into AI methods that can scale to large-scale agent-based models with the potential to redefine our capabilities of creating, calibrating, and validating such models. These methods include, but are not limited to, simulation-based inference, multi-agent learning, causal inference and discovery, program synthesis, and the development of domain-specific languages and tools that allow for tight integration of ABMs and AI approaches.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13471 2022\n",
      "In modern ML domains, state-of-the-art performance is attained by highly overparameterized models that are expensive to train, costing weeks of time and millions of dollars. At the same time, after deploying the model, the learner may realize issues such as leakage of private data or vulnerability to adversarial examples. The learner may also wish to impose additional constraints post-deployment, for example, to ensure fairness for different subgroups. Retraining the model from scratch to incorporate additional desiderata would be expensive. As a consequence, one would instead prefer to update the model, which can yield significant savings of resources such as time, computation, and memory over retraining from scratch. Some instances of this principle in action include the emerging field of machine unlearning, and the celebrated paradigm of fine-tuning pretrained models. The goal of our workshop is to provide a platform to stimulate discussion about both the state-of-the-art in updatable ML and future challenges in the field.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13472 2022\n",
      "In just the past couple of years, we have seen significant advances in the capabilities of (Large) Language Models. One of the most striking capabilities of these systems is knowledge retrieval — Language Models can answer a diverse set of questions, which differ substantially in the domain knowledge needed for their responses, and their input structure. The precise methods for knowledge retrieval vary from the language model directly generating a response (parametric approaches) to a combination of generation and referencing an external knowledge corpus, e.g. retrieval augmented generation, to primarily using an external knowledge corpus with language model embeddings (semi-parametric approaches.) Despite the rapid advances, there remain many pressing open questions on the limits of knowledge retrieval with language models, and connections between these different approaches. How factual are generated responses, and how does this vary with question complexity, model scale, and importantly, different methods of knowledge retrieval? How important is the role of (self-supervised/supervised) pretraining?  What are the tradeoffs between few-shot (prompt based) approaches and finetuning when adapting to novel domains? And relatedly, to what extent do different knowledge retrieval approaches generalize to unseen settings? This workshop seeks to bring together a diverse set of researchers across NLP, Machine Learning and Theory to discuss these questions. We hope to share current findings and challenges, identify promising directions for future study, and most importantly, build a community around this topic at this pivotal time.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13473 2022\n",
      "Formal verification of machine learning-based building blocks is important for complex and critical systems such as autonomous vehicles, medical devices, or cybersecurity systems where guarantees on safety, fault tolerance and correctness are essential. Formal verification of machine learning is an emerging and interdisciplinary field, intersecting with fields of computer-aided verification, programming languages, robotics, computer security, and optimization, with many challenging open problems. This workshop aims to raise awareness of the importance of formal verification methods in the machine learning community and to bring together researchers and practitioners interested in this emerging field from a broad range of disciplines and backgrounds. Organizers of this workshop include pioneering proponents of machine learning verification and six confirmed invited speakers who have solid works in this field with diverse research and demographic backgrounds. The workshop includes posters, contributed talks, and a panel to encourage novel contributed work and interdisciplinary discussions on open challenges.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13474 2022\n",
      "A long-standing objective of AI research has been to discover theories of reasoning that are general: accommodating various forms of knowledge and applicable across a diversity of domains. The last two decades have brought steady advances toward this goal, notably in the form of mature theories of probabilistic and causal inference, and in the explosion of reasoning methods built upon the deep learning revolution. However, these advances have only further exposed gaps in both our basic understanding of reasoning and in limitations in the flexibility and composability of automated reasoning technologies.  This workshop aims to reinvigorate work on the grand challenge of developing a computational foundation for reasoning in minds, brains, and machines.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13475 2022\n",
      "We propose the 1st ICML Workshop on Safe Learning for Autonomous Driving (SL4AD), as a venue for researchers in artificial intelligence to discuss research problems on autonomous driving, with a specific focus on safe learning. While there have been significant advances in vehicle autonomy (e.g., perception, trajectory forecasting, planning and control, etc.), it is of paramount importance for autonomous systems to adhere to safety specifications, as any safety infraction in urban and highway driving, or high-speed racing, could lead to catastrophic failures. We envision the workshop to bring together regulators, researchers, and industry practitioners from different AI subfields, to work towards safer and more robust autonomous technology. This workshop aims to: (i) highlight open questions about safety issues, when autonomous agents must operate in uncertain and dynamically-complex real-world environments; (ii) bring together researchers and industrial practitioners in autonomous driving with control theoreticians in safety analysis, dependability, and verification; (iii) provide a strong AI benchmark, where the joint evaluation of safety, performance, and generalisation capabilities of AD perception and control algorithms is systematically performed; (iv) provide a forum for discussion among researchers, industrial practitioners, and regulators on the core challenges, promising solution strategies, fundamental limitations, and regulatory realities involved in deploying safety-critical autonomous systems; (v) define new algorithms that handle increasingly complex real-world scenarios---where vehicles must: drive at their physical limits, where any infraction could lead to catastrophic failure, make sub-second decisions in fast-changing environments, and remain robust to distribution shifts, novel road features, and other obstacles, to facilitate cross-domain generalisation.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13476 2022\n",
      "As modern astrophysical surveys deliver an unprecedented amount of data, from the imaging of hundreds of millions of distant galaxies to the mapping of cosmic radiation fields at ultra-high resolution, conventional data analysis methods are reaching their limits in both computational complexity and optimality. Deep Learning has rapidly been adopted by the astronomical community as a promising way of exploiting these forthcoming big-data datasets and of extracting the physical principles that underlie these complex observations. This has led to an unprecedented exponential growth of publications with in the last year alone about 500 astrophysics papers mentioning deep learning or neural networks in their abstract. Yet, many of these works remain at an exploratory level and have not been translated into real scientific breakthroughs.The goal of this workshop is to bring together Machine Learning researchers and domain experts in the field of Astrophysics to discuss the key open issues which hamper the use of Deep Learning for scientific discovery.  Rather than focusing on the benefits of deep learning for astronomy, the proposed workshop aims at overcoming its limitations.Topics that we aim to cover include, but are not limited to, high-dimensional Bayesian inference, simulation-based inference, uncertainty quantification and robustness to covariate shifts, anomaly and outlier detection, symmetries and equivariance. In addition, we plan on hosting meta-research panel discussions on successfully bringing ML to Astrophysics.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13477 2022\n",
      "This workshop proposal builds on the success of the 1st Data-Centric AI Workshop organized at NeurIPS 2021 (which attracted more than 160 submissions and close to 200 participants) and expands the effort to engage the deeplearning.ai community with the active interdisciplinary MLCommons community of practitioners, researchers and engineers from both academia and industry by presenting the current state-of-the-art, work-in-progress and a set of open problems in the field of benchmarking data for ML. Many of these areas are in a nascent stage, and we hope to further their development by knitting them together into a coherent whole. We seek to drive progress in addressing these core problems by promoting the creation of a set of benchmarks for data quality and data-related algorithms. We want to bring together work that pushes forward this new view of data-centric ML benchmarks, e.g. the initiatives at MLCommons, a non-profit that operates the MLPerf benchmarks that have become standard for AI chip speed but also others including Dynabench, OpenML, data-centric AI hub, etc. We envision MLCommons as providing a framework and resources for the evolution of benchmarks in this space, and our workshop as showcasing the best innovations revealed by those benchmarks and providing a focus event for the community submitting to them.A huge amount of innovation — in algorithms, ideas, principles, and tools — is needed to make data-centric AI development efficient and effective. We hope that this workshop will help spark that innovation.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=13478 2022\n",
      "Machine learning (ML) approaches can support decision-making in key societal settings including healthcare and criminal justice, empower creative discovery in mathematics and the arts, and guide educational interventions. However, deploying such human-machine teams in practice raises critical questions, such as how a learning algorithm may know when to defer to a human teammate and broader systemic questions of when and which tasks to dynamically allocate to a human versus a machine, based on complementary strengths while avoiding dangerous automation bias. Effective synergistic teaming necessitates a prudent eye towards explainability and offers exciting potential for personalisation in interaction with human teammates while considering real-world distribution shifts. In light of these opportunities, our workshop offers a forum to focus and inspire core algorithmic developments from the ICML community towards efficacious human-machine teaming, and an open environment to advance critical discussions around the issues raised by human-AI collaboration in practice.\n",
      "\n",
      "https://icml.cc/Conferences/2022/Schedule?showEvent=21435 2022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for array in allArrays:\n",
    "    \n",
    "    for eventID in array[1]:\n",
    "    \n",
    "        #Change below later so can iterate through all arrays from 17-22\n",
    "        year = array[0][0]\n",
    "    \n",
    "        URL = f'https://icml.cc/Conferences/{year}/Schedule?showEvent={eventID}'\n",
    "\n",
    "        print(URL, year)\n",
    "    \n",
    "        req = requests.get(URL)\n",
    "        soup = bs(req.text, 'html.parser')\n",
    "      \n",
    "        abstracts = soup.find_all('div',attrs = {'class','abstractContainer'})\n",
    "    \n",
    "        with open('abstracts_icml', 'a') as f:\n",
    "        \n",
    "            write = csv.writer(f)\n",
    "        \n",
    "            for abstractNumber in range(0,len(abstracts)):\n",
    "                print(abstracts[abstractNumber].text)\n",
    "            \n",
    "                write.writerow([abstracts[abstractNumber].text, f'{year}'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b389e0d7-544b-456f-8158-6dd9557971e0",
   "metadata": {},
   "source": [
    "### With both titles and abstracts for each year and workshop we now perform some simple data exploration, starting with BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c16fe48c-2510-43b2-aa47-b985e1f80529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7577866c-e647-4f07-863a-b637383b9d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'abstracts_icml')\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = ['abstract', 'year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fb30420-e7c6-401e-a9a1-67d65987a611",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In recent years, deep learning has revolutioni...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For details see:http://machlearn.gitlab.io/hit...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although dramatic progress has been made in th...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Probabilistic models are a central implement i...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Retrieval techniques operating on text or sema...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>We propose the 1st ICML Workshop on Safe Learn...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>As modern astrophysical surveys deliver an unp...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>This workshop proposal builds on the success o...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Machine learning (ML) approaches can support d...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              abstract  year\n",
       "0    In recent years, deep learning has revolutioni...  2017\n",
       "1    For details see:http://machlearn.gitlab.io/hit...  2017\n",
       "2    Although dramatic progress has been made in th...  2017\n",
       "3    Probabilistic models are a central implement i...  2017\n",
       "4    Retrieval techniques operating on text or sema...  2017\n",
       "..                                                 ...   ...\n",
       "145  We propose the 1st ICML Workshop on Safe Learn...  2022\n",
       "146  As modern astrophysical surveys deliver an unp...  2022\n",
       "147  This workshop proposal builds on the success o...  2022\n",
       "148  Machine learning (ML) approaches can support d...  2022\n",
       "149                                                NaN  2022\n",
       "\n",
       "[150 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "146492d2-abd0-4ace-8596-53b04895f1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b48000b7-74ba-4725-a1e7-17e9f09bf4b8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In recent years, deep learning has revolutioni...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For details see:http://machlearn.gitlab.io/hit...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although dramatic progress has been made in th...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Probabilistic models are a central implement i...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Retrieval techniques operating on text or sema...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>A long-standing objective of AI research has b...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>We propose the 1st ICML Workshop on Safe Learn...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>As modern astrophysical surveys deliver an unp...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>This workshop proposal builds on the success o...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Machine learning (ML) approaches can support d...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              abstract  year\n",
       "0    In recent years, deep learning has revolutioni...  2017\n",
       "1    For details see:http://machlearn.gitlab.io/hit...  2017\n",
       "2    Although dramatic progress has been made in th...  2017\n",
       "3    Probabilistic models are a central implement i...  2017\n",
       "4    Retrieval techniques operating on text or sema...  2017\n",
       "..                                                 ...   ...\n",
       "144  A long-standing objective of AI research has b...  2022\n",
       "145  We propose the 1st ICML Workshop on Safe Learn...  2022\n",
       "146  As modern astrophysical surveys deliver an unp...  2022\n",
       "147  This workshop proposal builds on the success o...  2022\n",
       "148  Machine learning (ML) approaches can support d...  2022\n",
       "\n",
       "[149 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "298d49c8-5309-4dea-947a-6b6ca5962b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2017 = df.loc[df['year'] == 2017]\n",
    "df2019 = df.loc[df['year'] == 2019]\n",
    "df2020 = df.loc[df['year'] == 2020]\n",
    "df2021 = df.loc[df['year'] == 2021]\n",
    "df2022 = df.loc[df['year'] == 2022]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde5dca6-3a7e-4f48-860e-42ddaddbb784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba110eb1-f94a-4236-8fcd-469b26cde5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "\n",
    "stopwords = text.ENGLISH_STOP_WORDS.union(['machine learning','machine','ml','learning','workshop'])\n",
    "\n",
    "CountVec17 = CountVectorizer(ngram_range=(1,2), stop_words = stopwords)\n",
    "CountVec19 = CountVectorizer(ngram_range=(1,2), stop_words = stopwords)\n",
    "CountVec20 = CountVectorizer(ngram_range=(1,2), stop_words = stopwords)\n",
    "CountVec21 = CountVectorizer(ngram_range=(1,2), stop_words = stopwords)\n",
    "CountVec22 = CountVectorizer(ngram_range=(1,2), stop_words = stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "057866af-33f4-41cf-bad8-270a3257e460",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVec.fit_transform(df2017['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "5f7e9eeb-68d2-4871-b1d9-9f3dc560e98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ... 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bowCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "247358cc-aa52-4983-a26a-cd0f9986b906",
   "metadata": {},
   "outputs": [],
   "source": [
    "bowDf = pd.DataFrame(bow.toarray(), columns = CountVec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "b278adf8-106c-4b51-9666-e86f2de1708b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00625</th>\n",
       "      <th>00625 2016</th>\n",
       "      <th>01783</th>\n",
       "      <th>01783 2016</th>\n",
       "      <th>01868</th>\n",
       "      <th>01868 2016</th>\n",
       "      <th>03801</th>\n",
       "      <th>03801 2016</th>\n",
       "      <th>06057</th>\n",
       "      <th>06057 2016</th>\n",
       "      <th>...</th>\n",
       "      <th>years witnessed</th>\n",
       "      <th>years workshop</th>\n",
       "      <th>york</th>\n",
       "      <th>york max</th>\n",
       "      <th>zahavy</th>\n",
       "      <th>zahavy daniel</th>\n",
       "      <th>zeming</th>\n",
       "      <th>zeming lin</th>\n",
       "      <th>zero</th>\n",
       "      <th>zero fatality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21 rows × 4440 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00625  00625 2016  01783  01783 2016  01868  01868 2016  03801  \\\n",
       "0       0           0      0           0      0           0      0   \n",
       "1       0           0      0           0      0           0      0   \n",
       "2       0           0      0           0      0           0      0   \n",
       "3       0           0      0           0      0           0      0   \n",
       "4       0           0      0           0      0           0      0   \n",
       "5       0           0      0           0      0           0      0   \n",
       "6       0           0      0           0      0           0      0   \n",
       "7       0           0      0           0      0           0      0   \n",
       "8       0           0      0           0      0           0      0   \n",
       "9       0           0      0           0      0           0      0   \n",
       "10      0           0      0           0      0           0      0   \n",
       "11      0           0      0           0      0           0      0   \n",
       "12      0           0      0           0      0           0      0   \n",
       "13      0           0      0           0      0           0      0   \n",
       "14      0           0      0           0      0           0      0   \n",
       "15      0           0      0           0      0           0      0   \n",
       "16      1           1      1           1      1           1      1   \n",
       "17      0           0      0           0      0           0      0   \n",
       "18      0           0      0           0      0           0      0   \n",
       "19      0           0      0           0      0           0      0   \n",
       "20      0           0      0           0      0           0      0   \n",
       "\n",
       "    03801 2016  06057  06057 2016  ...  years witnessed  years workshop  york  \\\n",
       "0            0      0           0  ...                0               0     0   \n",
       "1            0      0           0  ...                0               0     0   \n",
       "2            0      0           0  ...                0               0     0   \n",
       "3            0      0           0  ...                0               1     0   \n",
       "4            0      0           0  ...                1               0     0   \n",
       "5            0      0           0  ...                0               0     0   \n",
       "6            0      0           0  ...                0               0     0   \n",
       "7            0      0           0  ...                0               0     0   \n",
       "8            0      0           0  ...                0               0     0   \n",
       "9            0      0           0  ...                0               0     0   \n",
       "10           0      0           0  ...                0               0     0   \n",
       "11           0      0           0  ...                0               0     0   \n",
       "12           0      0           0  ...                0               0     0   \n",
       "13           0      0           0  ...                0               0     0   \n",
       "14           0      0           0  ...                0               0     0   \n",
       "15           0      0           0  ...                0               0     0   \n",
       "16           1      1           1  ...                0               0     1   \n",
       "17           0      0           0  ...                0               0     0   \n",
       "18           0      0           0  ...                0               0     0   \n",
       "19           0      0           0  ...                0               0     0   \n",
       "20           0      0           0  ...                0               0     0   \n",
       "\n",
       "    york max  zahavy  zahavy daniel  zeming  zeming lin  zero  zero fatality  \n",
       "0          0       0              0       0           0     0              0  \n",
       "1          0       0              0       0           0     0              0  \n",
       "2          0       0              0       0           0     1              1  \n",
       "3          0       0              0       0           0     0              0  \n",
       "4          0       0              0       0           0     0              0  \n",
       "5          0       0              0       0           0     0              0  \n",
       "6          0       0              0       0           0     0              0  \n",
       "7          0       0              0       0           0     0              0  \n",
       "8          0       0              0       0           0     0              0  \n",
       "9          0       0              0       0           0     0              0  \n",
       "10         0       0              0       0           0     0              0  \n",
       "11         0       0              0       0           0     0              0  \n",
       "12         0       0              0       0           0     0              0  \n",
       "13         0       0              0       0           0     0              0  \n",
       "14         0       0              0       0           0     0              0  \n",
       "15         0       0              0       0           0     0              0  \n",
       "16         1       1              1       1           1     0              0  \n",
       "17         0       0              0       0           0     0              0  \n",
       "18         0       0              0       0           0     0              0  \n",
       "19         0       0              0       0           0     0              0  \n",
       "20         0       0              0       0           0     0              0  \n",
       "\n",
       "[21 rows x 4440 columns]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bowDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3546328d-2bfd-4198-b646-c8db9d1405e5",
   "metadata": {},
   "source": [
    "### Now lets explore the different topic as a function of year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49e32b51-ce03-4752-b1d4-d103cdca5a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "117785ed-d12b-4c73-92b3-efe2e8eb239a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_components=5, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(n_components=5, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(n_components=5, random_state=42)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldaBow  = LDA(n_components=5, random_state=42) \n",
    "ldaBow.fit(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5fa5b1b7-a589-443e-8511-f260c8efe018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99422791, 0.00143745, 0.00145025, 0.00144628, 0.0014381 ],\n",
       "       [0.86407728, 0.00121582, 0.1322501 , 0.00122719, 0.00122961]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldaBow.transform(bow[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1afec8f0-e73d-46a3-a4c8-334317cc8f36",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words in Topic #0:\n",
      "['information', 'retrieval', 'community', 'human', 'systems']\n",
      "\n",
      "Top 5 words in Topic #1:\n",
      "['video', 'ai', 'arxiv', '2016', 'games']\n",
      "\n",
      "Top 5 words in Topic #2:\n",
      "['lifelong', 'series', 'time', 'implicit', 'models']\n",
      "\n",
      "Top 5 words in Topic #3:\n",
      "['search', 'prediction', 'ml', 'time', 'driving']\n",
      "\n",
      "Top 5 words in Topic #4:\n",
      "['large', 'community', 'reproducibility', 'papers', 'models']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datadeliv/.local/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in enumerate(ldaBow.components_):\n",
    "    print(f\"Top 5 words in Topic #{idx}:\")\n",
    "    print([CountVec.get_feature_names()[i] for i in topic.argsort()[-5:]]) \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de9376a-f358-403a-9605-92caf9fabd79",
   "metadata": {},
   "source": [
    "## Interesting, now lets run LDA on the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db4e1915-3230-499a-85b8-8221a21b3016",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow17 = CountVec17.fit_transform(df2017['abstract'])\n",
    "bow19 = CountVec19.fit_transform(df2019['abstract'])\n",
    "bow20 = CountVec20.fit_transform(df2020['abstract'])\n",
    "bow21 = CountVec21.fit_transform(df2021['abstract'])\n",
    "bow22 = CountVec22.fit_transform(df2022['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3dbd4494-1c5b-45b6-9aef-bca74684410b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_components=5, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(n_components=5, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(n_components=5, random_state=42)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldaBow17  = LDA(n_components=5, random_state=42) \n",
    "ldaBow17.fit(bow17)\n",
    "ldaBow19  = LDA(n_components=5, random_state=42) \n",
    "ldaBow19.fit(bow19)\n",
    "ldaBow20  = LDA(n_components=5, random_state=42) \n",
    "ldaBow20.fit(bow20)\n",
    "ldaBow21  = LDA(n_components=5, random_state=42) \n",
    "ldaBow21.fit(bow21)\n",
    "ldaBow22  = LDA(n_components=5, random_state=42) \n",
    "ldaBow22.fit(bow22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4d6de37-739d-4384-93d6-ed9c108b26c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words in Topic #0:\n",
      "['deep', 'implicit models', 'community', 'implicit', 'models']\n",
      "\n",
      "Top 5 words in Topic #1:\n",
      "['researchers', 'data', 'research', 'prediction', 'time']\n",
      "\n",
      "Top 5 words in Topic #2:\n",
      "['feedback', 'ai', 'arxiv', '2016', 'games']\n",
      "\n",
      "Top 5 words in Topic #3:\n",
      "['loop', 'privacy', 'systems', 'lifelong', 'human']\n",
      "\n",
      "Top 5 words in Topic #4:\n",
      "['collections', 'interactive', 'information retrieval', 'information', 'retrieval']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datadeliv/.local/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in enumerate(ldaBow17.components_):\n",
    "    print(f\"Top 5 words in Topic #{idx}:\")\n",
    "    print([CountVec17.get_feature_names()[i] for i in topic.argsort()[-5:]]) \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be3293ac-88c1-40f3-b98e-e110fe01b6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words in Topic #0:\n",
      "['methods', 'research', 'social', 'models', 'researchers']\n",
      "\n",
      "Top 5 words in Topic #1:\n",
      "['theory', 'techniques', 'coding', 'community', 'systems']\n",
      "\n",
      "Top 5 words in Topic #2:\n",
      "['data', 'deep', 'time', 'real', 'rl']\n",
      "\n",
      "Top 5 words in Topic #3:\n",
      "['modeling', 'inference', 'probabilistic', 'data', 'models']\n",
      "\n",
      "Top 5 words in Topic #4:\n",
      "['deep', 'agents', 'tasks', 'multi', 'reinforcement']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in enumerate(ldaBow19.components_):\n",
    "    print(f\"Top 5 words in Topic #{idx}:\")\n",
    "    print([CountVec19.get_feature_names()[i] for i in topic.argsort()[-5:]]) \n",
    "    \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ff63aa02-7629-46c0-8959-aa9f626a3367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words in Topic #0:\n",
      "['problems', 'healthcare', 'systems', 'health', 'research']\n",
      "\n",
      "Top 5 words in Topic #1:\n",
      "['networks', 'neural networks', 'graph', 'neural', 'data']\n",
      "\n",
      "Top 5 words in Topic #2:\n",
      "['values', 'br', 'open', 'missing', 'data']\n",
      "\n",
      "Top 5 words in Topic #3:\n",
      "['generalization', 'models', 'data', 'methods', 'systems']\n",
      "\n",
      "Top 5 words in Topic #4:\n",
      "['based', 'ai', 'systems', 'models', 'data']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in enumerate(ldaBow20.components_):\n",
    "    print(f\"Top 5 words in Topic #{idx}:\")\n",
    "    print([CountVec20.get_feature_names()[i] for i in topic.argsort()[-5:]]) \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed6183fb-be1f-463c-95eb-cf08ae5fd8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words in Topic #0:\n",
      "['order', 'collaboration', 'human', 'data', 'methods']\n",
      "\n",
      "Top 5 words in Topic #1:\n",
      "['applications', 'ssl', 'tasks', 'data', 'systems']\n",
      "\n",
      "Top 5 words in Topic #2:\n",
      "['theoretical', 'researchers', 'new', 'causal', 'data']\n",
      "\n",
      "Top 5 words in Topic #3:\n",
      "['systems', 'distribution', 'privacy', 'data', 'models']\n",
      "\n",
      "Top 5 words in Topic #4:\n",
      "['researchers', 'systems', 'medical', 'models', 'climate']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in enumerate(ldaBow21.components_):\n",
    "    print(f\"Top 5 words in Topic #{idx}:\")\n",
    "    print([CountVec21.get_feature_names()[i] for i in topic.argsort()[-5:]]) \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1dc1106-cb25-4392-97d7-5b105a1ba1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words in Topic #0:\n",
      "['bring', 'systems', 'methods', 'researchers', 'new']\n",
      "\n",
      "Top 5 words in Topic #1:\n",
      "['methods', 'training', 'privacy', 'models', 'data']\n",
      "\n",
      "Top 5 words in Topic #2:\n",
      "['language', 'data', 'models', 'approaches', 'knowledge']\n",
      "\n",
      "Top 5 words in Topic #3:\n",
      "['2022', 'decision', 'pre', 'exvo', 'models']\n",
      "\n",
      "Top 5 words in Topic #4:\n",
      "['science', 'ai', 'data', 'systems', 'community']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in enumerate(ldaBow22.components_):\n",
    "    print(f\"Top 5 words in Topic #{idx}:\")\n",
    "    print([CountVec22.get_feature_names()[i] for i in topic.argsort()[-5:]]) \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f9337c1b-e898-452f-b9a5-35a91d7481be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis \n",
    "import pyLDAvis.sklearn \n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bd681aab-4a2c-4d76-8e9f-1e69862b15a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datadeliv/.local/lib/python3.10/site-packages/pyLDAvis/_prepare.py:247: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  by='saliency', ascending=False).head(R).drop('saliency', 1)\n",
      "/usr/lib/python3.10/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/usr/lib/python3.10/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/usr/lib/python3.10/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/usr/lib/python3.10/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "display_data = pyLDAvis.sklearn.prepare(ldaBow22, bow22, CountVec22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "73f0b617-af96-409b-b289-db56343cf532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el705671398216148605446096742864\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el705671398216148605446096742864_data = {\"mdsDat\": {\"x\": [0.12665823241312077, 0.011239208042147182, -0.04834485545591982, -0.07578653182970996, -0.013766053169638139], \"y\": [-0.051469384962550616, 0.12873704348953996, -0.0012624401422068766, -0.0639271838954392, -0.012078034489343322], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [25.918087359033983, 24.319265897264657, 19.345304652557545, 15.741513874883923, 14.675828216259884]}, \"tinfo\": {\"Term\": [\"exvo\", \"pre\", \"training\", \"2022\", \"disinformation\", \"advml\", \"privacy\", \"rl\", \"task\", \"trained\", \"knowledge\", \"energy\", \"decision\", \"retrieval\", \"vocal\", \"models\", \"language\", \"continuous\", \"differential privacy\", \"ai\", \"audio\", \"model\", \"tasks\", \"knowledge retrieval\", \"emotion\", \"pre training\", \"differential\", \"autonomous\", \"data\", \"shot\", \"disinformation\", \"advml\", \"19\", \"covid\", \"covid 19\", \"verification\", \"driving\", \"distribution free\", \"free\", \"healthcare ai\", \"audio\", \"autonomous driving\", \"control\", \"discoml\", \"media\", \"social media\", \"free methods\", \"pandemic\", \"genomics\", \"formal\", \"formal verification\", \"present\", \"highlight\", \"complex real\", \"generalisation\", \"industrial\", \"industrial practitioners\", \"infraction\", \"perception\", \"regulators\", \"autonomous\", \"safety\", \"practitioners\", \"prediction\", \"generation\", \"recent\", \"new\", \"researchers\", \"bring\", \"aims\", \"methods\", \"systems\", \"research\", \"distribution\", \"biology\", \"advances\", \"complex\", \"provide\", \"ai\", \"work\", \"real\", \"open\", \"challenges\", \"algorithms\", \"icml\", \"continuous\", \"reasoning\", \"cyber\", \"continuous time\", \"discrete\", \"interpretable\", \"develop\", \"centric\", \"data centric\", \"cyber security\", \"dynamical\", \"numerical\", \"time methods\", \"successfully\", \"level\", \"centric ai\", \"mlcommons\", \"astrophysics\", \"innovation\", \"ai science\", \"comprehensive\", \"cybersecurity communities\", \"greater\", \"impacts\", \"series\", \"technical\", \"dynamical systems\", \"gradient\", \"impact\", \"numerical analysis\", \"clinical\", \"cybersecurity\", \"gaps\", \"discussions\", \"science\", \"critical\", \"needed\", \"scientific\", \"community\", \"interpretability\", \"ai\", \"time\", \"systems\", \"human\", \"data\", \"benchmarks\", \"deep\", \"security\", \"including\", \"healthcare\", \"development\", \"methods\", \"approaches\", \"work\", \"bring\", \"tools\", \"set\", \"energy\", \"differential privacy\", \"shift\", \"distribution shift\", \"stimulate\", \"geometry\", \"memory\", \"abm\", \"abms\", \"individual\", \"modelling\", \"algebra\", \"algebra geometry\", \"topology\", \"topology algebra\", \"differential\", \"privacy\", \"agent based\", \"based modelling\", \"calibrating validating\", \"creating\", \"individual agents\", \"validating\", \"domain specific\", \"known\", \"intuition\", \"mathematically\", \"broadly\", \"rigor\", \"results\", \"training\", \"design\", \"parameters\", \"calibrating\", \"agents\", \"data\", \"potential\", \"models\", \"problems\", \"time\", \"agent\", \"end\", \"art\", \"challenges\", \"methods\", \"communities\", \"domains\", \"development\", \"ai\", \"researchers\", \"applications\", \"world\", \"domain\", \"based\", \"deep\", \"recent\", \"distribution\", \"algorithms\", \"approaches\", \"exvo\", \"pre\", \"vocal\", \"emotion\", \"pre training\", \"competition\", \"emotional\", \"multi task\", \"spurious\", \"decision awareness\", \"pre trained\", \"trained models\", \"2022\", \"trained\", \"affective\", \"bursts\", \"nonverbal\", \"task exvo\", \"vocal bursts\", \"correlations\", \"spurious correlations\", \"aware\", \"critic\", \"decision aware\", \"rl\", \"task\", \"shot\", \"2022 exvo\", \"affective computing\", \"april\", \"multi\", \"rely\", \"tasks\", \"decision\", \"learn\", \"models\", \"training\", \"model\", \"awareness\", \"large\", \"understanding\", \"following\", \"world\", \"retrieval\", \"knowledge retrieval\", \"dynamic\", \"ood\", \"language models\", \"responses\", \"imagenet\", \"conditional\", \"cost trade\", \"early\", \"early exit\", \"exit\", \"quality cost\", \"trade\", \"unfortunately\", \"corpus\", \"external\", \"external knowledge\", \"importantly\", \"knowledge corpus\", \"language model\", \"methods knowledge\", \"parametric\", \"parametric approaches\", \"retrieval language\", \"vary\", \"primarily\", \"algorithmic decision\", \"biased\", \"feedback loops\", \"language\", \"knowledge\", \"cost\", \"different\", \"decisions\", \"algorithmic\", \"diverse\", \"approaches\", \"current\", \"quality\", \"model\", \"models\", \"data\", \"decision making\", \"aspects\", \"research\", \"vision\", \"methods\", \"talks\", \"speakers\", \"performance\", \"making\", \"set\", \"questions\", \"applications\", \"decision\"], \"Freq\": [5.0, 5.0, 8.0, 5.0, 6.0, 6.0, 6.0, 5.0, 5.0, 4.0, 8.0, 4.0, 8.0, 3.0, 3.0, 18.0, 5.0, 5.0, 4.0, 15.0, 6.0, 9.0, 5.0, 3.0, 3.0, 3.0, 4.0, 7.0, 20.0, 3.0, 6.054909600058013, 6.054909328166771, 4.080482314886159, 4.080482314886159, 4.080482314886159, 4.080482256121891, 3.422340079870259, 3.4223400159003527, 3.4223400159003527, 3.4223399085642363, 5.396814892748163, 2.1060553192935063, 2.1060553192935063, 2.1060553123039463, 2.1060553123039463, 2.1060553123039463, 2.106055245481469, 2.106055243051423, 2.1060552333032785, 2.1060550524291557, 2.1060550524291557, 2.106055023794711, 3.4223611729332877, 1.4479129265682837, 1.4479129265682837, 1.4479129265682837, 1.4479129265682837, 1.4479129265682837, 1.4479129265682837, 1.4479129265682837, 5.39684703357801, 4.738791604868912, 4.738806467266979, 3.42243050722584, 4.080516933562388, 5.396965288372722, 8.029673388643312, 8.029634620033836, 6.0551630357830355, 4.080572973265006, 8.029520490496797, 7.371364122436561, 6.05512706723473, 4.738850781071835, 3.4223775776661167, 4.080661066653814, 3.42247453199572, 3.4223768221837596, 4.738521086351496, 4.080642456053104, 3.4224356733798627, 3.422432216051709, 3.422397799957286, 3.4223897814200352, 3.422388221598678, 4.636010253735105, 3.992119515659765, 3.3482295586179025, 3.3482295477076915, 2.7043391928809366, 2.704339176903523, 2.704339174134881, 2.704339136234877, 2.704339136234877, 2.060448846526978, 2.060448835174126, 2.060448835174126, 2.060448835174126, 2.0604488175098936, 2.060448780534191, 2.060448777104628, 2.060448777104628, 2.0604487594756695, 2.0604487591437275, 2.0604487232219038, 1.4165584819894617, 1.4165584819894617, 1.4165584819894617, 1.4165584819894617, 1.4165584819894617, 1.4165584819894617, 1.4165584700655036, 1.4165584700655036, 1.4165584700655036, 1.4165584700655036, 3.348295951725564, 4.636115236622086, 2.704430044938035, 2.7044306762104813, 5.2802615041279415, 4.6361901803556735, 3.3483403588971763, 5.280095542125389, 7.212051749343319, 2.70434731431249, 7.211655350659783, 5.280255594952981, 7.2118753319348965, 3.9923029122916502, 7.211793896939242, 3.3482874297475735, 3.9922965065147555, 3.348419448890769, 3.3484063715810644, 3.3482525128982035, 3.3483500791016345, 4.635940125658684, 3.992178113548611, 3.348288564939244, 3.348270603867265, 2.704429739643433, 2.704427756891868, 4.248778879562026, 3.6586706074911644, 3.068562496691841, 2.4784542682422357, 2.4784542477055367, 2.478454238352735, 2.4784541842180223, 1.8883461163298774, 1.8883461163298774, 1.8883461163298774, 1.8883461163298774, 1.888346071059121, 1.888346071059121, 1.888346071059121, 1.888346071059121, 3.6587457385823257, 4.8389354057593925, 1.2982378818029654, 1.2982378818029654, 1.2982378818029654, 1.2982378818029654, 1.2982378818029654, 1.2982378818029654, 1.2982378580289207, 1.298237840128043, 1.2982378342548622, 1.2982378342548622, 1.2982378163539734, 1.2982377984530775, 1.298237759274601, 4.838894314444887, 3.0687125040863434, 1.8883859231530264, 1.888390834565495, 1.8883808147478915, 6.609505698438326, 2.4785437812062616, 6.0192893330680795, 3.068660784703666, 3.6588333419587453, 1.888353737856453, 1.8883457848251863, 1.8883954387234336, 3.0687366765727315, 4.248857808780135, 2.478543199798559, 2.4785368748766374, 2.4785730869181424, 3.068579090647879, 3.068557266620065, 2.478538105166384, 2.47859284359326, 2.4785918901378667, 2.4786044781739, 2.4785402054320165, 2.4784955950198255, 2.4785333240714422, 2.4784742363192023, 2.4784706570689368, 4.969688334996021, 4.969688181281135, 3.3491377485159632, 2.808954219119277, 2.808954061252563, 2.2687706890704993, 2.2687706890704993, 2.2687706890704993, 2.2687706585970013, 2.268770654359244, 2.268770528825665, 2.268770528825665, 4.429623165867748, 3.8894281841780463, 1.7285871576936624, 1.7285871576936624, 1.7285871576936624, 1.7285871576936624, 1.7285871576936624, 1.7285871264544084, 1.7285871264544084, 1.7285871221101627, 1.7285871221101627, 1.7285871221101627, 3.8894201763226057, 3.889418674004754, 2.809008125562227, 1.1884036229036763, 1.1884036229036763, 1.1884036229036763, 2.2688115323730655, 2.2688103230268033, 3.349392206658508, 4.429846415861935, 2.268832213348252, 4.969991266945358, 3.3491311676979167, 3.3493193221314055, 2.2689829379429107, 2.2688890372781345, 1.7287346837437094, 1.7287098239717342, 1.728702031328386, 3.239477130642952, 2.7169807900382317, 2.1944844674583734, 2.194484337173266, 1.6719881052643908, 1.6719881045876384, 1.6719879910884043, 1.149491785211311, 1.149491785211311, 1.149491785211311, 1.149491785211311, 1.149491785211311, 1.149491785211311, 1.149491785211311, 1.1494917702176788, 1.1494917573564292, 1.1494917573564292, 1.1494917573564292, 1.1494917573564292, 1.1494917573564292, 1.1494917573564292, 1.1494917573564292, 1.1494917573564292, 1.1494917573564292, 1.1494917573564292, 1.1494917573564292, 1.1494917562902345, 1.1494917552240396, 1.1494917552240396, 1.1494917552240396, 3.2396466405414412, 4.284827676900522, 1.6720422013325698, 2.717201096591604, 2.7171410391921413, 2.1945766891468343, 2.194666569908504, 3.7622579624909456, 2.194676049661269, 1.6720725137768702, 2.7171053933078686, 3.7621252897148256, 3.761936695765245, 1.6721273528127696, 1.6720997803923485, 2.717085578231261, 1.6721097023113642, 2.7170644261287022, 1.672088851397244, 1.6720940019636472, 1.6720310713071405, 1.6720945784641004, 1.672085194423422, 1.6720785144382126, 1.6720661591673585, 1.6720190821956524], \"Total\": [5.0, 5.0, 8.0, 5.0, 6.0, 6.0, 6.0, 5.0, 5.0, 4.0, 8.0, 4.0, 8.0, 3.0, 3.0, 18.0, 5.0, 5.0, 4.0, 15.0, 6.0, 9.0, 5.0, 3.0, 3.0, 3.0, 4.0, 7.0, 20.0, 3.0, 6.514245529184932, 6.514245492143946, 4.539818374835109, 4.539818374835109, 4.539818374835109, 4.53981836667835, 3.8816760158062924, 3.8816760069977354, 3.8816760069977354, 3.881675992970691, 6.396295238755484, 2.56539125900273, 2.56539125900273, 2.5653912579621303, 2.5653912579621303, 2.5653912579621303, 2.5653912488589916, 2.5653912487795676, 2.5653912474967537, 2.565391222997581, 2.565391222997581, 2.5653912188688905, 4.421863341051403, 1.907248878918792, 1.907248878918792, 1.907248878918792, 1.907248878918792, 1.907248878918792, 1.907248878918792, 1.907248878918792, 7.143885540549392, 6.431967747283719, 7.0220761855873945, 5.065756396862188, 6.647496727191019, 9.400624382893238, 16.02264610043919, 16.14017031029877, 12.431517555348037, 7.011682731690377, 20.819536847225102, 18.201281691071383, 13.422697160138199, 9.769766170041885, 5.813347832555785, 8.106594639810023, 6.836081156955893, 6.993568353697522, 15.231291953422573, 11.13250534917948, 8.542650262427403, 7.569863545031432, 9.72287121599029, 10.839332776681859, 9.204215331815353, 5.098196588144962, 4.454306193820094, 3.810415887542048, 3.810415886271909, 3.1665255351307566, 3.1665255333807485, 3.16652553305843, 3.166525528777612, 3.166525528777612, 2.522635184986222, 2.5226351836645518, 2.5226351836645518, 2.5226351836645518, 2.5226351817499153, 2.522635177595707, 2.5226351771517606, 2.5226351771517606, 2.522635175277296, 2.5226351751212928, 2.5226351710603527, 1.8787448327511134, 1.8787448327511134, 1.8787448327511134, 1.8787448327511134, 1.8787448327511134, 1.8787448327511134, 1.8787448313629567, 1.8787448313629567, 1.8787448313629567, 1.8787448313629567, 4.468556737267234, 6.414478963391575, 3.756641323076324, 3.8246657880965835, 8.07070580163471, 7.004595016086836, 4.923033882229669, 8.238591887718256, 11.959172614895412, 4.211519766529115, 15.231291953422573, 11.003573998127441, 18.201281691071383, 7.541352123337708, 20.995191869543056, 6.1209332262000675, 8.517863030905835, 6.761805324197058, 6.797177954788992, 7.101127199029121, 7.369165265587078, 20.819536847225102, 13.64492485783521, 11.13250534917948, 12.431517555348037, 5.004880364775455, 6.572353730788534, 4.721721730171427, 4.131613505369323, 3.5415052824001854, 2.9513970579658735, 2.9513970577123407, 2.951397057635668, 2.95139705671061, 2.3612888346402854, 2.3612888346402854, 2.3612888346402854, 2.3612888346402854, 2.361288834013318, 2.361288834013318, 2.361288834013318, 2.361288834013318, 4.7754969765547175, 6.356828193872051, 1.7711806101315746, 1.7711806101315746, 1.7711806101315746, 1.7711806101315746, 1.7711806101315746, 1.7711806101315746, 1.7711806098023213, 1.7711806095299039, 1.7711806094730678, 1.7711806094730678, 1.7711806092006495, 1.7711806089282318, 1.7711806083440844, 8.55293175405468, 5.230388302198975, 2.9014757697497635, 3.0194260082555564, 3.0194271735606555, 20.995191869543056, 4.761674083009203, 18.931404262091284, 8.002041621931543, 11.003573998127441, 3.441656549871312, 3.44165588288675, 3.5276716183496304, 9.72287121599029, 20.819536847225102, 6.4859457051158165, 7.12983650695182, 7.369165265587078, 15.231291953422573, 16.14017031029877, 7.573656447544045, 8.370887192342197, 8.456928772129487, 8.460355531814457, 8.517863030905835, 9.400624382893238, 9.769766170041885, 10.839332776681859, 13.64492485783521, 5.452615930189444, 5.452615945924128, 3.832065346008027, 3.2918818180071905, 3.291881834166867, 2.751698290074575, 2.751698290074575, 2.751698290074575, 2.7516982932210228, 2.7516982936343477, 2.7516983064776825, 2.7516983064776825, 5.570548902554973, 4.962347222774364, 2.211514762280906, 2.211514762280906, 2.211514762280906, 2.211514762280906, 2.211514762280906, 2.211514765506419, 2.211514765506419, 2.21151476593013, 2.21151476593013, 2.21151476593013, 5.016120210600059, 5.016120503245777, 3.81437995729371, 1.6713312348443283, 1.6713312348443283, 1.6713312348443283, 3.3418027423279537, 3.3418028544278466, 5.134046476581521, 8.425766265576978, 3.395576798443618, 18.931404262091284, 8.55293175405468, 9.531104867873239, 4.6437999928132845, 7.64857131262944, 5.3376198074255115, 4.55849894875791, 8.370887192342197, 3.7259422235500224, 3.203445884918907, 2.680949543508081, 2.680949563526354, 2.1584532082034027, 2.1584532083159624, 2.158453225736302, 1.6359568663892885, 1.6359568663892885, 1.6359568663892885, 1.6359568663892885, 1.6359568663892885, 1.6359568663892885, 1.6359568663892885, 1.6359568687182247, 1.6359568706924943, 1.6359568706924943, 1.6359568706924943, 1.6359568706924943, 1.6359568706924943, 1.6359568706924943, 1.6359568706924943, 1.6359568706924943, 1.6359568706924943, 1.6359568706924943, 1.6359568706924943, 1.6359568708698276, 1.6359568710471608, 1.6359568710471608, 1.6359568710471608, 5.464416399825129, 8.47675606136152, 2.748554440425033, 5.532023990286801, 5.689521420281006, 4.508898508489917, 4.523120626497427, 13.64492485783521, 5.753248316857079, 3.460465273519755, 9.531104867873239, 18.931404262091284, 20.995191869543056, 4.000636325625371, 4.000641196336141, 13.422697160138199, 4.658794364361725, 20.819536847225102, 5.313081310482503, 5.366857622686833, 5.78524008861586, 6.53668732716249, 6.572353730788534, 8.378901897325376, 7.573656447544045, 8.425766265576978], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.8211, -5.8211, -6.2158, -6.2158, -6.2158, -6.2158, -6.3917, -6.3917, -6.3917, -6.3917, -5.9362, -6.8772, -6.8772, -6.8772, -6.8772, -6.8772, -6.8772, -6.8772, -6.8772, -6.8772, -6.8772, -6.8772, -6.3917, -7.2519, -7.2519, -7.2519, -7.2519, -7.2519, -7.2519, -7.2519, -5.9362, -6.0662, -6.0662, -6.3916, -6.2158, -5.9361, -5.5388, -5.5388, -5.8211, -6.2157, -5.5389, -5.6244, -5.8211, -6.0662, -6.3916, -6.2157, -6.3916, -6.3916, -6.0663, -6.2157, -6.3916, -6.3916, -6.3916, -6.3916, -6.3916, -6.0245, -6.174, -6.3499, -6.3499, -6.5635, -6.5635, -6.5635, -6.5635, -6.5635, -6.8354, -6.8354, -6.8354, -6.8354, -6.8354, -6.8354, -6.8354, -6.8354, -6.8354, -6.8354, -6.8354, -7.2101, -7.2101, -7.2101, -7.2101, -7.2101, -7.2101, -7.2101, -7.2101, -7.2101, -7.2101, -6.3499, -6.0244, -6.5634, -6.5634, -5.8943, -6.0244, -6.3498, -5.8944, -5.5826, -6.5634, -5.5826, -5.8943, -5.5826, -6.1739, -5.5826, -6.3499, -6.1739, -6.3498, -6.3498, -6.3499, -6.3498, -6.0245, -6.174, -6.3499, -6.3499, -6.5634, -6.5634, -5.8829, -6.0324, -6.2083, -6.4219, -6.4219, -6.4219, -6.4219, -6.6938, -6.6938, -6.6938, -6.6938, -6.6938, -6.6938, -6.6938, -6.6938, -6.0324, -5.7528, -7.0685, -7.0685, -7.0685, -7.0685, -7.0685, -7.0685, -7.0685, -7.0685, -7.0685, -7.0685, -7.0685, -7.0685, -7.0685, -5.7528, -6.2082, -6.6938, -6.6938, -6.6938, -5.441, -6.4218, -5.5345, -6.2082, -6.0323, -6.6938, -6.6938, -6.6938, -6.2082, -5.8828, -6.4218, -6.4218, -6.4218, -6.2083, -6.2083, -6.4218, -6.4218, -6.4218, -6.4218, -6.4218, -6.4218, -6.4218, -6.4218, -6.4218, -5.52, -5.52, -5.9146, -6.0905, -6.0905, -6.3041, -6.3041, -6.3041, -6.3041, -6.3041, -6.3041, -6.3041, -5.635, -5.7651, -6.576, -6.576, -6.576, -6.576, -6.576, -6.576, -6.576, -6.576, -6.576, -6.576, -5.7651, -5.7651, -6.0905, -6.9507, -6.9507, -6.9507, -6.3041, -6.3041, -5.9146, -5.635, -6.3041, -5.5199, -5.9146, -5.9146, -6.304, -6.3041, -6.576, -6.576, -6.576, -5.8778, -6.0537, -6.2673, -6.2673, -6.5392, -6.5392, -6.5392, -6.9139, -6.9139, -6.9139, -6.9139, -6.9139, -6.9139, -6.9139, -6.9139, -6.9139, -6.9139, -6.9139, -6.9139, -6.9139, -6.9139, -6.9139, -6.9139, -6.9139, -6.9139, -6.9139, -6.9139, -6.9139, -6.9139, -6.9139, -5.8778, -5.5982, -6.5392, -6.0536, -6.0537, -6.2673, -6.2672, -5.7282, -6.2672, -6.5392, -6.0537, -5.7283, -5.7283, -6.5391, -6.5392, -6.0537, -6.5392, -6.0537, -6.5392, -6.5392, -6.5392, -6.5392, -6.5392, -6.5392, -6.5392, -6.5392], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.2771, 1.2771, 1.2436, 1.2436, 1.2436, 1.2436, 1.2243, 1.2243, 1.2243, 1.2243, 1.1803, 1.1529, 1.1529, 1.1529, 1.1529, 1.1529, 1.1529, 1.1529, 1.1529, 1.1529, 1.1529, 1.1529, 1.094, 1.0747, 1.0747, 1.0747, 1.0747, 1.0747, 1.0747, 1.0747, 1.0698, 1.0447, 0.957, 0.9581, 0.8622, 0.7953, 0.6594, 0.6521, 0.6309, 0.8089, 0.3975, 0.4463, 0.5542, 0.6267, 0.8204, 0.6638, 0.6584, 0.6356, 0.1826, 0.3466, 0.4355, 0.5564, 0.3061, 0.1974, 0.3609, 1.3189, 1.3044, 1.2846, 1.2846, 1.2561, 1.2561, 1.2561, 1.2561, 1.2561, 1.2115, 1.2115, 1.2115, 1.2115, 1.2115, 1.2115, 1.2115, 1.2115, 1.2115, 1.2115, 1.2115, 1.1315, 1.1315, 1.1315, 1.1315, 1.1315, 1.1315, 1.1315, 1.1315, 1.1315, 1.1315, 1.1253, 1.0892, 1.0853, 1.0673, 0.9896, 1.0012, 1.0284, 0.969, 0.9082, 0.9709, 0.6662, 0.6797, 0.4881, 0.7779, 0.3453, 0.8106, 0.6561, 0.7111, 0.7059, 0.6621, 0.6251, -0.0882, 0.1849, 0.2125, 0.1021, 0.7984, 0.5259, 1.5372, 1.5212, 1.4994, 1.4681, 1.4681, 1.4681, 1.4681, 1.4192, 1.4192, 1.4192, 1.4192, 1.4192, 1.4192, 1.4192, 1.4192, 1.3763, 1.3699, 1.3321, 1.3321, 1.3321, 1.3321, 1.3321, 1.3321, 1.3321, 1.3321, 1.3321, 1.3321, 1.3321, 1.3321, 1.3321, 1.0731, 1.1095, 1.2132, 1.1734, 1.1734, 0.4869, 0.9898, 0.4969, 0.6843, 0.5416, 1.0425, 1.0425, 1.0178, 0.4895, 0.0535, 0.6808, 0.5861, 0.5531, 0.0406, -0.0174, 0.5257, 0.4257, 0.4154, 0.415, 0.4082, 0.3096, 0.2711, 0.1672, -0.063, 1.7561, 1.7561, 1.7142, 1.6902, 1.6902, 1.6559, 1.6559, 1.6559, 1.6559, 1.6559, 1.6559, 1.6559, 1.6197, 1.6053, 1.6025, 1.6025, 1.6025, 1.6025, 1.6025, 1.6025, 1.6025, 1.6025, 1.6025, 1.6025, 1.5945, 1.5945, 1.5429, 1.5079, 1.5079, 1.5079, 1.4616, 1.4616, 1.4218, 1.2059, 1.4457, 0.5115, 0.9113, 0.8031, 1.1327, 0.6336, 0.7215, 0.8793, 0.2715, 1.7791, 1.7543, 1.7187, 1.7187, 1.6636, 1.6636, 1.6636, 1.5661, 1.5661, 1.5661, 1.5661, 1.5661, 1.5661, 1.5661, 1.5661, 1.5661, 1.5661, 1.5661, 1.5661, 1.5661, 1.5661, 1.5661, 1.5661, 1.5661, 1.5661, 1.5661, 1.5661, 1.5661, 1.5661, 1.5661, 1.3962, 1.2367, 1.4219, 1.208, 1.1799, 1.1989, 1.1958, 0.6306, 0.9552, 1.1916, 0.664, 0.3031, 0.1996, 1.0466, 1.0466, 0.3216, 0.8943, -0.1174, 0.7629, 0.7528, 0.6777, 0.5556, 0.5502, 0.3073, 0.4084, 0.3017]}, \"token.table\": {\"Topic\": [1, 1, 4, 4, 3, 3, 1, 2, 3, 5, 1, 4, 4, 3, 4, 3, 1, 3, 1, 2, 3, 2, 1, 2, 4, 3, 3, 2, 4, 5, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 2, 3, 4, 5, 4, 2, 3, 5, 1, 2, 4, 5, 2, 1, 4, 1, 2, 1, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 3, 2, 3, 4, 5, 1, 2, 1, 2, 3, 4, 5, 3, 4, 1, 3, 3, 2, 2, 1, 2, 3, 4, 5, 1, 2, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 1, 2, 5, 2, 2, 1, 5, 4, 3, 5, 5, 1, 1, 3, 4, 1, 2, 3, 1, 2, 3, 5, 2, 2, 1, 2, 2, 1, 2, 3, 4, 5, 2, 1, 2, 4, 5, 4, 4, 1, 2, 4, 5, 1, 2, 4, 5, 1, 2, 3, 5, 2, 3, 5, 2, 1, 2, 3, 4, 1, 3, 4, 5, 2, 3, 3, 1, 2, 1, 2, 1, 1, 2, 3, 5, 1, 3, 1, 2, 4, 5, 1, 2, 3, 4, 5, 3, 2, 3, 4, 5, 1, 5, 2, 2, 5, 5, 4, 4, 3, 4, 3, 5, 5, 5, 4, 5, 1, 2, 4, 5, 1, 1, 1, 1, 2, 3, 1, 1, 4, 5, 1, 3, 2, 2, 1, 2, 1, 1, 4, 1, 2, 3, 5, 1, 2, 3, 4, 5, 2, 2, 5, 1, 2, 3, 4, 3, 3, 1, 1, 1, 2, 2, 5, 2, 3, 2, 3, 4, 5, 5, 5, 3, 1, 4, 5, 5, 5, 1, 2, 3, 4, 5, 2, 4, 2, 1, 2, 3, 4, 5, 3, 1, 3, 1, 2, 3, 4, 5, 5, 2, 1, 3, 4, 5, 3, 1, 2, 3, 4, 5, 3, 4, 4, 2, 3, 5, 1, 2, 3, 4, 5, 4, 2, 2, 5, 1, 2, 3, 5, 1, 3, 4, 5, 5, 1, 1, 3, 4, 5, 2, 3, 5, 1, 2, 3, 4, 4, 4, 1, 2, 4, 1, 5, 3, 5, 1, 2, 3, 4, 1, 2, 3, 1, 2, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 1, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 3, 5, 5, 3, 2, 4, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 5, 2, 1, 2, 3, 5, 3, 4, 5, 1, 1, 2, 3, 5, 4, 4, 3, 2, 1, 2, 3, 4, 5, 1, 3, 5, 2, 4, 4, 1, 2, 4, 2, 1, 2, 3, 4, 5, 2, 1, 2, 3, 3, 3, 5, 3, 4, 4, 3, 4, 1, 2, 3, 4, 5, 3, 5, 1, 1, 2, 4, 5, 4, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5], \"Freq\": [0.8810925173069912, 0.17951552306476345, 0.7180620922590538, 0.5983254421097098, 0.846995069243478, 0.846995069243478, 0.4934254366632226, 0.2467127183316113, 0.12335635916580565, 0.12335635916580565, 0.9210583186089446, 0.9043575173503456, 0.5983254421097098, 0.5811155096445584, 0.2905577548222792, 0.5645951600191206, 0.33118864689183786, 0.6623772937836757, 0.32827156194563434, 0.45958018672388806, 0.1969629371673806, 0.7928217377383704, 0.5704764680696954, 0.2852382340348477, 0.14261911701742386, 0.8469950694683714, 0.8469950694683714, 0.22178365694350297, 0.22178365694350297, 0.44356731388700593, 0.6112630581513492, 0.27676980325336603, 0.1845132021689107, 0.1845132021689107, 0.1845132021689107, 0.09225660108445535, 0.2640732404291393, 0.2640732404291393, 0.13203662021456966, 0.2640732404291393, 0.21986196562140356, 0.29314928749520475, 0.14657464374760237, 0.07328732187380119, 0.29314928749520475, 0.5983254421097098, 0.28347309732526504, 0.5669461946505301, 0.28347309732526504, 0.2499599316519107, 0.2499599316519107, 0.2499599316519107, 0.4999198633038214, 0.7928217364130561, 0.7817025033029653, 0.15634050066059305, 0.6998992315343676, 0.13997984630687352, 0.7796081759386208, 0.9043575158580638, 0.21534088495361423, 0.21534088495361423, 0.21534088495361423, 0.43068176990722845, 0.23639668480587697, 0.23639668480587697, 0.23639668480587697, 0.11819834240293849, 0.11819834240293849, 0.5645951600191206, 0.49012134083717623, 0.3267475605581175, 0.16337378027905874, 0.6112630581513492, 0.5160537587652101, 0.34403583917680675, 0.48264421244522965, 0.24132210622261482, 0.08044070207420494, 0.08044070207420494, 0.16088140414840987, 0.5645951603158694, 0.9043575173503456, 0.33118877470944885, 0.6623775494188977, 0.5645951600191206, 0.9474106470122486, 0.7928217358239434, 0.308550831678834, 0.102850277226278, 0.308550831678834, 0.102850277226278, 0.102850277226278, 0.22378590198041312, 0.6713577059412393, 0.3083590413688619, 0.3083590413688619, 0.15417952068443094, 0.15417952068443094, 0.2508534742832823, 0.5853247733276586, 0.0836178247610941, 0.0836178247610941, 0.0836178247610941, 0.7268238699039193, 0.4388479204854701, 0.14628264016182335, 0.2925652803236467, 0.14628264016182335, 0.524315421575654, 0.5322702596795246, 0.6112630598917285, 0.9807389561294474, 0.7873156341826992, 0.7796081759386208, 0.6112630582838677, 0.9043575160313326, 0.3638276125414352, 0.7276552250828704, 0.6112630598917285, 0.8810925173069912, 0.8810925173069912, 0.5645951600191206, 0.9043575158580638, 0.14276342853560958, 0.7138171426780479, 0.14276342853560958, 0.1738148511806781, 0.1738148511806781, 0.3476297023613562, 0.3476297023613562, 0.7873156339202606, 0.7928217333617043, 0.15589730759226975, 0.7794865379613487, 0.5322702596795246, 0.14288985871817578, 0.33340967034241015, 0.33340967034241015, 0.04762995290605859, 0.19051981162423437, 0.9474106470122486, 0.11868356758072521, 0.11868356758072521, 0.47473427032290083, 0.23736713516145042, 0.9043575158580638, 0.7268238689636535, 0.24996023597413145, 0.24996023597413145, 0.24996023597413145, 0.4999204719482629, 0.1757617075551163, 0.1757617075551163, 0.1757617075551163, 0.5272851226653489, 0.11740033813312618, 0.46960135253250473, 0.23480067626625237, 0.11740033813312618, 0.19119039394829962, 0.5735711818448989, 0.19119039394829962, 0.9474106457314465, 0.13570057991097764, 0.4071017397329329, 0.2714011598219553, 0.13570057991097764, 0.18076566583149548, 0.18076566583149548, 0.18076566583149548, 0.5422969974944865, 0.20940228941814765, 0.8376091576725906, 0.968144768333659, 0.7796081762548531, 0.9474106451114154, 0.2614607538029274, 0.7843822614087821, 0.9210583133716677, 0.5117829754546279, 0.10235659509092558, 0.20471319018185116, 0.20471319018185116, 0.7728620303682523, 0.677645183186032, 0.22108629916738942, 0.22108629916738942, 0.22108629916738942, 0.44217259833477884, 0.35473870962313764, 0.11824623654104588, 0.23649247308209176, 0.11824623654104588, 0.11824623654104588, 0.564595160124076, 0.42076701156820406, 0.2805113410454694, 0.1402556705227347, 0.1402556705227347, 0.7728620286144224, 0.7460043419477997, 0.792821733777083, 0.5322702600728054, 0.6112630598917285, 0.6112630598917285, 0.9113328381321152, 0.7268238699039193, 0.5811156222633347, 0.29055781113166734, 0.8471486098895489, 0.6112630598917285, 0.6112630582838677, 0.6112630582838677, 0.9169910487031646, 0.6112630581513492, 0.2193704575214343, 0.2193704575214343, 0.4387409150428686, 0.2193704575214343, 0.779608186880386, 0.779608186880386, 0.7728620303682523, 0.7796081790212466, 0.7985856891824029, 0.2661952297274676, 0.524315421575654, 0.6017302699282786, 0.15043256748206965, 0.3008651349641393, 0.7796081794352231, 0.6776451832618476, 0.5322702600728054, 0.5322702596795246, 0.4224681400454516, 0.4224681400454516, 0.7728620331611104, 0.6784470185111326, 0.22614900617037753, 0.2652044311537631, 0.5304088623075262, 0.13260221557688154, 0.13260221557688154, 0.3259376157389734, 0.21729174382598226, 0.21729174382598226, 0.21729174382598226, 0.9265894558904563, 0.5322702600728054, 0.5322702596795246, 0.6112630582838677, 0.14711987925745626, 0.44135963777236875, 0.14711987925745626, 0.14711987925745626, 0.846995069243478, 0.5645951600191206, 0.524315421575654, 0.524315421575654, 0.524315421575654, 0.7928217364620853, 0.7123319291630494, 0.23744397638768314, 0.9474106456350102, 0.5645951602290313, 0.35390896921931075, 0.11796965640643692, 0.11796965640643692, 0.47187862562574767, 0.6112630582838677, 0.9364915493416999, 0.5645951602109138, 0.1830021592117324, 0.1830021592117324, 0.5490064776351973, 0.6112630582838677, 0.9265894634170495, 0.13074337142529932, 0.13074337142529932, 0.26148674285059864, 0.26148674285059864, 0.13074337142529932, 0.29450077537882685, 0.5890015507576537, 0.7928217356844185, 0.15298268831746156, 0.3059653766349231, 0.15298268831746156, 0.15298268831746156, 0.3059653766349231, 0.5645951602290313, 0.7796081762548531, 0.6776451834742423, 0.38425446534687285, 0.2401590408417955, 0.19212723267343643, 0.04803180816835911, 0.1440954245050773, 0.6112630582838677, 0.7928217358239434, 0.10491963039570867, 0.20983926079141735, 0.314758891187126, 0.314758891187126, 0.846995069243478, 0.15846685002692984, 0.05282228334230995, 0.3169337000538597, 0.26411141671154975, 0.2112891333692398, 0.2992396850160533, 0.5984793700321066, 0.7268238699039193, 0.6093803276123876, 0.20312677587079586, 0.20312677587079586, 0.4992933095976398, 0.18723499109911493, 0.12482332739940995, 0.12482332739940995, 0.12482332739940995, 0.9043575173503456, 0.792821733777083, 0.5322702600728054, 0.7460043363774903, 0.3963083326606442, 0.3963083326606442, 0.13210277755354805, 0.13210277755354805, 0.779608179045383, 0.6893043949743165, 0.34465219748715825, 0.6112630582838677, 0.6112630582838677, 0.524315421575654, 0.17285367325857234, 0.3457073465171447, 0.17285367325857234, 0.3457073465171447, 0.2100101734321213, 0.4200203468642426, 0.2100101734321213, 0.7120401242957679, 0.14240802485915358, 0.14240802485915358, 0.9169910460569918, 0.7268238655712604, 0.9113328336584297, 0.5922116590245533, 0.1974038863415178, 0.1974038863415178, 0.7796081881350722, 0.6112630582176085, 0.786555786550905, 0.157311157310181, 0.37490432338889235, 0.12496810779629745, 0.37490432338889235, 0.12496810779629745, 0.4289655649699757, 0.28597704331331714, 0.14298852165665857, 0.2889784814927117, 0.2889784814927117, 0.5779569629854234, 0.6112630598917285, 0.23869476269180556, 0.35804214403770834, 0.11934738134590278, 0.11934738134590278, 0.23869476269180556, 0.35117907298566464, 0.11705969099522155, 0.2341193819904431, 0.11705969099522155, 0.11705969099522155, 0.8980074170809365, 0.5318795642020053, 0.10637591284040106, 0.21275182568080211, 0.10637591284040106, 0.524315421575654, 0.2992396749781372, 0.5984793499562744, 0.4470040505583622, 0.2235020252791811, 0.07450067509306037, 0.07450067509306037, 0.2235020252791811, 0.49565771898301064, 0.18587164461862898, 0.18587164461862898, 0.06195721487287633, 0.06195721487287633, 0.9265894633687294, 0.5645951605889147, 0.8051654641981122, 0.6112630582838677, 0.5645951604027074, 0.19935726378462806, 0.7974290551385123, 0.7773670821206384, 0.15547341642412768, 0.15547341642412768, 0.12390490058471111, 0.6195245029235555, 0.12390490058471111, 0.12390490058471111, 0.1213799656092636, 0.606899828046318, 0.1213799656092636, 0.147889498743998, 0.44366849623199406, 0.147889498743998, 0.147889498743998, 0.5322702596795246, 0.1521524922366013, 0.45645747670980386, 0.1521524922366013, 0.3043049844732026, 0.847097423490728, 0.7864974212292396, 0.26216580707641324, 0.7796081762548531, 0.3726575476020792, 0.1863287738010396, 0.1863287738010396, 0.3726575476020792, 0.7268238690728277, 0.9043575160313326, 0.6776451832442434, 0.792821734378821, 0.38458830091255836, 0.38458830091255836, 0.05494118584465119, 0.05494118584465119, 0.10988237168930239, 0.37642939814492915, 0.18821469907246458, 0.37642939814492915, 0.1993572521539167, 0.7974290086156668, 0.9043575173503456, 0.19477813544567774, 0.19477813544567774, 0.5843344063370333, 0.5322702596795246, 0.09087956332825838, 0.45439781664129186, 0.3635182533130335, 0.09087956332825838, 0.09087956332825838, 0.792821733777083, 0.19980497576686135, 0.599414927300584, 0.19980497576686135, 0.8469950694683714, 0.8469950694683714, 0.6112630598917285, 0.20151753900060967, 0.8060701560024387, 0.7268238655712604, 0.5845948668571633, 0.35075692011429804, 0.18734942466468568, 0.18734942466468568, 0.18734942466468568, 0.37469884932937136, 0.6112630590215389, 0.5645951600191206, 0.6112630582838677, 0.8810925188900631, 0.21464780837928318, 0.21464780837928318, 0.21464780837928318, 0.42929561675856637, 0.782867652067882, 0.9043575173503456, 0.35930815881393846, 0.26948111911045386, 0.17965407940696923, 0.08982703970348462, 0.08982703970348462, 0.3583849514474931, 0.1194616504824977, 0.2389233009649954, 0.2389233009649954, 0.1194616504824977], \"Term\": [\"19\", \"2022\", \"2022\", \"2022 exvo\", \"abm\", \"abms\", \"advances\", \"advances\", \"advances\", \"advances\", \"advml\", \"affective\", \"affective computing\", \"agent\", \"agent\", \"agent based\", \"agents\", \"agents\", \"ai\", \"ai\", \"ai\", \"ai science\", \"aims\", \"aims\", \"aims\", \"algebra\", \"algebra geometry\", \"algorithmic\", \"algorithmic\", \"algorithmic\", \"algorithmic decision\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"applications\", \"applications\", \"applications\", \"applications\", \"approaches\", \"approaches\", \"approaches\", \"approaches\", \"approaches\", \"april\", \"art\", \"art\", \"art\", \"aspects\", \"aspects\", \"aspects\", \"aspects\", \"astrophysics\", \"audio\", \"audio\", \"autonomous\", \"autonomous\", \"autonomous driving\", \"aware\", \"awareness\", \"awareness\", \"awareness\", \"awareness\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based modelling\", \"benchmarks\", \"benchmarks\", \"benchmarks\", \"biased\", \"biology\", \"biology\", \"bring\", \"bring\", \"bring\", \"bring\", \"bring\", \"broadly\", \"bursts\", \"calibrating\", \"calibrating\", \"calibrating validating\", \"centric\", \"centric ai\", \"challenges\", \"challenges\", \"challenges\", \"challenges\", \"challenges\", \"clinical\", \"clinical\", \"communities\", \"communities\", \"communities\", \"communities\", \"community\", \"community\", \"community\", \"community\", \"community\", \"competition\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex real\", \"comprehensive\", \"conditional\", \"continuous\", \"continuous time\", \"control\", \"corpus\", \"correlations\", \"cost\", \"cost\", \"cost trade\", \"covid\", \"covid 19\", \"creating\", \"critic\", \"critical\", \"critical\", \"critical\", \"current\", \"current\", \"current\", \"current\", \"cyber\", \"cyber security\", \"cybersecurity\", \"cybersecurity\", \"cybersecurity communities\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data centric\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision aware\", \"decision awareness\", \"decision making\", \"decision making\", \"decision making\", \"decision making\", \"decisions\", \"decisions\", \"decisions\", \"decisions\", \"deep\", \"deep\", \"deep\", \"deep\", \"design\", \"design\", \"design\", \"develop\", \"development\", \"development\", \"development\", \"development\", \"different\", \"different\", \"different\", \"different\", \"differential\", \"differential\", \"differential privacy\", \"discoml\", \"discrete\", \"discussions\", \"discussions\", \"disinformation\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution free\", \"distribution shift\", \"diverse\", \"diverse\", \"diverse\", \"diverse\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain specific\", \"domains\", \"domains\", \"domains\", \"domains\", \"driving\", \"dynamic\", \"dynamical\", \"dynamical systems\", \"early\", \"early exit\", \"emotion\", \"emotional\", \"end\", \"end\", \"energy\", \"exit\", \"external\", \"external knowledge\", \"exvo\", \"feedback loops\", \"following\", \"following\", \"following\", \"following\", \"formal\", \"formal verification\", \"free\", \"free methods\", \"gaps\", \"gaps\", \"generalisation\", \"generation\", \"generation\", \"generation\", \"genomics\", \"geometry\", \"gradient\", \"greater\", \"healthcare\", \"healthcare\", \"healthcare ai\", \"highlight\", \"highlight\", \"human\", \"human\", \"human\", \"human\", \"icml\", \"icml\", \"icml\", \"icml\", \"imagenet\", \"impact\", \"impacts\", \"importantly\", \"including\", \"including\", \"including\", \"including\", \"individual\", \"individual agents\", \"industrial\", \"industrial practitioners\", \"infraction\", \"innovation\", \"interpretability\", \"interpretability\", \"interpretable\", \"intuition\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge\", \"knowledge corpus\", \"knowledge retrieval\", \"known\", \"language\", \"language\", \"language\", \"language model\", \"language models\", \"large\", \"large\", \"large\", \"large\", \"large\", \"learn\", \"learn\", \"level\", \"making\", \"making\", \"making\", \"making\", \"making\", \"mathematically\", \"media\", \"memory\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods knowledge\", \"mlcommons\", \"model\", \"model\", \"model\", \"model\", \"modelling\", \"models\", \"models\", \"models\", \"models\", \"models\", \"multi\", \"multi\", \"multi task\", \"needed\", \"needed\", \"needed\", \"new\", \"new\", \"new\", \"new\", \"new\", \"nonverbal\", \"numerical\", \"numerical analysis\", \"ood\", \"open\", \"open\", \"open\", \"open\", \"pandemic\", \"parameters\", \"parameters\", \"parametric\", \"parametric approaches\", \"perception\", \"performance\", \"performance\", \"performance\", \"performance\", \"potential\", \"potential\", \"potential\", \"practitioners\", \"practitioners\", \"practitioners\", \"pre\", \"pre trained\", \"pre training\", \"prediction\", \"prediction\", \"prediction\", \"present\", \"primarily\", \"privacy\", \"privacy\", \"problems\", \"problems\", \"problems\", \"problems\", \"provide\", \"provide\", \"provide\", \"quality\", \"quality\", \"quality\", \"quality cost\", \"questions\", \"questions\", \"questions\", \"questions\", \"questions\", \"real\", \"real\", \"real\", \"real\", \"real\", \"reasoning\", \"recent\", \"recent\", \"recent\", \"recent\", \"regulators\", \"rely\", \"rely\", \"research\", \"research\", \"research\", \"research\", \"research\", \"researchers\", \"researchers\", \"researchers\", \"researchers\", \"researchers\", \"responses\", \"results\", \"retrieval\", \"retrieval language\", \"rigor\", \"rl\", \"rl\", \"safety\", \"safety\", \"safety\", \"science\", \"science\", \"science\", \"science\", \"scientific\", \"scientific\", \"scientific\", \"security\", \"security\", \"security\", \"security\", \"series\", \"set\", \"set\", \"set\", \"set\", \"shift\", \"shot\", \"shot\", \"social media\", \"speakers\", \"speakers\", \"speakers\", \"speakers\", \"spurious\", \"spurious correlations\", \"stimulate\", \"successfully\", \"systems\", \"systems\", \"systems\", \"systems\", \"systems\", \"talks\", \"talks\", \"talks\", \"task\", \"task\", \"task exvo\", \"tasks\", \"tasks\", \"tasks\", \"technical\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time methods\", \"tools\", \"tools\", \"tools\", \"topology\", \"topology algebra\", \"trade\", \"trained\", \"trained\", \"trained models\", \"training\", \"training\", \"understanding\", \"understanding\", \"understanding\", \"understanding\", \"unfortunately\", \"validating\", \"vary\", \"verification\", \"vision\", \"vision\", \"vision\", \"vision\", \"vocal\", \"vocal bursts\", \"work\", \"work\", \"work\", \"work\", \"work\", \"world\", \"world\", \"world\", \"world\", \"world\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 5, 2, 4, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el705671398216148605446096742864\", ldavis_el705671398216148605446096742864_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el705671398216148605446096742864\", ldavis_el705671398216148605446096742864_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el705671398216148605446096742864\", ldavis_el705671398216148605446096742864_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.display(display_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cbc8a2-936d-4b24-a077-4fc2965c3770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
