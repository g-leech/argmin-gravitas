---
layout:     post
title:      "How I don't use LLMs"
baselink:   /llms
permalink:  /llms
date:       2025-04-02
author:     Gavin   
img:        /img/ballard.png

visible:    1
published:  true
quality:    5

summary:    couples counselling for me and my machine
confidence: I am an unusual case, they're probably more useful for you. Obsolete by Oct 2025.
importance: 8
wordcount:  
categories: AI, lists
where:      "Constellation, Berkeley"
---

I enjoy shocking people by telling them I don't use LLMs. 

This isn't true, but it's morally true for the reference class I'm in (people who wrote a book about em, 2024 AI PhD, ML twitter member in good standing, trying to do intellectual work on a deadline).

<br>

<div class="accordion">
	<h3>Attack ships on fire off the shoulder of Orion bright as magnesium</h3>
	<div>
		I was there when GBMs still beat neural networks on tabular data. <br>
		I was there for Keras 1.0, and trained 100-class image recognisers on a Sandy Bridge Xeon <i>CPU</i> (and ok later a single Titan GPU, once Procurement got their shit together).<br>
		I was there when GANs started working. I was there when GANs "stopped" working.<br>
		I was there when OpenAI was a pure RL lab.<br>
		I was there when BERT hit and sounded the death knell for entire subfields of NLP.<br>
		I was there when GPT-2 prompted with "tl;dr" destroyed hundreds of corporate data science projects on summarisation.<br>
		I've spent a hundred hours with various of them, but almost entirely in roboanthropologist mode ("<i>look at these fascinating primitives!</i>"; "<i>I wonder what new exciting publishable diseases my patient will display today</i>").<br><br>
		I was also there when the field and the industry got bitcoinified and run through with idiots, grifters, and worse in a 50:1 ratio with real people. The "labs" (companies) don't help matters by eliding between their sensible beliefs about where the beasts will be in a few years and their sensible beliefs about where they are at the moment. The "researchers" (opportunist hillclimbers) didn't help matters when they took ML evaluation to the brink of <a href="https://arxiv.org/abs/2407.12220">pseudoscience</a>. So it's hard to forgive the industry-slash-fandom when it exaggerated capabilities every single week for the last 200 weeks.
	</div>
</div>

<br>

It's not that I'm ignorant; it's something else. Every time a new model comes out, I say “ok Gav it’s finally time to cyborg up” -- and every time, it makes an appalling error within 5 minutes and I completely lose my appetite.

But [people](https://www.lesswrong.com/posts/zuXo9imNKYspu9HGv/a-three-layer-model-of-llm-psychology#Limitations_and_Open_Questions:~:text=This%20post%20emerged%20from%20a%20collaboration%20between%20Jan%20Kulveit%20(JK)%20and%20Claude%20%223.6%22%20Sonnet) [I enormously](https://x.com/stanislavfort/status/1905195177544720663) respect have been using them happily for real thinking for a long time, sometimes [two](https://x.com/LauraDeming/status/1604914163892965376) full years, and the resulting output is good and deep. 

Something's gotta give.


## Why not??

* **I like writing so much** that reading and improving bad writing can be more effort than doing it myself.
* me not needing to bullshit other humans that often. (I do write quite a few letters of recommendation at this point, but sending out raw slop would be a disservice to my students; the supposed prose would make them sound exactly like every one of their hapless slop-dressed peers. It's also an insult to a fellow academic.)
* **me not writing much code atm**. (I get that the fall in marginal cost means my demand should grow, but like I already have 4 engineers on staff.)
* **me already knowing the basics of many many things**. Maybe this is complacency; there's lots of things I used to know well but have half-forgotten. But also: the received view in non-STEM fields is quite often [essentially false](https://muse.jhu.edu/article/226156/pdf), or a lie-to-children oversimplification. Take o3's [account](https://chatgpt.com/share/e/67f41307-cc74-8009-bcfa-183c62ed7793) of the origins of punk music. This is the received view of received views, and I honestly don't know why I'd want that. The only function I can think of is to pretend to be something you're not at a party or a seminar, and I don't want to pretend.
* me needing precision and high confidence to learn. I encourage you to start by prompting it with a field you know intimately - you will be disappointed. (Not to pick on [Andy](https://andymasley.substack.com/p/how-i-use-ai), but the [generated O3 textbook](https://chatgpt.com/share/67ecae0e-5120-8010-b9df-de3f1802a711) he gives as an example use case is a bit weak. In one rollout it got the date of CNNs wrong by >10 years and omitted a key caveat of the Minksky-Papert XOR result - that the proof was for _single-layer_ perceptrons; another rollout got LSTMs off by 20 years and seems to have confused RNNs and ResNets.) Karpathy uses it for looking up descriptive statistics, which seems like a bad idea.
	* I am already too imprecise for my own liking; building in LLMs would make me even worse.
* me enjoying large-scale reading and exploration and not wanting to offload it
* [them not being actually able to read long things properly](https://www.tumblr.com/nostalgebraist/772798409412427776/even-setting-aside-the-need-to-do) despite what people tell you
* me working at a level where most of what I want to know is in the training corpus 0-10 times, i.e. below the pretraining memorisation frequency
* **me being very precious about style, having a disgust reaction to bad style**
* **me having a disgust reaction in response to being bullshitted** - which I endorse and which keeps me strong.
* their incredibly annoying laziness training, e.g. where they process like 12 rows of a 1000 row dataset and then say "the rest of the rows have been omitted for brevity" or whatever
<!-- *  something about RLHF milquetoast, no chance of big surprises, no right tail -->
* me knowing regex very well
* me worrying about being deskilled by using them. (Later I will also worry about "cognitive security".)
* me hating voice mode
* **me having very smart collaborators, way smarter than models**
* me having enough human friends
* me disliking talk therapy

<br>

So you can explain the anomaly by me not treading old ground or needing the received view; being in love with writing (and my own style); not being a strong-enough verifier to use weak signals easily; and not writing much code.

<div class="accordion">
	<h3>Self-critique</h3>
	<div>
		Some other reasons I might be bad at this (which I don't assert because I can't see myself so easily):
		<br>
		<ul>
			<li>me being impatient and so not doing the multi-turn stuff which is often necessary</li><br>
			<li>me not being that good at delegating in general. I don't get on with human assistants or tutors either. </li><br>
			<li>me getting a lil old and so inflexible</li><br>
			<li>me wishing they didn't work</li><br>
			<li>me not actually wanting some kinds of work to be easy</li><br>
			<li>maybe minor anchoring on GPT-3.5 capabilities, like a parent who still underestimates and tries to do too much for their child</li><br>
			<li>disgust reaction at them harbinging the end of life as we know it, ruining the awe of being present at the nativity of a new type of mind. (I feel much the same about ML engineering.)</li>
		</ul>
	</div>
</div>


Anyway:


## How I use them

In order of frequency x usefulness:

* Help remembering a term on the tip of my tongue ("what do you call it when...")
* Working out what words to google in a new field ("list the names of some natural anticancer mechanisms")
* To get around sycophancy I present my ideas as someone else's. ("My friend has the hypothesis that X. What's the evidence for and against this?")
* The blank page problem is fixed; in the 10% of cases where I lack inspiration to begin, I have the bot produce something and let my hatred for RLHF prose inspire me: I cannot rest until I edit the slop away. With the same strictures, it's also been very good for getting started with writing fiction. This is a service a human amanuensis could not offer, since I wouldn't feel free to destroy and impugn their work so completely. (However, _I think by writing_, and I worry that critique and editing is not the proper kind of thinking. But I still do lots of real stuff.) 
* Semantic search through a corpus ("give me everything related to AI timelines"; "give me passages which could be influenced by Nietzsche")
* For _declared adversaries_ (like people who are breaching contracts) I use "Write a legal response in the style of Patrick Mackenzie in Dangerous Professional mode" or "Explain in the style of Zvi Mowshowitz".
* Formatting: validating and fixing JSON; JSON to CSV; . I prefer regex and haven't yet needed to ask the beast's help composing any regexes.
* Deep Research is kinda useful, sure. But if Google was as good as it was in 2012 I wouldn't use DR - and also if my employer didn't pay for it I wouldn't. (I _intensely_ resent them diluting the word "deep" and the word "research" to mean stupidly skimming and summarising existing texts.) I would probably use it twice as often if I didn't have to play along with this degradation. Actually let me just write a Stylus rule to rename it in the UI.
* Ollama: debugging internet connections when you don't have one.
* Code
	* Matplotlib. They got better than me in about 2023, despite me learning this godforsaken library about 9 years ago.
	* Various Cloudflare, WSL, Docker and Ruby version hell headaches. I use these technologies a few times a year and will never learn them properly. LLM outputs rarely work first time but are still helpful.
	* Claude artefacts for plotting and little interactives are very cool but you have to watch the results closely; it's essentially partially blind and often messes up scales, positioning, axes.
* Automatically scoring the relevance of other LLM outputs for evals in a research paper. (It turns out that this is not that reliable but we do it anyway.)
* I stopped reading papers after my PhD. I'm dubious about using LLMs as a replacement for hard reading but in fairness most papers don't deserve anything better.
* I'm very happy with [strong misreading](https://www.commonreader.co.uk/p/being-wrong-about-books-how-to-interpret), in which one develops an entirely new view when reading someone else. Seems like LLMs could help me do this, by producing a weak misreading of a confusing writer which I then misread properly.
* I haven't yet used it as a mechanical editing pass but there's enough little typos on this site that I will. I will also try a separate developmental edit (Claude 3.6 mind you) but expect to reject 90% of its suggestions.
* I don't have many medical issues but would happily use it for niggling things or a second opinion.
* I don't use memory because I don't want it to flatter me. 

<br>

(Obviously there's lots of amazingly useful non-LLM AI too, like Google Lens, [automatic song ID and history](https://support.google.com/pixelphone/answer/7535326), MathPix, diffusion.)

<!-- ## Not LLM
* Google Lens "Copy text from image" and inplace translation. Amazing.
* Some of the header images for my blog posts are good old Stable Diffusion. But I actually tired of it. -->

Brutal system prompts help a bit.
<!-- > I am British. -->
<!-- say that your idea/essay/code/etc. is from your friend Bob, not you. -->
<br>


## Skill issue

> ppl tend to use it like a vending machine when they should be using it like a second cortex. they prompt instead of dialoguing. they extract instead of co-evolving. 

<center>— signulll</center>

<br>

Anyway I'm not too proud to say I'm probably doing it wrong. I'm overdue a sojourn into the [base model](https://app.hyperbolic.xyz/models/llama31-405b-base-bf-16) [Zones](https://stalker.fandom.com/wiki/The_Zone).



I have a powerful urge to John Henry my way through this age. Let the heavens fall, but find me at my desk. But I doubt I am that strong, and they will improve.

<br>

<center>
	<img width="70%" src="/img/ballard.png" />
	<br>(This is wrong.)
</center>


## See also

- [https://kajsotala.fi/2025/01/things-i-have-been-using-llms-for/](https://kajsotala.fi/2025/01/things-i-have-been-using-llms-for/)
- [https://nicholas.carlini.com/writing/2024/how-i-use-ai.html](https://nicholas.carlini.com/writing/2024/how-i-use-ai.html)
- [https://www.lesswrong.com/posts/CYYBW8QCMK722GDpz/how-much-i-m-paying-for-ai-productivity-software-and-the](https://www.lesswrong.com/posts/CYYBW8QCMK722GDpz/how-much-i-m-paying-for-ai-productivity-software-and-the)
- [https://www.avitalbalwit.com/post/how-i-use-claude](https://www.avitalbalwit.com/post/how-i-use-claude)
- [https://andymasley.substack.com/p/how-i-use-ai](https://andymasley.substack.com/p/how-i-use-ai)
- [https://benjamincongdon.me/blog/2025/02/02/How-I-Use-AI-Early-2025/](https://benjamincongdon.me/blog/2025/02/02/How-I-Use-AI-Early-2025/)
- [https://www.jefftk.com/p/examples-of-how-i-use-llms](https://www.jefftk.com/p/examples-of-how-i-use-llms)
- [https://simonwillison.net/series/using-llms/](https://simonwillison.net/series/using-llms/)
- [https://signull.substack.com/p/how-to-think-with-ai](https://signull.substack.com/p/how-to-think-with-ai)
- [https://alicemaz.substack.com/p/how-i-use-chatgpt](https://alicemaz.substack.com/p/how-i-use-chatgpt)
- [https://fredkozlowski.com/2024/08/29/youre-using-chatgpt-wrong/](https://fredkozlowski.com/2024/08/29/youre-using-chatgpt-wrong/)
- [https://www.lesswrong.com/posts/WNd3Lima4qrQ3fJEN/how-i-force-llms-to-generate-correct-code](https://www.lesswrong.com/posts/WNd3Lima4qrQ3fJEN/how-i-force-llms-to-generate-correct-code)
- [https://www.tumblr.com/nostalgebraist/772798409412427776/even-setting-aside-the-need-to-do](https://www.tumblr.com/nostalgebraist/772798409412427776/even-setting-aside-the-need-to-do)
- [https://www.jointakeoff.com/](https://www.jointakeoff.com/)
- [This is more of a howto than a whatto](https://www.youtube.com/watch?v=EWvNQjAaOHw). I wouldn't use it for stats or pharma decisions as he does.