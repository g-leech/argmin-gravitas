---
layout:     post
title:      "Four analogies for AGI"
baselink:   /
permalink:  /
date:       2025-10-10
author:     Gavin
img:        /img/

visible:    1
published:  false
quality:    

summary:    
warnings: 	
confidence: 
importance: 
wordcount:  
categories: 
where:      "Nettlecombe"
---

It used to be very hard to imagine a real AI. Most science fiction just puts "a fast ruthless human" in the place of the AI. Consciously or not, we understand future AI by analogy to things we do understand. But usually it's just one analogy. This is insane. 


Form warning: "AGI" is overloaded, t-AGI is the sensible measure.


## Normalism

AI as a tool, an extension of human will like a calculator. Then makes sense to be opposed to alignment (Yann Lecun). 

that reference class of new tech is that people overestimate negatives of new tech. Also, I would say that the intelligence is not the argument people outside AI safety find the most important or thus interesting. More fair/powerful/nuanced points they make are that: learning about the world involves itneracting with the world, upper limit to effectiveness/ability to impact world depends on making model of world more accurate. I.e. predicting what a politician will do not 100% possible bc depends on info that few/no people have access to. 

This one is the most convenient, and is in fact held by the most powerful people in the world

Sacks
Jensen just math

What evidence is there for these?


## Securitism

AI as tool more like a nuke; safety plan is RLHF++, no AI rights, and centralization of power (Ashenbrenner). 

What evidence is there for these?


## Alienism

Random

What evidence is there for these?

	Owl 87

AI as fundamentally alien, values random; alignment plan is bulletproof formalisation or bust (Yudkowsky). 

## Mirrorism

cultural technology

OR Values nonrandom. Somewhat mirror-image of humans. To AI, human values are fundamentally alien but also not random, and can be well-predicted using primatology.

## Personalism

AI as person. If childhood within human variation, then values within human variation. Alignment as parenting: raise it, treat it nicely (cyborgists).

do not plan for Alignment, instead plan for Interfacing
Who? Maybe "cyborg ecologists". Someone like Comprehensive AI services (CAIS) is kinda like it...



## HOWEVER

AI is of course not any of these things.

AGI will not be like normal technology, not be like nuclear weaponry, not be like an alien lifeform, not be a and not be like a humanlike person. 

If you are aware of which wrong assumption you are using, you might avoid not thinking. It might be that moving between four false assumptions saves you. 

which of these analogies has the least regret?


https://docs.google.com/document/d/17u2c6eLdNomtNURErN19NTOlmScTj2MKJrvERDZ9Ebs/edit?tab=t.0



<!-- > However, there are conceptual and logical flaws with [alienism]. On a conceptual level, intelligence—especially as a comparison between different species—is not well defined, let alone measurable on a one-dimensional scale. -->

<!-- Completely unargued!!! One cite to Mitchell, who does not establish any such thing -->
<!-- https://knightcolumbia.org/content/ai-as-normal-technology  -->




## See also 

* [Matthijs' metaphors](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4612468)
* Scott Viteri
* https://arxiv.org/abs/2509.20050