---
layout:     post
title:      "Existential risk as common cause"
baselink:   /x-for-all
permalink:  /x-for-all
date:       2018-10-17
author:     Gavin

img:        /img/grey.jpg
published:	true
visible: 	1

summary:    Why many different worldviews should prioritise reducing existential risk.
confidence:	70%.
quality: 	9
emotion: 	4
categories: longtermism, philosophy, xrisk
importance: 10
pride: 		9
wordcount: 	1000
cross:		https://forum.effectivealtruism.org/posts/2pNAPEQ8av3dQyXBX/existential-risk-as-common-cause
---

{%	include xriskAll/links.md		%}

<br> Imagine someone who thought that art was the only thing that made life worth living. <a href="#fn:1" id="fnref:1">1</a> 

What should they do? Binge on galleries? <a href="#fn:2" id="fnref:2">2</a> Work to increase the amount of art and artistic experience, by <a href="{{e2g}}">going into finance</a> to fund artists? Or by becoming an activist for government funding for the arts? Maybe. But there's a strong case that they should pay attention to <a href="{{gcr}}">the ways the world might end</a>: after all, you can't enjoy art if we're all dead.<br><br>

1. Aesthetic experience is good in itself: it's a 'terminal goal'.
2. The extinction of life would destroy all aesthetic experience & prevent future experiences.
3. So reducing existential risk is good, if only to protect the conditions for aesthetic experience.

<br>

The same argument applies to <a href="{{term}}">a huge range</a> of values.

1. [good] is good in itself: it's a 'terminal goal'.
2. The extinction of life would destroy [good], and prevent future [good].
3. So reducing existential risk is good, if only to protect the conditions for [good]. <a href="#fn:3" id="fnref:3">3</a>

<br>

<a href="{{oes}}">Caspar Oesterheld gives</a> a few examples of what people might plug into those brackets:

> Abundance, achievement, adventure, affiliation, altruism, apatheia, art, asceticism, austerity, autarky, authority, autonomy, beauty, benevolence, bodily integrity, challenge, collective property, commemoration, communism, community, compassion, competence, competition, competitiveness, complexity, comradery, conscientiousness, consciousness, contentment, cooperation, courage, [crab-mentality], creativity, crime, critical thinking, curiosity, democracy, determination, dignity, diligence, discipline, diversity, duties, education, emotion, envy, equality, equanimity, excellence, excitement, experience, fairness, faithfulness, family, fortitude, frankness, free will, freedom, friendship, frugality, fulfillment, fun, good intentions, greed, happiness, harmony, health, honesty, honor, humility, idealism, idolatry, imagination, improvement, incorruptibility, individuality, industriousness, intelligence, justice, knowledge, law abidance, life, love, loyalty, modesty, monogamy, mutual affection, nature, novelty, obedience, openness, optimism, order, organization, pain, parsimony, peace, peace of mind, pity, play, population size, preference fulfillment, privacy, progress, promises, property, prosperity, punctuality, punishment, purity, racism, rationality, reliability, religion, respect, restraint, rights, sadness, safety, sanctity, security, self-control, self-denial, self-determination, self-expression, self-pity, simplicity, sincerity, social parasitism, society, spirituality, stability, straightforwardness, strength, striving, subordination, suffering, surprise, technology, temperance, thought, tolerance, toughness, truth, tradition, transparency, valor, variety, veracity, wealth, welfare, wisdom.

<br>
So, from a huge variety of viewpoints, the end of the world is _bad_, you say? What a revelation! <br>

\: the above is only very interesting if we get from "it's good to reduce x-risk" to "it's the most important thing to do" for all these values. This would be the case if extinction was both 1) relatively likely relatively soon, and 2) we could do something about it.<br><br>


### 1) What could kill us all, in the coming century? 

Some big ones are: nuclear winter, runaway climate change, runaway AI, and biological weapons. Regarding (1), <a href="{{h80}}">80,000 Hours</a> report an educated guess of the total probability:

> Many experts who study these issues estimate that the total chance of human extinction in the next century is between 1 and 20%... These figures are about one million times higher than what people normally think.

(And if you think that knowledge of the future is radically uncertain, note you should <a href="{{prior}}">devote <i>more</i> attention</a> to the worst scenarios, not less: 'high uncertainty' is not the same as 'low probability'.)<br><br>

### 2) What can we do about it?

Most of the direct work involves technical research, going into the civil service, or improving the way other big institutions make decisions (e.g. philanthropy, science, NGOs). But anyone can <a href="{{fund}}">fundraise</a> for <a href="{{openPhil}}">the direct work</a> and have large expected effects. 

In fact, the amount of funding for mitigating existential risks is a terrifyingly small fraction of total <a href="{{gov}}">government</a> and <a href="{{charity}}">charity spending</a> (annually, maybe <a href="{{AI}}">$10m for AI safety</a>, <a href="{{nuc}}">$1bn-5bn</a> for nuclear security, <a href="{{bio}}">$1bn</a> for biosecurity): much less than 1%. <a href="{{whatToDo}}">Full list here</a>.

Say we did all that. How much would it reduce the risk? We don't know, but a <a href="{{snyder}}">1% relative decrease per $1bn spent</a> is a not-obviously-useless guess. 

Would this level of mitigation override direct promotion of [good]? As long as you place some value on future generation's access to [good], I think <a href="{{person}}">the answer's yes</a>.

<br>

So it looks like there's a strong apriori case to prioritise x-risk, for anyone who accepts the above estimates of risk and tractability, and accepts that _something_ about life has, or will eventually have, overall value.


<br><br>
<a name="except"></a>

---

<br>

### Who doesn't have to work on reducing x-risk?

* People with incredibly high confidence that extinction will not happen (that is, well above 99% confidence). This is <a href="{{wiki}}">much higher confidence</a> than most people who have looked at the matter.<br>

* People with incredibly high confidence that nothing can be done to affect extinction (that is, well above 99% confidence).<br>

* Avowed egoists. <br>

* <a href="{{paroch}}">People</a> who think that the responsibility to help those you're close to outweighs your responsibility to any number of distant others.<br>

* People with values that don't depend on the world:<br>

  * Nihilists, or other <a href="{{err}}">people who think there are no moral properties</a>.<br>

  * People with an 'honouring' kind of ethics - like Kantians, Aristotelians, or some religions. 

	<ul><a href="{{pettit}}">Philip Pettit makes a helpful distinction</a>: when you act, you can either 'honor' a value (directly instantiating it) or 'promote' it (make more opportunities for it, make it more likely in future). This is a key difference between consequentialism and two of the other big moral theories (deontology and virtue ethics): the latter two only value honouring. <br><br>
	This could get them off the logical hook because, unless "preventing extinction" was a duty or virtue itself, or fit easily into another duty or virtue, there's no moral force against it. (You could try to construe reducing x-risk as "care for others" or "generosity".)<br><br>

	I find it hard to empathise with strict honourers - they seem to value principles, or the cleanliness of their own conduct, <i>infinitely more</i> than the lives or well-being of others - but the intuition is pretty common (<a href="{{swede}}">at least 30%</a> ?).</ul><br>
  

* People that disvalue life:<br>

  * <a href="{{ord}}">Absolute negative utilitarians</a> or <a href="{{natal}}">antinatalists</a>: people who think that life is generally negative in itself.<br>

  * <a href="{{volunt}}">People</a> who think that human life has, and will continue to have, net-negative effects. Of course, deep ecologists who side with extinction would be aiming at a horrendously narrow window, between 'an event which ends all human life' and 'one which ends all life'. They'd still have to work against the latter.<br>

  * <a href="{{dickens}}">Ordinary utilitarians</a> might also be committed to this view, if certain unpleasant contingencies happen (e.g. if we increased the number of suffering beings via colonisation or simulation).<br><br>


* The end of the world is actually not the absolute worst scenario: you might instead have a world with unimaginable amounts of suffering lasting a very long time, a 'quality risk' or '<a href="{{suff}}">S-risk</a>'. You might work on those instead. This strikes me as admirable, but it doesn't have the kind of value-independence that impressed me about the argument at the start of this piece.<br>

* People who don't think that probability estimates or <a href="{{ev}}">expected value</a> should be used for moral decisions. (Intuitionists?)<br>

* You might have an eccentric kind of 'satisficing' about the good, i.e. a <a href="{{piece}}">piecewise function</a> where having some amount of the good is vitally important, but any more than that has no moral significance. This seems more implausible than maximisation.

<br>

(That list is long, but I think most of the bullet points hold few people.)

<br><br>

---

<br>

### Uncertainties

* We really don't know how tractable these risks are: we haven't acted, as a species, on unprecedented century-long projects with literally only one chance for success. (But again, this uncertainty doesn't licence inactivity, because the downside is so large.)<br>

* I place some probability (5% ?) on our future being negative - especially if we spread <a href="{{was}}">normal ecosystems</a> to other planets, or if hyper-detailed simulations of people turn out to have moral weight. If the risk increased, these could 'flip the sign' on extinction, for me.<br>

* I was going to exempt people with '<a href="{{affect}}">person-affecting</a> views' from the argument. But actually if the probability of extinction in the next 80 years (one lifetime) is high enough (1% ?) then they <a href="{{lewis}}">probably have reason</a> to act too (though not an overwhelming mandate), despite ignoring future generations.<br>

* Most people are neither technical researchers nor willing to go into government. So, if x-risk organisation ran out of "<a href="{{rfmf}}">room for more funding</a>" then most people would be off the hook (back to maximising their terminal goal directly), until they had some.<br>

* We don't really know how common real deontologists are. (<a href="{{swede}}">That one study</a> is n=1000 about Sweden, probably an unusually consequentialist place.) As value-honourers, they can maybe duck most of the force of the argument.<br>

* <a href="{{suspic}}">Convergence is often suspicious</a>, when humans are persuading themselves or others.


<br><br>


<div class="accordion">
	<h3>Fates other than death</h3>
	<div>
		The above talks only about extinction risk, and omits the other two kinds of existential catastrophe: "unrecoverable collapse" (e.g. humans surviving in the traditional subsistence manner, but losing all knowledge and exhausting all easily harnessable energy sources) and "unrecoverable dystopia" (e.g. technologically complete global fascism).<br><br>
<!--  -->
		These have their own wrinkles, but the general point remains: most values are ruined by them.
	</div>
</div>
{%  include comments.html %}




<div class="footnotes">
<ol>
    <li class="footnote" id="fn:1">
		For example, <a href="https://archive.org/stream/TwilightOfTheIdolsOrHowToPhilosophizeWithAHammer/TwilightIdols_djvu.txt">Nietzsche said</a> '<i>Without music, life would be a mistake.</i>'
	</li>
<!--  -->
	<li class="footnote" id="fn:2">
		<a href="https://en.wikipedia.org/wiki/Stendhal_syndrome">Steady now</a>!
	</li>
<!--  -->
	<li class="footnote" id="fn:3">
		I <a href="https://en.wikipedia.org/wiki/Cryptomnesia">think</a> I got this argument from Nick Bostrom but I can't find a reference.
	</li>

</ol>
</div>